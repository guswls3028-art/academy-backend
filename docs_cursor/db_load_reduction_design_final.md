# DB ë¶€í•˜ ìµœì†Œí™” ì„¤ê³„ (ìµœì¢… í•©ì¼ì )

## âœ… ìµœì¢… ì„¤ê³„ ì›ì¹™ (í•µì‹¬)

### 0. ê¸°ë³¸ ì›ì¹™
- **"DBëŠ” ì˜êµ¬ ì €ì¥ì†Œ"**, **"RedisëŠ” ìƒíƒœ ìŠ¤íŠ¸ë¦¼/ìºì‹œ"**
- ì§„í–‰ ì¤‘ ìƒíƒœ/ì§„í–‰ë¥ /ì„ì‹œ ê²°ê³¼: Redis
- ìµœì¢… ê²°ê³¼: DB (í•„ìˆ˜)
- ì¡°íšŒ: Redis ìš°ì„ , Redis ë¯¸ìŠ¤ ì‹œ DB

### 1. âŒ DB í´ë§ì€ "ê¼­" í•  í•„ìš” ì—†ë‹¤ â†’ âœ… ì™„ì „íˆ ì œê±° ê°€ëŠ¥

**í•µì‹¬: ì§„í–‰ ìƒíƒœëŠ” Redisì— ì´ë¯¸ ìˆìŒ. DBë¥¼ ì™œ ë•Œë¦¬ëƒ?**

#### ì§„í–‰ ì¤‘ ì‘ì—…
í•„ìš”í•œ ë°ì´í„°:
- `status` â†’ Redisì— ìˆìŒ
- `progress` â†’ Redisì— ìˆìŒ
- `step` â†’ Redisì— ìˆìŒ
- `error` â†’ Redisì— ìˆìŒ

ğŸ‘‰ **DB í•„ìš” ì—†ìŒ**

#### ì™„ë£Œëœ ì‘ì—…
í•„ìš”í•œ ë°ì´í„°:
- `status` â†’ ì™„ë£Œ ì‹œ Redisì— ì €ì¥
- `hls_path` â†’ ì™„ë£Œ ì‹œ Redisì— ì €ì¥
- `duration` â†’ ì™„ë£Œ ì‹œ Redisì— ì €ì¥
- `result` â†’ ì™„ë£Œ ì‹œ Redisì— ì €ì¥
- `error_message` â†’ ì™„ë£Œ ì‹œ Redisì— ì €ì¥

ğŸ‘‰ **DB í•„ìš” ì—†ìŒ**

#### DBëŠ” ì–¸ì œ í•„ìš”í•¨?
ì˜¤ì§ ì´ëŸ° ê²½ìš°ë§Œ:
- ì‚¬ìš©ìê°€ ì‘ì—… í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í–ˆëŠ”ë° Redisì— ìºì‹œê°€ ì—†ì„ ë•Œ
- ê³¼ê±° ê¸°ë¡ì„ ì¡°íšŒí•  ë•Œ

ğŸ‘‰ **ì´ë•Œë§Œ DB fallback**

### 2. ê°€ì¥ ì´ìƒì ì¸ êµ¬ì¡°

```
ì§„í–‰ ì¤‘:
  Frontend
    â†“
  GET /progress/ (Redis only)
    â†“
  DB 0ë²ˆ âœ…

ì™„ë£Œ ê°ì§€:
  Redis status == READY
    â†“
  Frontend stops polling
    â†“
  (ì„ íƒ) GET /detail/ 1íšŒ í˜¸ì¶œ
    â†“
  DB 1íšŒ âœ…

ë.
```

### 3. ì§„í–‰ ìƒí™©ì€ "ë³´ê¸° í¸í•˜ë¼ê³  ì£¼ëŠ” ê²ƒ"
- ì§„í–‰ ìƒí™© ë•Œë¬¸ì— DB í„°ì§€ëŠ” ê²Œ ë¬¸ì œ
- ì‹œì²­ ë¡œê·¸, ì •ì±„ íŒë‹¨ í”„ë¡œê·¸ë˜ìŠ¤ë°”ëŠ” **ë¬´ì¡°ê±´ DB ì•ˆ ë•Œë¦¬ê²Œ**

### 2. ì™„ë£Œ ìƒíƒœëŠ” TTLë¡œ ë‚ ë¦¬ì§€ ì•ŠëŠ”ë‹¤ (í­íƒ„ ë°©ì§€)
- ì™„ë£ŒëŠ” ìì£¼ ì¡°íšŒë˜ê³ , í¬ê¸°ë„ ì‘ê³ , DB ë¶€í•˜ë¥¼ ë§‰ëŠ” í•µì‹¬
- DONE/FAILED/READYëŠ” **TTL ì—†ìŒ** (ê¶Œì¥)
- ë˜ëŠ” ë¹„ìš© ë°©ì–´ ëª¨ë“œë©´ **24ì‹œê°„** (ìµœì†Œ 1ì‹œê°„ì€ ë¹„ì¶”: ë§Œë£Œ í­íƒ„ ê°€ëŠ¥)
- Redis ë©”ëª¨ë¦¬ ê±±ì •? ìƒíƒœ JSON ëª‡ë°± ë°”ì´íŠ¸ ìˆ˜ì¤€ì´ë¼ "ì™„ë£Œ ìºì‹œ"ëŠ” ê±°ì˜ ê³µì§œ

### 3. ë©€í‹°í…Œë„ŒíŠ¸ í‚¤ëŠ” "í…Œë„ŒíŠ¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤" í•„ìˆ˜
- `tenant:{tenant_id}:...` ê³ ì •
- ì´ê±° ì•ˆ í•˜ë©´ ë‚˜ì¤‘ì— ë°˜ë“œì‹œ ì‚¬ê³  ë‚œë‹¤

## ğŸ§± Redis í‚¤ ì„¤ê³„ (ìµœì¢…)

### Video
```
# ì§„í–‰ë¥  (ê¸°ì¡´)
tenant:{tid}:video:{vid}:progress (HASH ë˜ëŠ” JSON)

# ìƒíƒœ (ì‹ ê·œ)
tenant:{tid}:video:{vid}:status (JSON)

# ì„¸ë¶€ ìŠ¤í… (ì„ íƒ)
tenant:{tid}:video:{vid}:step (JSON)
```

### Job (AI/Message ê³µí†µ)
```
# ì§„í–‰ë¥  (ê¸°ì¡´)
tenant:{tid}:job:{jid}:progress

# ìƒíƒœ (ì‹ ê·œ)
tenant:{tid}:job:{jid}:status
```

## â± TTL ì •ì±… (ìš´ì˜ ì•ˆì •ì„±/ê°€ì„±ë¹„ ë°¸ëŸ°ìŠ¤)

### ì§„í–‰ ì¤‘ (PROCESSING)
- **TTL: 6ì‹œê°„** (2ì‹œê°„ì€ ì§§ì„ ìˆ˜ ìˆìŒ: ì¥ì• /ì¬ì‹œë„/ëŒ€ê¸° ë•Œë¬¸ì—)
- ë§¤ progress ì—…ë°ì´íŠ¸ë§ˆë‹¤ TTL **"ìŠ¬ë¼ì´ë”©"** ê°±ì‹ 

### ì™„ë£Œ (DONE/FAILED/READY)
- **TTL: ì—†ìŒ** (ê¶Œì¥)
- ë˜ëŠ” ë¹„ìš© ë°©ì–´ ëª¨ë“œë©´ **24ì‹œê°„** (ìµœì†Œ 1ì‹œê°„ì€ ë¹„ì¶”: ë§Œë£Œ í­íƒ„ ê°€ëŠ¥)

**Redis ë©”ëª¨ë¦¬ ê±±ì •?**
- ìƒíƒœ JSON ëª‡ë°± ë°”ì´íŠ¸ ìˆ˜ì¤€ì´ë¼ "ì™„ë£Œ ìºì‹œ"ëŠ” ê±°ì˜ ê³µì§œì— ê°€ê¹Œì›€
- ë°˜ëŒ€ë¡œ DB ë¶€í•˜ë¥¼ ì—„ì²­ ì¤„ì—¬ì¤Œ

## ğŸ§­ API ì„¤ê³„ (DB ë¶€í•˜ 0ë¡œ ë§Œë“œëŠ” í•µì‹¬)

### 1. Progress/Status ì „ìš© endpoint ì‹ ì„¤ (ê°•ì¶”)

#### Video
```
GET /media/videos/{id}/progress/
```
- **Redis-only ì‘ë‹µ**
- ì‘ë‹µ: `status` + `progress` + `step` + (ì™„ë£Œë©´ `hls_path`/`duration`/`error`)
- **ì§„í–‰ ì¤‘ í´ë§ì€ ì—¬ê¸°ë§Œ**

#### Job
```
GET /api/v1/jobs/{job_id}/progress/
```
- **Redis-only**
- ì‘ë‹µ: `status` + `progress` + (ì™„ë£Œë©´ `result`/`error`)

### 2. ê¸°ì¡´ Detail endpointëŠ” ê·¸ëŒ€ë¡œ ë‘”ë‹¤
```
GET /media/videos/{id}/ â†’ DB ê¸°ë°˜ (ê¸°ì¡´ ìœ ì§€)
GET /api/v1/jobs/{job_id}/ â†’ DB ê¸°ë°˜ (ê¸°ì¡´ ìœ ì§€)
```

**í”„ë¡ íŠ¸ ì „ëµ:**
- ì§„í–‰ ì¤‘: **progress endpointë§Œ í´ë§**
- ì™„ë£Œ ê°ì§€: **detail endpoint 1íšŒ í˜¸ì¶œ í›„ í´ë§ ì¢…ë£Œ**

ì´ ë°©ì‹ì´ **"ì„±ëŠ¥ + ì•ˆì • + ìœ ì§€ë³´ìˆ˜"** ë‹¤ ì¡ëŠ” ë² ìŠ¤íŠ¸.

## ğŸ§© ì›Œì»¤ ìª½ ì €ì¥ ë¡œì§ (ìµœì¢…)

### Video ì›Œì»¤

#### mark_processing
- DB ì—…ë°ì´íŠ¸(í•„ìš”) + Redis status ì €ì¥(PROCESSING, TTL 6h)

#### progress ì—…ë°ì´íŠ¸(ë§¤ step)
- Redis progressë§Œ ì—…ë°ì´íŠ¸
- Redis statusëŠ” "PROCESSING ìœ ì§€" ì •ë„ë§Œ(ì„ íƒ)

#### complete/fail
- DB ì—…ë°ì´íŠ¸(ì˜êµ¬)
- Redis status ì €ì¥(READY/FAILED, **TTL ì—†ìŒ**)
- Redis progressëŠ” ë‚¨ê²¨ë„ ë˜ê³  ì§€ì›Œë„ ë¨(ì„ íƒ)

### AI/Message ì›Œì»¤

#### ìƒíƒœ ë³€í™”(START/PROCESSING)
- Redis status ì €ì¥(PROCESSING, TTL 6h)

#### ì™„ë£Œ(DONE/FAILED)
- DB ì €ì¥
- Redis statusì— **result/errorê¹Œì§€ í¬í•¨**í•´ì„œ ì €ì¥(=ì™„ë£Œ í›„ DB ì•ˆ ë´ë„ ë¨)

**ì—¬ê¸°ì„œ resultê¹Œì§€ Redisì— ë„£ìœ¼ë©´**
- JobStatusViewì—ì„œ ì™„ë£Œ ì‹œì—ë„ DB ì¡°íšŒê°€ ê±°ì˜ ì‚¬ë¼ì§

## ğŸ”’ ë©±ë“±ì„±/ì¼ê´€ì„± ê·œì¹™ (ìš´ì˜ ì•ˆì •ì„±)

### 1. "DBê°€ ì†ŒìŠ¤ ì˜¤ë¸Œ íŠ¸ë£¨ìŠ¤"
- Redis ì“°ê¸° ì‹¤íŒ¨í•´ë„ DBê°€ ì €ì¥ë˜ë©´ OK
- RedisëŠ” ìºì‹œì´ì ì§„í–‰ ìŠ¤íŠ¸ë¦¼

### 2. ì™„ë£Œ ìƒíƒœëŠ” "ë‹¨ë°©í–¥"
- PROCESSING â†’ READY/FAILED/DONE
- READY/DONE/FAILEDê°€ Redisì— ìˆìœ¼ë©´, ì›Œì»¤ê°€ ê°™ì€ ì´ë²¤íŠ¸ë¥¼ ë˜ ë³´ë‚´ë„ ë®ì–´ì“°ê¸° OK (ë©±ë“±)

### 3. í…Œë„ŒíŠ¸ ê²€ì¦
- progress endpointì—ì„œ tenant_id í™•ì¸ í›„ í‚¤ ì¡°íšŒ
- ë‹¤ë¥¸ í…Œë„ŒíŠ¸ê°€ ë‹¤ë¥¸ ì‘ì—…ì„ ì¡°íšŒ ëª»í•˜ê²Œ

## âš¡ ì„±ëŠ¥ ì¶”ê°€ íŒ (ê±°ì˜ ê³µì§œ)

### 1. Redis êµ¬ì¡°ëŠ” HASH ì¶”ì²œ
- `HSET key field value`ë¡œ ê°±ì‹ í•˜ë©´ JSON dump/loads ë¹„ìš© ì¤„ì–´ë“¦
- step/progress/status ìì£¼ ë°”ë€ŒëŠ” ì• ë“¤ì€ HASHê°€ ìœ ë¦¬

### 2. í´ë§ ê°„ê²© "ì ì‘í˜•"
```
0~10ì´ˆ: 1ì´ˆ
10~60ì´ˆ: 2ì´ˆ
60ì´ˆ ì´ìƒ: 3~5ì´ˆ
ì™„ë£Œ ì‹œ ì¦‰ì‹œ ì¤‘ì§€
```
=> DBëŠ” ì´ë¯¸ 0ì´ì§€ë§Œ, Redis/ë„¤íŠ¸ì›Œí¬ ë¹„ìš©ë„ ì¤„ì–´ë“¦

### 3. DB_CONN_MAX_AGE ì¤„ì´ê¸°
- 15~20 ì¶”ì²œ
- connection ì ìœ  ì¤„ì—¬ì„œ ì•ˆì •ì„± ìƒìŠ¹

## ğŸ’¸ ê°€ì„±ë¹„ ê´€ì  ê²°ë¡ 

### âŒ DB í´ë§ì€ êµ¬ì¡°ì ìœ¼ë¡œ ë¶ˆí•„ìš”

**ì§€ê¸ˆ DBê°€ í„°ì§€ëŠ” ì´ìœ :**
1. DB ì²´ê¸‰ ì‘ìŒ (ê·¼ë³¸) â†’ RDS í¬ê¸° ì¦ê°€ í•„ìš”
2. **í´ë§ì´ DB ë•Œë¦¼ (ë¶ˆí•„ìš”í•œ ë¶€í•˜)** â†’ Redis-onlyë¡œ í•´ê²°

**Redis-onlyë¡œ ë°”ê¾¸ë©´:**
- DB SELECT í­ê²© **0ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆìŒ** âœ…
- ì§„í–‰ ìƒí™©ì€ "ë³´ê¸° í¸í•˜ë¼ê³  ì£¼ëŠ” ê²ƒ"ì¼ ë¿
- ì§„í–‰ ìƒí™© ë•Œë¬¸ì— DB í„°ì§€ëŠ” ê²Œ ë¬¸ì œ

**ê·¸ë˜ì„œ "í•©ì¼ì "ì€:**
- Redis progress/status endpoint ë¶„ë¦¬ë¡œ **í´ë§ DB 0 ë§Œë“¤ê¸°** (í•µì‹¬)
- **ì™„ë£Œ ìºì‹œ TTL ì œê±°(ë˜ëŠ” 24h)**ë¡œ ë§Œë£Œ í­íƒ„ ë°©ì§€
- RDSëŠ” ìµœì†Œ small, ì¶”ì²œ medium (ì´ê±´ ì²´ê¸‰ ë¬¸ì œë¼ ê²°êµ­ í•„ìš”)
- Excel bulk ìµœì í™”ëŠ” ë‹¤ìŒ ë‹¨ê³„(í•˜ì§€ë§Œ ì´ê²Œ ì¥ê¸°ì ìœ¼ë¡œ ë¹„ìš©ì„ ë” ì¤„ì„)

### ğŸ”¥ ì§„ì§œ ë‹µ

**âŒ DB í´ë§ì€ êµ¬ì¡°ì ìœ¼ë¡œ ë¶ˆí•„ìš”**

í´ë§ì„ í•´ì•¼ í•œë‹¤ë©´:
- ê·¸ê±´ Redis ì„¤ê³„ê°€ ë¶ˆì™„ì „í•´ì„œì„
- Redisì— ëª¨ë“  ë°ì´í„°ê°€ ìˆìœ¼ë©´ DB í´ë§ ë¶ˆí•„ìš”

**ë” ë‚˜ì•„ê°€ë©´?**
- WebSocket ì“°ë©´ í´ë§ë„ í•„ìš” ì—†ìŒ
- Worker â†’ Redis â†’ PubSub â†’ WebSocket â†’ Frontend
- ê·¼ë° ì§€ê¸ˆ ë‹¨ê³„ì—ì„œëŠ” Redis-only pollingì´ë©´ ì¶©ë¶„íˆ ì•ˆì •ì ì„

## âœ… ìµœì¢… ì‹¤í–‰ ìš°ì„ ìˆœìœ„ (í˜„ì‹¤ì ì¸ ë¡œë“œë§µ)

### ì˜¤ëŠ˜ (í•µì‹¬)
1. progress/status ì „ìš© endpoint ì¶”ê°€ (Redis-only)
2. ì™„ë£Œ ìƒíƒœ Redis ìºì‹œ(TTL ì—†ìŒ/24h)
3. í‚¤ì— tenant ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì ìš©

### ì´ë²ˆì£¼
4. í”„ë¡ íŠ¸ í´ë§ì„ progress endpointë¡œ ì „í™˜ + ì™„ë£Œì‹œ í´ë§ ì¤‘ì§€
5. DB_CONN_MAX_AGE 15~20 ì¡°ì •

### ë‹¤ìŒ
6. Excel bulk_create/ì—…ì„œíŠ¸ ìµœì í™”

---

## ğŸ”§ êµ¬í˜„ ìƒì„¸

### 1. Redis í‚¤ í—¬í¼ (Tenant ë„¤ì„ìŠ¤í˜ì´ìŠ¤)

**íŒŒì¼**: `apps/support/video/redis_status_cache.py`

```python
"""ë¹„ë””ì˜¤ ìƒíƒœ Redis ìºì‹± í—¬í¼ (Tenant ë„¤ì„ìŠ¤í˜ì´ìŠ¤)"""
from typing import Optional, Dict, Any
from libs.redis.client import get_redis_client
import json
import logging

logger = logging.getLogger(__name__)


def _get_video_status_key(tenant_id: int, video_id: int) -> str:
    """ë¹„ë””ì˜¤ ìƒíƒœ Redis í‚¤ (Tenant ë„¤ì„ìŠ¤í˜ì´ìŠ¤)"""
    return f"tenant:{tenant_id}:video:{video_id}:status"


def _get_video_progress_key(tenant_id: int, video_id: int) -> str:
    """ë¹„ë””ì˜¤ ì§„í–‰ë¥  Redis í‚¤ (Tenant ë„¤ì„ìŠ¤í˜ì´ìŠ¤)"""
    return f"tenant:{tenant_id}:video:{video_id}:progress"


def get_video_status_from_redis(tenant_id: int, video_id: int) -> Optional[Dict[str, Any]]:
    """Redisì—ì„œ ë¹„ë””ì˜¤ ìƒíƒœ ì¡°íšŒ (Tenant ê²€ì¦)"""
    try:
        redis_client = get_redis_client()
        if not redis_client:
            return None
        
        key = _get_video_status_key(tenant_id, video_id)
        cached_data = redis_client.get(key)
        if not cached_data:
            return None
        
        return json.loads(cached_data)
    except Exception as e:
        logger.debug("Redis video status lookup failed: %s", e)
        return None


def cache_video_status(
    tenant_id: int,
    video_id: int,
    status: str,
    hls_path: Optional[str] = None,
    duration: Optional[int] = None,
    error_reason: Optional[str] = None,
    ttl: Optional[int] = None,  # Noneì´ë©´ TTL ì—†ìŒ
) -> bool:
    """ë¹„ë””ì˜¤ ìƒíƒœë¥¼ Redisì— ìºì‹± (Tenant ë„¤ì„ìŠ¤í˜ì´ìŠ¤)"""
    try:
        redis_client = get_redis_client()
        if not redis_client:
            return False
        
        status_data = {
            "status": status,
        }
        if hls_path is not None:
            status_data["hls_path"] = hls_path
        if duration is not None:
            status_data["duration"] = duration
        if error_reason is not None:
            status_data["error_reason"] = error_reason
        
        key = _get_video_status_key(tenant_id, video_id)
        if ttl is None:
            # TTL ì—†ìŒ (ì™„ë£Œ ìƒíƒœ)
            redis_client.set(key, json.dumps(status_data, default=str))
        else:
            # TTL ì„¤ì • (ì§„í–‰ ì¤‘ ìƒíƒœ)
            redis_client.setex(key, ttl, json.dumps(status_data, default=str))
        
        return True
    except Exception as e:
        logger.warning("Failed to cache video status in Redis: %s", e)
        return False


def refresh_video_progress_ttl(tenant_id: int, video_id: int, ttl: int = 21600) -> bool:
    """ë¹„ë””ì˜¤ ì§„í–‰ë¥  TTL ìŠ¬ë¼ì´ë”© ê°±ì‹  (6ì‹œê°„)"""
    try:
        redis_client = get_redis_client()
        if not redis_client:
            return False
        
        progress_key = _get_video_progress_key(tenant_id, video_id)
        status_key = _get_video_status_key(tenant_id, video_id)
        
        # âœ… exists ì²´í¬ í›„ TTL ê°±ì‹  (ì˜ë„ì¹˜ ì•Šì€ ìƒíƒœ ë°©ì§€)
        if redis_client.exists(progress_key):
            redis_client.expire(progress_key, ttl)
        if redis_client.exists(status_key):
            redis_client.expire(status_key, ttl)
        
        return True
    except Exception as e:
        logger.warning("Failed to refresh video TTL: %s", e)
        return False
```

**íŒŒì¼**: `apps/domains/ai/redis_status_cache.py`

```python
"""AI Job ìƒíƒœ Redis ìºì‹± í—¬í¼ (Tenant ë„¤ì„ìŠ¤í˜ì´ìŠ¤)"""
from typing import Optional, Dict, Any
from libs.redis.client import get_redis_client
import json
import logging

logger = logging.getLogger(__name__)


def _get_job_status_key(tenant_id: str, job_id: str) -> str:
    """Job ìƒíƒœ Redis í‚¤ (Tenant ë„¤ì„ìŠ¤í˜ì´ìŠ¤)"""
    return f"tenant:{tenant_id}:job:{job_id}:status"


def _get_job_progress_key(tenant_id: str, job_id: str) -> str:
    """Job ì§„í–‰ë¥  Redis í‚¤ (Tenant ë„¤ì„ìŠ¤í˜ì´ìŠ¤)"""
    return f"tenant:{tenant_id}:job:{job_id}:progress"


def get_job_status_from_redis(tenant_id: str, job_id: str) -> Optional[Dict[str, Any]]:
    """Redisì—ì„œ Job ìƒíƒœ ì¡°íšŒ (Tenant ê²€ì¦)"""
    try:
        redis_client = get_redis_client()
        if not redis_client:
            return None
        
        key = _get_job_status_key(tenant_id, job_id)
        cached_data = redis_client.get(key)
        if not cached_data:
            return None
        
        return json.loads(cached_data)
    except Exception as e:
        logger.debug("Redis job status lookup failed: %s", e)
        return None


def cache_job_status(
    tenant_id: str,
    job_id: str,
    status: str,
    job_type: Optional[str] = None,
    error_message: Optional[str] = None,
    result: Optional[Dict[str, Any]] = None,
    ttl: Optional[int] = None,  # Noneì´ë©´ TTL ì—†ìŒ
) -> bool:
    """Job ìƒíƒœë¥¼ Redisì— ìºì‹± (Tenant ë„¤ì„ìŠ¤í˜ì´ìŠ¤, ì™„ë£Œ ì‹œ result í¬í•¨)"""
    try:
        redis_client = get_redis_client()
        if not redis_client:
            return False
        
        status_data = {
            "status": status,
        }
        if job_type is not None:
            status_data["job_type"] = job_type
        if error_message is not None:
            status_data["error_message"] = error_message
        if result is not None:
            status_data["result"] = result
        
        key = _get_job_status_key(tenant_id, job_id)
        if ttl is None:
            # TTL ì—†ìŒ (ì™„ë£Œ ìƒíƒœ)
            redis_client.set(key, json.dumps(status_data, default=str))
        else:
            # TTL ì„¤ì • (ì§„í–‰ ì¤‘ ìƒíƒœ)
            redis_client.setex(key, ttl, json.dumps(status_data, default=str))
        
        return True
    except Exception as e:
        logger.warning("Failed to cache job status in Redis: %s", e)
        return False


def refresh_job_progress_ttl(tenant_id: str, job_id: str, ttl: int = 21600) -> bool:
    """Job ì§„í–‰ë¥  TTL ìŠ¬ë¼ì´ë”© ê°±ì‹  (6ì‹œê°„)"""
    try:
        redis_client = get_redis_client()
        if not redis_client:
            return False
        
        progress_key = _get_job_progress_key(tenant_id, job_id)
        status_key = _get_job_status_key(tenant_id, job_id)
        
        # ì§„í–‰ë¥ ê³¼ ìƒíƒœ ëª¨ë‘ TTL ê°±ì‹ 
        redis_client.expire(progress_key, ttl)
        redis_client.expire(status_key, ttl)
        
        return True
    except Exception as e:
        logger.warning("Failed to refresh job TTL: %s", e)
        return False
```

### 2. Progress/Status ì „ìš© Endpoint

**íŒŒì¼**: `apps/support/video/views/progress_views.py` (ì‹ ê·œ ë˜ëŠ” ìˆ˜ì •)

```python
"""ë¹„ë””ì˜¤ ì§„í–‰ë¥ /ìƒíƒœ ì „ìš© endpoint (Redis-only)"""
from rest_framework import status
from rest_framework.decorators import action
from rest_framework.response import Response
from rest_framework.views import APIView
from rest_framework.permissions import IsAuthenticated

from apps.support.video.models import Video
from apps.support.video.encoding_progress import (
    get_video_encoding_progress,
    get_video_encoding_step_detail,
    get_video_encoding_remaining_seconds,
)
from apps.support.video.redis_status_cache import (
    get_video_status_from_redis,
)


class VideoProgressView(APIView):
    """ë¹„ë””ì˜¤ ì§„í–‰ë¥ /ìƒíƒœ ì¡°íšŒ (Redis-only, DB ë¶€í•˜ 0)"""
    
    permission_classes = [IsAuthenticated]
    
    def get(self, request, pk):
        """GET /media/videos/{id}/progress/"""
        video_id = int(pk)
        tenant = getattr(request, "tenant", None)
        
        if not tenant:
            return Response(
                {"detail": "tenantê°€ í•„ìš”í•©ë‹ˆë‹¤."},
                status=status.HTTP_400_BAD_REQUEST,
            )
        
        # âœ… Redisì—ì„œ ìƒíƒœ ì¡°íšŒ (Tenant ë„¤ì„ìŠ¤í˜ì´ìŠ¤)
        cached_status = get_video_status_from_redis(tenant.id, video_id)
        
        if not cached_status:
            # Redisì— ì—†ìœ¼ë©´ 404 (ì§„í–‰ ì¤‘ì´ ì•„ë‹ˆê±°ë‚˜ ì™„ë£Œ í›„ TTL ë§Œë£Œ)
            return Response(
                {"detail": "ì§„í–‰ ì¤‘ì¸ ì‘ì—…ì´ ì•„ë‹™ë‹ˆë‹¤."},
                status=status.HTTP_404_NOT_FOUND,
            )
        
        video_status = cached_status.get("status")
        
        # âœ… ì§„í–‰ë¥ ì€ Redisì—ì„œ ì¡°íšŒ
        progress = None
        step_detail = None
        remaining_seconds = None
        
        if video_status == "PROCESSING":
            progress = get_video_encoding_progress(video_id)
            step_detail = get_video_encoding_step_detail(video_id)
            remaining_seconds = get_video_encoding_remaining_seconds(video_id)
        
        # âœ… ì‘ë‹µ êµ¬ì„±
        response_data = {
            "id": video_id,
            "status": video_status,
            "encoding_progress": progress,
            "encoding_remaining_seconds": remaining_seconds,
            "encoding_step_index": step_detail.get("step_index") if step_detail else None,
            "encoding_step_total": step_detail.get("step_total") if step_detail else None,
            "encoding_step_name": step_detail.get("step_name_display") if step_detail else None,
            "encoding_step_percent": step_detail.get("step_percent") if step_detail else None,
        }
        
        # âœ… ì™„ë£Œ ìƒíƒœë©´ ì¶”ê°€ ì •ë³´ í¬í•¨
        if video_status in ["READY", "FAILED"]:
            response_data["hls_path"] = cached_status.get("hls_path")
            response_data["duration"] = cached_status.get("duration")
            if video_status == "FAILED":
                response_data["error_reason"] = cached_status.get("error_reason")
        
        return Response(response_data)
```

**íŒŒì¼**: `apps/domains/ai/views/job_progress_view.py` (ì‹ ê·œ)

```python
"""AI Job ì§„í–‰ë¥ /ìƒíƒœ ì „ìš© endpoint (Redis-only)"""
from rest_framework import status
from rest_framework.permissions import IsAuthenticated
from rest_framework.response import Response
from rest_framework.views import APIView

from apps.domains.ai.redis_status_cache import get_job_status_from_redis
from src.infrastructure.cache.redis_progress_adapter import RedisProgressAdapter


class JobProgressView(APIView):
    """Job ì§„í–‰ë¥ /ìƒíƒœ ì¡°íšŒ (Redis-only, DB ë¶€í•˜ 0)"""
    
    permission_classes = [IsAuthenticated]
    
    def get(self, request, job_id: str):
        """GET /api/v1/jobs/{job_id}/progress/"""
        tenant = getattr(request, "tenant", None)
        
        if not tenant:
            return Response(
                {"detail": "tenantê°€ í•„ìš”í•©ë‹ˆë‹¤."},
                status=status.HTTP_400_BAD_REQUEST,
            )
        
        # âœ… Redisì—ì„œ ìƒíƒœ ì¡°íšŒ (Tenant ë„¤ì„ìŠ¤í˜ì´ìŠ¤)
        cached_status = get_job_status_from_redis(str(tenant.id), job_id)
        
        if not cached_status:
            # Redisì— ì—†ìœ¼ë©´ 404 (ì§„í–‰ ì¤‘ì´ ì•„ë‹ˆê±°ë‚˜ ì™„ë£Œ í›„ TTL ë§Œë£Œ)
            return Response(
                {"detail": "ì§„í–‰ ì¤‘ì¸ ì‘ì—…ì´ ì•„ë‹™ë‹ˆë‹¤."},
                status=status.HTTP_404_NOT_FOUND,
            )
        
        job_status = cached_status.get("status")
        
        # âœ… ì§„í–‰ë¥ ì€ Redisì—ì„œ ì¡°íšŒ
        progress = None
        if job_status == "PROCESSING":
            progress_adapter = RedisProgressAdapter()
            progress = progress_adapter.get_progress(job_id)
        
        # âœ… ì‘ë‹µ êµ¬ì„±
        response_data = {
            "job_id": job_id,
            "job_type": cached_status.get("job_type"),
            "status": job_status,
            "progress": progress,
        }
        
        # âœ… ì™„ë£Œ ìƒíƒœë©´ result/error í¬í•¨
        if job_status in ["DONE", "FAILED"]:
            response_data["error_message"] = cached_status.get("error_message")
            if job_status == "DONE":
                response_data["result"] = cached_status.get("result")
        
        return Response(response_data)
```

### 3. ì›Œì»¤ ì €ì¥ ë¡œì§ ìˆ˜ì •

**íŒŒì¼**: `apps/support/video/services/sqs_queue.py`

```python
def complete_video(
    self,
    video_id: int,
    hls_path: str,
    duration: Optional[int] = None,
) -> tuple[bool, str]:
    """ë¹„ë””ì˜¤ ì²˜ë¦¬ ì™„ë£Œ ì²˜ë¦¬"""
    video = get_video_for_update(video_id)
    if not video:
        return False, "not_found"
    
    # DB ì—…ë°ì´íŠ¸ (ì˜êµ¬ ì €ì¥)
    video.hls_path = str(hls_path)
    if duration is not None and duration >= 0:
        video.duration = int(duration)
    video.status = Video.Status.READY
    
    # ... (ê¸°ì¡´ ì½”ë“œ)
    
    video.save(update_fields=update_fields)
    
    # âœ… Redisì— ìµœì¢… ìƒíƒœ ì €ì¥ (TTL ì—†ìŒ)
    try:
        from apps.support.video.redis_status_cache import cache_video_status
        # tenant_idëŠ” videoì—ì„œ ê°€ì ¸ì˜¤ê¸° (ì˜ˆ: video.session.tenant_id)
        tenant_id = video.session.tenant_id if hasattr(video, "session") and video.session else None
        if tenant_id:
            cache_video_status(
                tenant_id=tenant_id,
                video_id=video_id,
                status=Video.Status.READY.value,
                hls_path=hls_path,
                duration=duration,
                ttl=None,  # TTL ì—†ìŒ
            )
    except Exception as e:
        logger.warning("Failed to cache video status in Redis: %s", e)
    
    return True, "ok"
```

**íŒŒì¼**: `academy/adapters/db/django/repositories_ai.py`

```python
def save(self, job: AIJob) -> None:
    """AIJob ì €ì¥ (ì™„ë£Œ ì‹œ Redisì—ë„ ì €ì¥, result í¬í•¨)"""
    from django.utils import timezone
    from apps.domains.ai.models import AIJobModel
    now = timezone.now()
    
    # DB ì €ì¥
    model, created = AIJobModel.objects.update_or_create(
        job_id=job.job_id,
        defaults={
            "job_type": job.job_type,
            "status": job.status.value,
            "payload": job.payload,
            "tenant_id": job.tenant_id,
            "error_message": job.error_message,
            "updated_at": now,
        }
    )
    
    # âœ… ì™„ë£Œ/ì‹¤íŒ¨ ì‹œ Redisì— ìƒíƒœ ì €ì¥ (TTL ì—†ìŒ, result í¬í•¨)
    if job.status.value in ["DONE", "FAILED"]:
        try:
            from apps.domains.ai.redis_status_cache import cache_job_status
            from academy.adapters.db.django.repositories_ai import DjangoAIJobRepository
            
            # result ê°€ì ¸ì˜¤ê¸°
            result_payload = None
            if job.status.value == "DONE":
                repo = DjangoAIJobRepository()
                result_payload = repo.get_result_payload_for_job(model)
            
            cache_job_status(
                tenant_id=job.tenant_id,
                job_id=job.job_id,
                status=job.status.value,
                job_type=job.job_type,
                error_message=job.error_message,
                result=result_payload,  # ì™„ë£Œ ì‹œ result í¬í•¨
                ttl=None,  # TTL ì—†ìŒ
            )
        except Exception as e:
            logger.warning("Failed to cache job status in Redis: %s", e)
    
    # âœ… PROCESSING ìƒíƒœë„ Redisì— ì €ì¥ (TTL 6ì‹œê°„)
    elif job.status.value == "PROCESSING":
        try:
            from apps.domains.ai.redis_status_cache import cache_job_status
            cache_job_status(
                tenant_id=job.tenant_id,
                job_id=job.job_id,
                status=job.status.value,
                job_type=job.job_type,
                ttl=21600,  # 6ì‹œê°„
            )
        except Exception as e:
            logger.warning("Failed to cache job status in Redis: %s", e)
```

### 4. í”„ë¡ íŠ¸ì—”ë“œ í´ë§ ì „í™˜

**íŒŒì¼**: `src/shared/ui/asyncStatus/useWorkerJobPoller.ts`

```typescript
// Progress endpointë¡œ ì „í™˜
function pollVideoJob(taskId: string, videoId: string, onSuccess?: () => void) {
  api
    .get<{
      status: string;
      encoding_progress?: number | null;
      encoding_remaining_seconds?: number | null;
      encoding_step_index?: number | null;
      encoding_step_total?: number | null;
      encoding_step_name?: string | null;
      encoding_step_percent?: number | null;
      hls_path?: string | null;
      duration?: number | null;
      error_reason?: string | null;
    }>(`/media/videos/${videoId}/progress/`)  // âœ… progress endpoint ì‚¬ìš©
    .then((res) => {
      const status = res.data?.status;
      
      if (status === "PROCESSING") {
        // ì§„í–‰ ì¤‘: ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        const encodingProgress = res.data?.encoding_progress;
        const remainingSeconds = res.data?.encoding_remaining_seconds ?? null;
        const stepIndex = res.data?.encoding_step_index;
        const stepTotal = res.data?.encoding_step_total;
        const stepName = res.data?.encoding_step_name;
        const stepPercent = res.data?.encoding_step_percent;
        
        const encodingStep =
          typeof stepIndex === "number" &&
          typeof stepTotal === "number" &&
          typeof stepName === "string" &&
          typeof stepPercent === "number"
            ? { index: stepIndex, total: stepTotal, name: stepName, percent: stepPercent }
            : null;
        
        if (typeof encodingProgress === "number") {
          asyncStatusStore.updateProgress(
            taskId,
            Math.min(99, Math.max(1, encodingProgress)),
            remainingSeconds ?? undefined,
            encodingStep
          );
        }
      } else if (status === "READY") {
        // âœ… ì™„ë£Œ: detail endpoint 1íšŒ í˜¸ì¶œ í›„ í´ë§ ì¢…ë£Œ
        onSuccess?.();
        asyncStatusStore.completeTask(taskId, "success");
      } else if (status === "FAILED") {
        asyncStatusStore.completeTask(taskId, "error", res.data?.error_reason || "ì˜ìƒ ì²˜ë¦¬ ì‹¤íŒ¨");
      }
    })
    .catch(() => {});
}

function pollExcelJob(taskId: string, onSuccess?: () => void) {
  api
    .get<{
      status: string;
      progress?: { percent?: number; step_index?: number; step_total?: number; step_name_display?: string; step_percent?: number };
      error_message?: string | null;
      result?: any;
    }>(`/api/v1/jobs/${taskId}/progress/`)  // âœ… progress endpoint ì‚¬ìš©
    .then((res) => {
      const status = res.data?.status;
      
      if (status === "PROCESSING") {
        // ì§„í–‰ ì¤‘: ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
        const progress = res.data?.progress;
        if (progress?.percent !== undefined) {
          const encodingStep =
            typeof progress.step_index === "number" &&
            typeof progress.step_total === "number" &&
            typeof progress.step_name_display === "string" &&
            typeof progress.step_percent === "number"
              ? {
                  index: progress.step_index,
                  total: progress.step_total,
                  name: progress.step_name_display,
                  percent: progress.step_percent,
                }
              : null;
          asyncStatusStore.updateProgress(taskId, progress.percent, undefined, encodingStep);
        }
      } else if (status === "DONE") {
        // âœ… ì™„ë£Œ: detail endpoint 1íšŒ í˜¸ì¶œ í›„ í´ë§ ì¢…ë£Œ
        onSuccess?.();
        asyncStatusStore.completeTask(taskId, "success");
      } else if (status === "FAILED") {
        asyncStatusStore.completeTask(taskId, "error", res.data?.error_message || "ì²˜ë¦¬ ì‹¤íŒ¨");
      }
    })
    .catch(() => {});
}

// âœ… ì ì‘í˜• í´ë§ ê°„ê²©
const getPollInterval = (elapsedSeconds: number): number => {
  if (elapsedSeconds < 10) return 1000;  // 0~10ì´ˆ: 1ì´ˆ
  if (elapsedSeconds < 60) return 2000;  // 10~60ì´ˆ: 2ì´ˆ
  return 3000;  // 60ì´ˆ ì´ìƒ: 3ì´ˆ
};
```

## ğŸ“Š ì˜ˆìƒ íš¨ê³¼

### Before (í˜„ì¬)
- ë¹„ë””ì˜¤ 3ê°œ + ì—‘ì…€ 2ê°œ ì§„í–‰ ì¤‘
- ì´ˆë‹¹: 5ë²ˆ DB SELECT (í´ë§)
- 10ë¶„: ì•½ 3,000ë²ˆ DB SELECT
- RDS CPU: 80-100%

### After (ê°œì„  í›„)
- ë¹„ë””ì˜¤ 3ê°œ + ì—‘ì…€ 2ê°œ ì§„í–‰ ì¤‘
- ì´ˆë‹¹: **0ë²ˆ DB SELECT** (ì§„í–‰ ì¤‘ ì‘ì—…ì€ Redisë§Œ ì¡°íšŒ)
- ì™„ë£Œ í›„: **0ë²ˆ DB SELECT** (Redis ìºì‹±, TTL ì—†ìŒ)
- RDS CPU: **10-20%** (ëŒ€í­ ê°ì†Œ)

### ğŸ”¥ í•µì‹¬ ì •ë¦¬

**âŒ DB í´ë§ì€ êµ¬ì¡°ì ìœ¼ë¡œ ë¶ˆí•„ìš”**

**ì§„í–‰ ìƒí™©ì€ "ë³´ê¸° í¸í•˜ë¼ê³  ì£¼ëŠ” ê²ƒ"**
- ì§„í–‰ ìƒí™© ë•Œë¬¸ì— DB í„°ì§€ëŠ” ê²Œ ë¬¸ì œ
- ì‹œì²­ ë¡œê·¸, ì •ì±„ íŒë‹¨ í”„ë¡œê·¸ë˜ìŠ¤ë°”ëŠ” **ë¬´ì¡°ê±´ DB ì•ˆ ë•Œë¦¬ê²Œ**

**Redis-onlyë¡œ ë°”ê¾¸ë©´:**
- DB SELECT í­ê²© **0ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆìŒ** âœ…
- ì§„í–‰ ì¤‘: Redisë§Œ ì¡°íšŒ
- ì™„ë£Œ í›„: Redis ìºì‹± (TTL ì—†ìŒ)
- DBëŠ” ì˜¤ì§ fallbackìœ¼ë¡œë§Œ ì‚¬ìš© (ìƒˆë¡œê³ ì¹¨, ê³¼ê±° ê¸°ë¡)

## ğŸ¯ ì—‘ì…€ ëŒ€ëŸ‰ ì¿¼ë¦¬ ìµœì í™” ì „ëµ

### âŒ í˜„ì¬ ë¬¸ì œì 

**ì—‘ì…€ íŒŒì‹±ì—ì„œ DB ë¶€í•˜ëŠ” Redisë¡œ í•´ê²°í•˜ëŠ” ë¬¸ì œê°€ ì•„ë‹˜**
- âœ… **DB ì ‘ê·¼ íŒ¨í„´ì„ ë°”ê¾¸ëŠ” ë¬¸ì œì„**

#### í˜„ì¬ êµ¬ì¡° (ë¹„íš¨ìœ¨)
```python
# ê° í•™ìƒë§ˆë‹¤ ê°œë³„ ì¿¼ë¦¬
for row in students_data:
    student, created = get_or_create_student_for_lecture_enroll(...)
    # ê° í•™ìƒë§ˆë‹¤:
    # 1. ê¸°ì¡´ í™œì„± í•™ìƒ ì¡°íšŒ (SELECT)
    # 2. ì†Œí”„íŠ¸ ì‚­ì œëœ í•™ìƒ ì¡°íšŒ (SELECT)
    # 3. ì—†ìœ¼ë©´ ì‹ ê·œ ìƒì„± (INSERT)
```

**ë¬¸ì œ:**
- í•™ìƒ 100ëª… â†’ ìµœì†Œ 200-300ë²ˆì˜ ì¿¼ë¦¬
- ê° ì¿¼ë¦¬ê°€ ê°œë³„ íŠ¸ëœì­ì…˜
- DB CPU ì§‘ì•½ì 

### âš ï¸ ìš´ì˜ ë¦¬ìŠ¤í¬ ë° ê°œì„  ì‚¬í•­

#### 1. Q OR ì¡°ê±´ ë°©ì‹ì˜ ìœ„í—˜ì„±
**ë¬¸ì œ:**
- `Q()` OR ì¡°ê±´ì„ 500ê°œ ë¶™ì´ë©´ ë¹„íš¨ìœ¨ì ì¸ execution plan ìƒì„±
- Indexë¥¼ ì˜ íƒ€ì§€ ì•Šì„ ìˆ˜ ìˆìŒ

**ê°œì„ :**
- Tuple IN ë°©ì‹ ì‚¬ìš© (Postgres composite index í™œìš©)
- ë˜ëŠ” Raw SQLë¡œ ìµœì í™”

### âœ… ìµœì í™” ì „ëµ

#### 1. ë°°ì¹˜ ì¡°íšŒ (ê¸°ì¡´ í•™ìƒ ì¼ê´„ ì¡°íšŒ)

**Before:**
```python
for row in students_data:
    existing = student_repo.student_filter_tenant_name_parent_phone_active(
        tenant, name, parent_phone
    )  # ê° í•™ìƒë§ˆë‹¤ SELECT
```

**After:**
```python
# ëª¨ë“  í•™ìƒì˜ (name, parent_phone) ìŒì„ í•œ ë²ˆì— ì¡°íšŒ
name_phone_pairs = [
    (normalize_name(row["name"]), normalize_phone(row["parent_phone"]))
    for row in students_data
]

# ë°°ì¹˜ë¡œ ê¸°ì¡´ í•™ìƒ ì¡°íšŒ (IN ì¿¼ë¦¬)
existing_students = student_repo.student_batch_filter_by_name_phone(
    tenant_id=tenant_id,
    name_phone_pairs=name_phone_pairs
)
existing_map = {
    (s.name, s.parent_phone): s
    for s in existing_students
}
```

**íš¨ê³¼:**
- 100ë²ˆ SELECT â†’ 1ë²ˆ SELECT
- ì¿¼ë¦¬ ìˆ˜ 99% ê°ì†Œ

#### 2. ë°°ì¹˜ ì¡°íšŒ (ì‚­ì œëœ í•™ìƒ ì¼ê´„ ì¡°íšŒ)

**Before:**
```python
for row in students_data:
    deleted_student = student_repo.student_filter_tenant_name_parent_phone_deleted(
        tenant, name, parent_phone
    )  # ê° í•™ìƒë§ˆë‹¤ SELECT
```

**After:**
```python
# ë°°ì¹˜ë¡œ ì‚­ì œëœ í•™ìƒ ì¡°íšŒ
deleted_students = student_repo.student_batch_filter_deleted_by_name_phone(
    tenant_id=tenant_id,
    name_phone_pairs=name_phone_pairs
)
deleted_map = {
    (s.name, s.parent_phone): s
    for s in deleted_students
}
```

**íš¨ê³¼:**
- 100ë²ˆ SELECT â†’ 1ë²ˆ SELECT
- ì¿¼ë¦¬ ìˆ˜ 99% ê°ì†Œ

#### 3. Bulk Create (ì‹ ê·œ í•™ìƒ ì¼ê´„ ìƒì„±)

**Before:**
```python
for row in students_data:
    if not existing and not deleted:
        student = Student.objects.create(...)  # ê° í•™ìƒë§ˆë‹¤ INSERT
```

**After:**
```python
# ì‹ ê·œ ìƒì„±í•  í•™ìƒë“¤ ìˆ˜ì§‘
new_students = []
for row in students_data:
    name, parent_phone = normalize_pair(row)
    if (name, parent_phone) not in existing_map and (name, parent_phone) not in deleted_map:
        new_students.append(Student(...))

# ë°°ì¹˜ë¡œ ì¼ê´„ ìƒì„±
if new_students:
    Student.objects.bulk_create(new_students, batch_size=500)
```

**íš¨ê³¼:**
- 100ë²ˆ INSERT â†’ 1ë²ˆ INSERT
- ì¿¼ë¦¬ ìˆ˜ 99% ê°ì†Œ

#### 4. íŠ¸ëœì­ì…˜ ìµœì í™”

**Before:**
```python
for row in students_data:
    with transaction.atomic():  # ê° í•™ìƒë§ˆë‹¤ íŠ¸ëœì­ì…˜
        student, created = get_or_create_student_for_lecture_enroll(...)
```

**After:**
```python
with transaction.atomic():  # ì „ì²´ë¥¼ í•˜ë‚˜ì˜ íŠ¸ëœì­ì…˜
    # 1. ë°°ì¹˜ ì¡°íšŒ (ê¸°ì¡´ + ì‚­ì œëœ)
    existing_map = batch_fetch_existing(...)
    deleted_map = batch_fetch_deleted(...)
    
    # 2. ë³µì›í•  í•™ìƒë“¤ ë°°ì¹˜ ì—…ë°ì´íŠ¸
    bulk_restore_deleted(deleted_map.values())
    
    # 3. ì‹ ê·œ í•™ìƒë“¤ ë°°ì¹˜ ìƒì„±
    bulk_create_new(new_students)
```

**íš¨ê³¼:**
- íŠ¸ëœì­ì…˜ ì˜¤ë²„í—¤ë“œ ê°ì†Œ
- ì¼ê´€ì„± ë³´ì¥

### ğŸ“Š ì˜ˆìƒ íš¨ê³¼

#### Before (í˜„ì¬)
- í•™ìƒ 100ëª… ë“±ë¡
- ì¿¼ë¦¬ ìˆ˜: ì•½ 200-300ë²ˆ
- ì‹¤í–‰ ì‹œê°„: 10-30ì´ˆ
- RDS CPU: 80-100%

#### After (ìµœì í™” í›„)
- í•™ìƒ 100ëª… ë“±ë¡
- ì¿¼ë¦¬ ìˆ˜: ì•½ 3-5ë²ˆ (ë°°ì¹˜ ì¡°íšŒ 2ë²ˆ + bulk_create 1ë²ˆ)
- ì‹¤í–‰ ì‹œê°„: 1-3ì´ˆ
- RDS CPU: 20-40%

**ì¿¼ë¦¬ ìˆ˜ 99% ê°ì†Œ**

### ğŸ”§ êµ¬í˜„ ì„¤ê³„

#### 1. Repositoryì— ë°°ì¹˜ ì¡°íšŒ ë©”ì„œë“œ ì¶”ê°€ (ìµœì í™” ë²„ì „)

**íŒŒì¼**: `academy/adapters/db/django/repositories_students.py`

```python
def student_batch_filter_by_name_phone(
    self,
    tenant_id: int,
    name_phone_pairs: list[tuple[str, str]],
) -> list[Student]:
    """ë°°ì¹˜ë¡œ ê¸°ì¡´ í™œì„± í•™ìƒ ì¡°íšŒ (Tuple IN ë°©ì‹, Index í™œìš©)"""
    if not name_phone_pairs:
        return []
    
    from apps.domains.students.models import Student
    from django.db import connection
    
    # âœ… Tuple IN ë°©ì‹ (Postgres composite index í™œìš©)
    # WHERE (tenant_id, name, parent_phone) IN ((...), (...), ...)
    # Index: idx_student_tenant_name_phone (tenant_id, name, parent_phone)
    
    if len(name_phone_pairs) > 1000:
        # ëŒ€ëŸ‰ ë°ì´í„°ëŠ” chunkë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬
        results = []
        for chunk in [name_phone_pairs[i:i+1000] for i in range(0, len(name_phone_pairs), 1000)]:
            results.extend(self._student_batch_filter_chunk(tenant_id, chunk))
        return results
    
    return self._student_batch_filter_chunk(tenant_id, name_phone_pairs)


def _student_batch_filter_chunk(
    self,
    tenant_id: int,
    name_phone_pairs: list[tuple[str, str]],
) -> list[Student]:
    """Chunk ë‹¨ìœ„ ë°°ì¹˜ ì¡°íšŒ (Raw SQLë¡œ ìµœì í™”)"""
    from apps.domains.students.models import Student
    from django.db import connection
    
    if not name_phone_pairs:
        return []
    
    # âœ… Raw SQLë¡œ Tuple IN ì¿¼ë¦¬ (Index í™œìš©)
    placeholders = ','.join(['(%s, %s, %s)'] * len(name_phone_pairs))
    values = []
    for name, parent_phone in name_phone_pairs:
        values.extend([tenant_id, name, parent_phone])
    
    query = f"""
        SELECT * FROM students
        WHERE (tenant_id, name, parent_phone) IN ({placeholders})
        AND deleted_at IS NULL
    """
    
    with connection.cursor() as cursor:
        cursor.execute(query, values)
        columns = [col[0] for col in cursor.description]
        return [
            Student(**dict(zip(columns, row)))
            for row in cursor.fetchall()
        ]


def student_batch_filter_deleted_by_name_phone(
    self,
    tenant_id: int,
    name_phone_pairs: list[tuple[str, str]],
) -> list[Student]:
    """ë°°ì¹˜ë¡œ ì‚­ì œëœ í•™ìƒ ì¡°íšŒ (IN ì¿¼ë¦¬)"""
    if not name_phone_pairs:
        return []
    
    from django.db.models import Q
    from apps.domains.students.models import Student
    
    conditions = Q()
    for name, parent_phone in name_phone_pairs:
        conditions |= Q(tenant_id=tenant_id, name=name, parent_phone=parent_phone, deleted_at__isnull=False)
    
    return list(Student.objects.filter(conditions))
```

#### 2. Bulk Create í•¨ìˆ˜ êµ¬í˜„

**íŒŒì¼**: `apps/domains/students/services/bulk_from_excel.py` (ìˆ˜ì •)

```python
def bulk_create_students_from_excel_rows_optimized(
    *,
    tenant_id: int,
    students_data: list[dict],
    initial_password: str,
    on_row_progress: Callable[[int, int], None] | None = None,
) -> dict:
    """
    ì—‘ì…€ íŒŒì‹±ëœ í–‰ìœ¼ë¡œ í•™ìƒ ì¼ê´„ ìƒì„± (ìµœì í™” ë²„ì „)
    - ë°°ì¹˜ ì¡°íšŒë¡œ ì¿¼ë¦¬ ìˆ˜ ìµœì†Œí™”
    - bulk_createë¡œ ì¼ê´„ ìƒì„±
    """
    from django.db import transaction
    from academy.adapters.db.django import repositories_enrollment as enroll_repo
    from academy.adapters.db.django import repositories_students as student_repo
    from apps.domains.students.models import Student
    from apps.core.models import TenantMembership
    from .lecture_enroll import _normalize_phone, _grade_value, normalize_school_from_name
    from ..ps_number import _generate_unique_ps_number
    from apps.domains.parents.services import ensure_parent_for_student
    
    tenant = enroll_repo.get_tenant_by_id(tenant_id)
    if not tenant:
        raise ValueError("tenant_id not found")
    
    initial_password = (initial_password or "").strip()
    if len(initial_password) < 4:
        raise ValueError("initial_passwordëŠ” 4ì ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
    
    total = len(students_data)
    created_count = 0
    failed: list[dict] = []
    
    # âœ… 1. ëª¨ë“  í•™ìƒì˜ (name, parent_phone) ìŒ ì •ê·œí™”
    normalized_pairs = []
    valid_rows = []
    
    for row_index, raw in enumerate(students_data, start=1):
        item = dict(raw) if isinstance(raw, dict) else {}
        name = (item.get("name") or "").strip()
        parent_phone = _normalize_phone(item.get("parent_phone") or "")
        
        if not parent_phone or len(parent_phone) != 11 or not parent_phone.startswith("010"):
            failed.append({
                "row": row_index,
                "name": name or "(ì´ë¦„ ì—†ìŒ)",
                "error": "í•™ë¶€ëª¨ ì „í™”ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            })
            continue
        
        if not name:
            failed.append({
                "row": row_index,
                "name": "(ì´ë¦„ ì—†ìŒ)",
                "error": "ì´ë¦„ì´ í•„ìš”í•©ë‹ˆë‹¤.",
            })
            continue
        
        normalized_pairs.append((name, parent_phone))
        valid_rows.append((row_index, item))
    
    if not normalized_pairs:
        return {
            "created": 0,
            "failed": failed,
            "total": total,
            "processed_by": "worker",
        }
    
    # âœ… 2. ë°°ì¹˜ë¡œ ê¸°ì¡´ í™œì„± í•™ìƒ ì¡°íšŒ
    existing_students = student_repo.student_batch_filter_by_name_phone(
        tenant_id=tenant_id,
        name_phone_pairs=normalized_pairs
    )
    existing_map = {
        (s.name, s.parent_phone): s
        for s in existing_students
    }
    
    # âœ… 3. ë°°ì¹˜ë¡œ ì‚­ì œëœ í•™ìƒ ì¡°íšŒ
    deleted_students = student_repo.student_batch_filter_deleted_by_name_phone(
        tenant_id=tenant_id,
        name_phone_pairs=normalized_pairs
    )
    deleted_map = {
        (s.name, s.parent_phone): s
        for s in deleted_students
    }
    
    # âœ… 4. Chunked íŠ¸ëœì­ì…˜ìœ¼ë¡œ ì¼ê´„ ì²˜ë¦¬ (ìš´ì˜ ì•ˆì •ì„±)
    # í•˜ë‚˜ì˜ giant transaction ëŒ€ì‹  chunk ë‹¨ìœ„ë¡œ ì²˜ë¦¬
    # â†’ ì¤‘ê°„ ì‹¤íŒ¨ ì‹œ ì „ì²´ ë¡¤ë°± ë°©ì§€, lock ì‹œê°„ ë‹¨ì¶•
    
    CHUNK_SIZE = 200  # ìš´ì˜ ì•ˆì •ì„±ì„ ìœ„í•œ chunk í¬ê¸°
    
    restored_students = []
    created_count = 0
    
    # 4-1. ì‚­ì œëœ í•™ìƒ ë³µì› (chunk ë‹¨ìœ„)
    deleted_items = list(deleted_map.items())
    for chunk in [deleted_items[i:i+CHUNK_SIZE] for i in range(0, len(deleted_items), CHUNK_SIZE)]:
        with transaction.atomic():
            for (name, parent_phone), deleted_student in chunk:
                try:
                    deleted_student.deleted_at = None
                    # ... ì—…ë°ì´íŠ¸ í•„ë“œ ì„¤ì •
                    deleted_student.save(update_fields=["deleted_at", ...])
                    TenantMembership.ensure_active(
                        tenant=tenant,
                        user=deleted_student.user,
                        role="student",
                    )
                    restored_students.append((name, parent_phone))
                except Exception as e:
                    logger.warning("Failed to restore deleted student: %s", e)
                    failed.append({
                        "row": next((r[0] for r in valid_rows if (r[1].get("name"), _normalize_phone(r[1].get("parent_phone"))) == (name, parent_phone)), 0),
                        "name": name,
                        "error": f"ë³µì› ì‹¤íŒ¨: {str(e)[:500]}",
                    })
    
    # 4-2. ì‹ ê·œ í•™ìƒ ìˆ˜ì§‘
    new_students_data = []
    for idx, (row_index, item) in enumerate(valid_rows):
            if on_row_progress and total > 0:
                on_row_progress(idx + 1, total)
            
            name = (item.get("name") or "").strip()
            parent_phone = _normalize_phone(item.get("parent_phone") or "")
            key = (name, parent_phone)
            
            # ì´ë¯¸ ì¡´ì¬í•˜ê±°ë‚˜ ë³µì›ëœ í•™ìƒì€ ìŠ¤í‚µ
            if key in existing_map or key in restored_students:
                continue
            
            # ì‹ ê·œ í•™ìƒ ìƒì„± (ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë§Œ ìƒì„±, ì•„ì§ ì €ì¥ ì•ˆ í•¨)
            try:
                phone_raw = item.get("phone")
                phone = _normalize_phone(phone_raw) if phone_raw else None
                if phone and (len(phone) != 11 or not phone.startswith("010")):
                    phone = None
                
                school_val = (item.get("school") or "").strip() or None
                school_type = None
                high_school = None
                middle_school = None
                if school_val:
                    school_type, high_school, middle_school = normalize_school_from_name(
                        school_val, item.get("school_type")
                    )
                
                student = Student(
                    tenant_id=tenant_id,
                    name=name,
                    parent_phone=parent_phone,
                    phone=phone,
                    school_type=school_type,
                    high_school=high_school,
                    middle_school=middle_school,
                    high_school_class=(
                        (item.get("high_school_class") or "").strip() or None
                        if school_type == "HIGH"
                        else None
                    ),
                    major=(
                        (item.get("major") or "").strip() or None
                        if school_type == "HIGH"
                        else None
                    ),
                    grade=_grade_value(item.get("grade")),
                    memo=(item.get("memo") or "").strip() or None,
                    gender=(
                        (item.get("gender") or "").strip().upper()[:1] or None
                        if item.get("gender")
                        else None
                    ),
                    ps_number=_generate_unique_ps_number(tenant_id),
                )
                new_students.append(student)
            except Exception as e:
                failed.append({
                    "row": row_index,
                    "name": name or "(ì´ë¦„ ì—†ìŒ)",
                    "error": str(e)[:500],
                })
        
    # 4-3. ì‹ ê·œ í•™ìƒ Bulk Create (chunk ë‹¨ìœ„)
    # âœ… ê°œì„ : bulk_create í›„ ê°œë³„ save ì œê±°
    # â†’ User, Parentë„ bulk ì²˜ë¦¬ë¡œ ë³€ê²½
    
    for chunk in [new_students_data[i:i+CHUNK_SIZE] for i in range(0, len(new_students_data), CHUNK_SIZE)]:
        with transaction.atomic():
            try:
                # Chunk ë‚´ í•™ìƒë“¤ ìƒì„±
                students_to_create = []
                for student_data in chunk:
                    students_to_create.append(student_data["student"])
                
                # âœ… Bulk Createë¡œ ì¼ê´„ ìƒì„±
                Student.objects.bulk_create(students_to_create, batch_size=500)
                
                # âœ… User Bulk Create (FK ì—°ê²°)
                users_to_create = []
                for student_data, student in zip(chunk, students_to_create):
                    from apps.core.models import User
                    user = User(
                        username=f"student_{student.ps_number}",
                    )
                    user.set_password(initial_password)
                    users_to_create.append(user)
                
                User.objects.bulk_create(users_to_create, batch_size=500)
                
                # âœ… Student FK ì—…ë°ì´íŠ¸ (bulk_update)
                student_user_map = {
                    s.ps_number: u
                    for s, u in zip(students_to_create, users_to_create)
                }
                for student in students_to_create:
                    student.user = student_user_map[student.ps_number]
                
                Student.objects.bulk_update(
                    students_to_create,
                    ["user"],
                    batch_size=500
                )
                
                # âœ… TenantMembership Bulk Create
                memberships_to_create = []
                for student in students_to_create:
                    memberships_to_create.append(
                        TenantMembership(
                            tenant=tenant,
                            user=student.user,
                            role="student",
                        )
                    )
                TenantMembership.objects.bulk_create(memberships_to_create, batch_size=500)
                
                # âœ… Parent ìƒì„± (ê°œë³„ ì²˜ë¦¬ í•„ìš” - ì™¸ë˜í‚¤ ê´€ê³„)
                for student_data in chunk:
                    try:
                        ensure_parent_for_student(
                            student_data["student"],
                            student_data["parent_phone"]
                        )
                    except Exception as e:
                        logger.warning("Failed to create parent: %s", e)
                        failed.append({
                            "row": student_data["row_index"],
                            "name": student_data["student"].name,
                            "error": f"Parent ìƒì„± ì‹¤íŒ¨: {str(e)[:500]}",
                        })
                
                created_count += len(students_to_create)
                
            except Exception as e:
                logger.exception("Failed to process chunk: %s", e)
                # Chunk ì‹¤íŒ¨ ì‹œ í•´ë‹¹ chunkë§Œ ì‹¤íŒ¨ ì²˜ë¦¬
                for student_data in chunk:
                    failed.append({
                        "row": student_data["row_index"],
                        "name": student_data.get("name", "(ì´ë¦„ ì—†ìŒ)"),
                        "error": f"ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)[:500]}",
                    })
    
    return {
        "created": created_count,
        "failed": failed,
        "total": total,
        "processed_by": "worker",
    }
```

### ğŸ“ˆ ìµœì í™” íš¨ê³¼ ë¹„êµ

| í•­ëª© | Before | After | ê°œì„ ìœ¨ |
|------|--------|-------|--------|
| ì¿¼ë¦¬ ìˆ˜ (100ëª…) | 200-300ë²ˆ | 3-5ë²ˆ | **99% ê°ì†Œ** |
| ì‹¤í–‰ ì‹œê°„ | 10-30ì´ˆ | 1-3ì´ˆ | **90% ê°ì†Œ** |
| RDS CPU | 80-100% | 20-40% | **60% ê°ì†Œ** |
| íŠ¸ëœì­ì…˜ ìˆ˜ | 100ê°œ | 1ê°œ | **99% ê°ì†Œ** |

### âš ï¸ ì£¼ì˜ì‚¬í•­

1. **ì™¸ë˜í‚¤ ê´€ê³„**
   - User, Parent ìƒì„±ì€ ê°œë³„ ì²˜ë¦¬ í•„ìš”
   - bulk_create í›„ ê°œë³„ ì—…ë°ì´íŠ¸ í•„ìš”

2. **ì—ëŸ¬ ì²˜ë¦¬**
   - ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì¼ë¶€ ì‹¤íŒ¨ ì‹œ ë¡¤ë°± ì „ëµ í•„ìš”
   - ë¶€ë¶„ ì„±ê³µ í—ˆìš© ì—¬ë¶€ ê²°ì •

3. **ë©”ëª¨ë¦¬ ì‚¬ìš©**
   - ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œ ë©”ëª¨ë¦¬ ê³ ë ¤
   - ë°°ì¹˜ í¬ê¸° ì¡°ì • (batch_size)

## ğŸ¯ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### í•„ìˆ˜ (DB í´ë§ ì œê±°)
- [ ] Redis í‚¤ í—¬í¼ ìƒì„± (Tenant ë„¤ì„ìŠ¤í˜ì´ìŠ¤)
- [ ] Progress/Status ì „ìš© endpoint ì¶”ê°€ (Redis-only)
- [ ] ì›Œì»¤ ì™„ë£Œ ì‹œ Redis ì €ì¥ (TTL ì—†ìŒ, result í¬í•¨)
- [ ] í”„ë¡ íŠ¸ì—”ë“œ í´ë§ ì „í™˜ (progress endpointë§Œ ì‚¬ìš©)
- [ ] ì§„í–‰ ì¤‘ ì‘ì—…: DB ì¡°íšŒ ì™„ì „ ì œê±°

### í•„ìˆ˜ (ì—‘ì…€ ëŒ€ëŸ‰ ì¿¼ë¦¬ ìµœì í™”)
- [ ] ë°°ì¹˜ ì¡°íšŒ ë©”ì„œë“œ ì¶”ê°€ (ê¸°ì¡´ í•™ìƒ, ì‚­ì œëœ í•™ìƒ)
- [ ] Bulk Create í•¨ìˆ˜ êµ¬í˜„
- [ ] íŠ¸ëœì­ì…˜ ìµœì í™” (ì „ì²´ë¥¼ í•˜ë‚˜ì˜ íŠ¸ëœì­ì…˜)
- [ ] ê¸°ì¡´ ì½”ë“œì™€ í˜¸í™˜ì„± ìœ ì§€ (ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜)

### ê¶Œì¥ (ì„±ëŠ¥ ìµœì í™”)
- [ ] ì ì‘í˜• í´ë§ ê°„ê²© êµ¬í˜„
- [ ] DB_CONN_MAX_AGE 15~20 ì¡°ì •
- [ ] ì‹œì²­ ë¡œê·¸/ì •ì±„ íŒë‹¨ í”„ë¡œê·¸ë˜ìŠ¤ë°” DB ì¡°íšŒ ì œê±° í™•ì¸

### ëª¨ë‹ˆí„°ë§
- [ ] DB ì¿¼ë¦¬ ìˆ˜ ëª¨ë‹ˆí„°ë§ (í´ë§ ì œê±° í™•ì¸)
- [ ] ì—‘ì…€ íŒŒì‹± ì¿¼ë¦¬ ìˆ˜ ëª¨ë‹ˆí„°ë§ (ë°°ì¹˜ ì²˜ë¦¬ í™•ì¸)
- [ ] Redis ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
- [ ] RDS CPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§ (80% â†’ 20% ëª©í‘œ)
