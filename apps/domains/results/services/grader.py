# apps/domains/results/services/grader.py
from __future__ import annotations

from typing import Any, Dict, List, Optional, Tuple

from django.db import transaction

# ======================================================
# ğŸ”½ submissions ë„ë©”ì¸ (raw input)
# ======================================================
from apps.domains.submissions.models import Submission, SubmissionAnswer

# ======================================================
# ğŸ”½ results ë„ë©”ì¸ (apply / attempt)
# ======================================================
from apps.domains.results.services.applier import ResultApplier
from apps.domains.results.services.attempt_service import ExamAttemptService

# ======================================================
# ğŸ”½ exams ë„ë©”ì¸ (ì •ë‹µ / ë¬¸ì œ ì •ì˜)
# ======================================================
from apps.domains.exams.models import ExamQuestion, AnswerKey
# (ì„ íƒ) pass_scoreë¥¼ Examì—ì„œ ì½ì„ ìˆ˜ ìˆìœ¼ë©´ ì“°ê³ , ì—†ìœ¼ë©´ ì•ˆì „í•˜ê²Œ ìŠ¤í‚µ
try:
    from apps.domains.exams.models import Exam  # type: ignore
except Exception:  # pragma: no cover
    Exam = None  # type: ignore

# ======================================================
# ğŸ”½ progress pipeline (side-effect)
# ======================================================
from apps.domains.progress.tasks.progress_pipeline_task import (
    run_progress_pipeline_task,
)

# ======================================================
# Constants (STEP 1 ê³ ì •)
# ======================================================
OMR_CONF_THRESHOLD_V1 = 0.70


# ======================================================
# Utils
# ======================================================
def _norm(s: Optional[str]) -> str:
    """
    ë¬¸ìì—´ ì •ê·œí™” (STEP 1 exact match ê³ ì •):
    - None ë°©ì–´
    - ê³µë°± ì œê±°
    - ëŒ€ë¬¸ì í†µì¼
    """
    return (s or "").strip().upper()


def _get_omr_meta(meta: Any) -> Dict[str, Any]:
    """
    submissions.SubmissionAnswer.meta ì—ì„œ
    omr dict ë§Œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
    """
    if not isinstance(meta, dict):
        return {}
    omr = meta.get("omr")
    return omr if isinstance(omr, dict) else {}


def _ensure_dict(v: Any) -> Dict[str, Any]:
    return v if isinstance(v, dict) else {}


def _with_invalid_reason(meta: Any, reason: str) -> Dict[str, Any]:
    """
    âœ… STEP 1 í•µì‹¬:
    low_conf / blank / multi ë“± "ë¬´íš¨ ì²˜ë¦¬"ëŠ” 0ì  ì²˜ë¦¬ ë¿ ì•„ë‹ˆë¼
    **ì‚¬ìœ ë¥¼ append-onlyë¡œ ë‚¨ê²¨ì•¼ ìš´ì˜/ì¬ì²˜ë¦¬/í”„ë¡ íŠ¸ í‘œì‹œê°€ ê°€ëŠ¥**í•´ì§.
    """
    base = _ensure_dict(meta)
    out = dict(base)
    out.setdefault("grading", {})
    if isinstance(out["grading"], dict):
        out["grading"]["invalid_reason"] = reason
    return out


# ======================================================
# Grading helpers
# ======================================================
def _grade_choice_v1(
    *,
    detected: List[str],
    marking: str,
    confidence: Optional[float],
    status: str,
    correct_answer: str,
    max_score: float,
    # âœ… ê¸°ì¡´ metaë¥¼ ë°›ì•„ì„œ invalid_reasonì„ ì‹¬ëŠ”ë‹¤
    original_meta: Any,
) -> Tuple[bool, float, Dict[str, Any]]:
    """
    OMR ê°ê´€ì‹ ì±„ì  v1 (STEP 1 ê³ ì •)

    âœ… ì •ì±…:
    - status != ok -> ë¬´íš¨ (0ì )
    - marking blank/multi -> ë¬´íš¨ (0ì )
    - confidence < threshold -> ë¬´íš¨ (0ì ) + LOW_CONFIDENCE ì‚¬ìœ  ì €ì¥  â­â­â­
    - detected != 1ê°œ -> ë¬´íš¨ (0ì )
    """
    st = (status or "").lower()
    mk = (marking or "").lower()

    # 1) statusê°€ okê°€ ì•„ë‹ˆë©´ ë¬´íš¨
    if st != "ok":
        return False, 0.0, _with_invalid_reason(original_meta, "OMR_STATUS_NOT_OK")

    # 2) blank/multiëŠ” ë¬´íš¨
    if mk in ("blank", "multi"):
        reason = "OMR_BLANK" if mk == "blank" else "OMR_MULTI"
        return False, 0.0, _with_invalid_reason(original_meta, reason)

    # 3) ì‹ ë¢°ë„ ì²´í¬ (STEP 1: low confidence ìë™ 0ì  + ì‚¬ìœ  ì €ì¥)
    conf = float(confidence) if confidence is not None else 0.0
    if conf < OMR_CONF_THRESHOLD_V1:
        return False, 0.0, _with_invalid_reason(original_meta, "LOW_CONFIDENCE")

    # 4) detected 1ê°œ ê°•ì œ
    if not detected or len(detected) != 1:
        return False, 0.0, _with_invalid_reason(original_meta, "OMR_DETECTED_INVALID")

    ans = _norm(detected[0])
    cor = _norm(correct_answer)

    is_correct = ans != "" and cor != "" and ans == cor
    return is_correct, (float(max_score) if is_correct else 0.0), _ensure_dict(original_meta)


def _grade_short_v1(
    *,
    answer_text: str,
    correct_answer: str,
    max_score: float,
    original_meta: Any,
) -> Tuple[bool, float, Dict[str, Any]]:
    """
    ì£¼ê´€ì‹ / fallback ì±„ì  (STEP 1: exact match)

    âœ… ì •ì±…:
    - empty => 0ì 
    - exact match only
    """
    ans = _norm(answer_text)
    cor = _norm(correct_answer)

    if ans == "":
        return False, 0.0, _with_invalid_reason(original_meta, "EMPTY_ANSWER")

    is_correct = cor != "" and ans == cor
    return is_correct, (float(max_score) if is_correct else 0.0), _ensure_dict(original_meta)


def _infer_answer_type(q: ExamQuestion) -> str:
    """
    ExamQuestion.answer_type ì¶”ë¡ 
    """
    v = getattr(q, "answer_type", None)
    if isinstance(v, str) and v.strip():
        return v.strip().lower()
    return "choice"


def _get_correct_answer_map_v2(exam_id: int) -> Dict[str, Any]:
    """
    âœ… AnswerKey v2 ê³ ì •

    answers = {
        "123": "B",
        "124": "D"
    }

    key == ExamQuestion.id (string)
    """
    ak = AnswerKey.objects.filter(exam_id=int(exam_id)).first()
    if not ak or not isinstance(ak.answers, dict):
        return {}
    return ak.answers


def _get_pass_score(exam_id: int) -> Optional[float]:
    """
    (ì„ íƒ) Exam.pass_scoreê°€ ìˆìœ¼ë©´ ì½ì–´ì„œ attempt/metaì— ê¸°ë¡.
    - ResultApplierê°€ ì´ë¯¸ is_passë¥¼ ê³„ì‚°í•œë‹¤ë©´ ì´ê±´ "ì§„ë‹¨/í‘œì‹œìš©" ì •ë³´ë¡œë§Œ ë‚¨ëŠ”ë‹¤.
    """
    if Exam is None:
        return None
    try:
        exam = Exam.objects.filter(id=int(exam_id)).first()
        if not exam:
            return None
        v = getattr(exam, "pass_score", None)
        return float(v) if v is not None else None
    except Exception:
        return None


# ======================================================
# Main grading pipeline
# ======================================================
@transaction.atomic
def grade_submission_to_results(submission: Submission) -> None:
    """
    Submission â†’ ExamAttempt â†’ Result / ResultItem / ResultFact

    ğŸ”¥ v2 í•µì‹¬ ê³„ì•½:
    - SubmissionAnswer.exam_question_id ë§Œ ì‚¬ìš©
    - number / fallback ì™„ì „ ì œê±°
    - AnswerKey v2 ê³ ì •
    - âœ… STEP 1: LOW_CONF ë¬´íš¨ 0ì  + ì‚¬ìœ  ì €ì¥
    """

    # --------------------------------------------------
    # 0ï¸âƒ£ Submission ìƒíƒœ ì „ì´
    # --------------------------------------------------
    submission.status = Submission.Status.GRADING
    if hasattr(submission, "error_message"):
        submission.error_message = ""
        submission.save(update_fields=["status", "error_message"])
    else:
        submission.save(update_fields=["status"])

    if submission.target_type != Submission.TargetType.EXAM:
        raise ValueError("Only exam grading is supported")

    attempt = None

    try:
        # --------------------------------------------------
        # 1ï¸âƒ£ ExamAttempt ìƒì„±
        # --------------------------------------------------
        attempt = ExamAttemptService.create_for_submission(
            exam_id=int(submission.target_id),
            enrollment_id=int(submission.enrollment_id),
            submission_id=int(submission.id),
        )
        attempt.status = "grading"
        attempt.save(update_fields=["status"])

        # --------------------------------------------------
        # 2ï¸âƒ£ Raw answers
        # --------------------------------------------------
        answers = list(
            SubmissionAnswer.objects.filter(submission=submission)
        )

        # --------------------------------------------------
        # 3ï¸âƒ£ ExamQuestion ë¡œë”© (id ê¸°ì¤€)
        # --------------------------------------------------
        questions_by_id = (
            ExamQuestion.objects
            .filter(sheet__exam_id=submission.target_id)
            .in_bulk(field_name="id")
        )

        correct_map = _get_correct_answer_map_v2(int(submission.target_id))

        items: List[dict] = []

        total_score = 0.0
        total_max_score = 0.0

        # --------------------------------------------------
        # 4ï¸âƒ£ ë¬¸í•­ë³„ ì±„ì 
        # --------------------------------------------------
        for sa in answers:
            eqid = getattr(sa, "exam_question_id", None)
            if not eqid:
                continue

            try:
                q = questions_by_id.get(int(eqid))
            except (TypeError, ValueError):
                continue

            if not q:
                continue

            max_score = float(getattr(q, "score", 0) or 0.0)
            correct_answer = str(correct_map.get(str(q.id)) or "")

            answer_text = str(getattr(sa, "answer", "") or "").strip()

            # submissions meta
            original_meta = getattr(sa, "meta", None)
            omr = _get_omr_meta(original_meta)

            detected = omr.get("detected") or []
            marking = str(omr.get("marking") or "")
            confidence = omr.get("confidence", None)
            status = str(omr.get("status") or "")
            omr_version = str(omr.get("version") or "")

            # âœ… STEP 1: low_confidence statusëŠ” ì¦‰ì‹œ ë¬´íš¨ ì²˜ë¦¬ (0ì +ì‚¬ìœ )
            if (status or "").lower() == "low_confidence":
                is_correct = False
                score = 0.0
                final_answer = ""
                final_meta = _with_invalid_reason(original_meta, "LOW_CONFIDENCE")
            else:
                answer_type = _infer_answer_type(q)

                if answer_type in ("choice", "omr", "multiple_choice"):
                    if omr_version.lower() in ("v1", "v2"):
                        is_correct, score, final_meta = _grade_choice_v1(
                            detected=[str(x) for x in detected],
                            marking=marking,
                            confidence=confidence,
                            status=status,
                            correct_answer=correct_answer,
                            max_score=max_score,
                            original_meta=original_meta,
                        )
                        # í‘œì‹œìš© answer: ê°ì§€ëœ ê°’ 1ê°œë©´ ê·¸ ê°’, ì•„ë‹ˆë©´ ""
                        final_answer = (
                            "".join([_norm(x) for x in detected]) if detected else ""
                        )
                    else:
                        # OMR metaê°€ ì—†ê±°ë‚˜ ë²„ì „ì´ ì—†ì„ ë•Œ: í…ìŠ¤íŠ¸ ê¸°ë°˜ exact match
                        is_correct, score, final_meta = _grade_short_v1(
                            answer_text=answer_text,
                            correct_answer=correct_answer,
                            max_score=max_score,
                            original_meta=original_meta,
                        )
                        final_answer = answer_text
                else:
                    # subjective: exact match (STEP 1)
                    is_correct, score, final_meta = _grade_short_v1(
                        answer_text=answer_text,
                        correct_answer=correct_answer,
                        max_score=max_score,
                        original_meta=original_meta,
                    )
                    final_answer = answer_text

            # ì ìˆ˜ ëˆ„ì 
            total_score += float(score)
            total_max_score += float(max_score)

            items.append({
                "question_id": q.id,
                "answer": final_answer,
                "is_correct": bool(is_correct),
                "score": float(score),
                "max_score": float(max_score),
                "source": submission.source,
                # âœ… ìµœì¢… metaì—ëŠ” invalid_reasonì´ ë°˜ì˜ë  ìˆ˜ ìˆìŒ
                "meta": final_meta,
            })

        # --------------------------------------------------
        # 4-1) (ì„ íƒ) attempt/metaì— total/pass ì •ë³´ ê¸°ë¡
        # - ResultApplierê°€ ì‹¤ì œ ResultSummary.is_passë¥¼ ë§Œë“¤ë”ë¼ë„,
        #   attemptì—ëŠ” ìš´ì˜/ë””ë²„ê¹…ìš©ìœ¼ë¡œ ë‚¨ê²¨ë‘ë©´ ì¢‹ìŒ.
        # --------------------------------------------------
        try:
            pass_score = _get_pass_score(int(submission.target_id))
            meta = getattr(attempt, "meta", None)
            if isinstance(meta, dict):
                new_meta = dict(meta)
            else:
                new_meta = {}

            new_meta.setdefault("grading", {})
            if isinstance(new_meta["grading"], dict):
                new_meta["grading"]["total_score"] = float(total_score)
                new_meta["grading"]["total_max_score"] = float(total_max_score)
                if pass_score is not None:
                    new_meta["grading"]["pass_score"] = float(pass_score)
                    new_meta["grading"]["is_pass_inferred"] = bool(total_score >= pass_score)

            if hasattr(attempt, "meta"):
                attempt.meta = new_meta
                attempt.save(update_fields=["meta"])
        except Exception:
            # meta í•„ë“œê°€ ì—†ê±°ë‚˜ ì €ì¥ ì‹¤íŒ¨í•´ë„ grading ìì²´ëŠ” ê³„ì† ì§„í–‰
            pass

        # --------------------------------------------------
        # 5ï¸âƒ£ Result ë°˜ì˜
        # --------------------------------------------------
        ResultApplier.apply(
            target_type=submission.target_type,
            target_id=int(submission.target_id),
            enrollment_id=int(submission.enrollment_id),
            submission_id=int(submission.id),
            attempt_id=int(attempt.id),
            items=items,
        )

        # --------------------------------------------------
        # 6ï¸âƒ£ ìƒíƒœ ë§ˆë¬´ë¦¬
        # --------------------------------------------------
        attempt.status = "done"
        attempt.save(update_fields=["status"])

        submission.status = Submission.Status.DONE
        submission.save(update_fields=["status"])

        transaction.on_commit(
            lambda: run_progress_pipeline_task.delay(submission.id)
        )

    except Exception as e:
        if attempt:
            attempt.status = "failed"
            attempt.save(update_fields=["status"])

        submission.status = Submission.Status.FAILED
        if hasattr(submission, "error_message"):
            submission.error_message = str(e)[:2000]
            submission.save(update_fields=["status", "error_message"])
        else:
            submission.save(update_fields=["status"])
        raise
