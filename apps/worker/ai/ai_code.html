
<!DOCTYPE html>
<html lang="ko">
<head>
<meta charset="utf-8">
<title>ai Code Browser</title>
<style>
body {
    background:#0f1115;
    color:#eaeaea;
    font-family:Consolas, monospace;
    padding:24px;
}
h1 { margin-bottom:24px; }
h2 { color:#7dd3fc; font-size:15px; }
pre {
    background:#111418;
    padding:16px;
    border-radius:8px;
    overflow-x:auto;
    font-size:13px;
    line-height:1.5;
}
section { margin-bottom:32px; }
</style>
</head>
<body>
<h1>üì¶ apps\worker\ai</h1>

        <section>
            <h2>__init__.py</h2>
            <pre><code># apps/worker/ai/__init__.py
&quot;&quot;&quot;
Worker AI package

- Í≥ÑÏÇ∞ Ï†ÑÏö© (OCR/Segmentation/Handwriting/Embedding/ProblemGen/VideoAnalyze)
- DB Ï†ëÍ∑º/ÎπÑÏ¶àÎãàÏä§ Í∑úÏπô/Í∂åÌïú ÌåêÎã® Í∏àÏßÄ
&quot;&quot;&quot;
</code></pre>
        </section>
        
        <section>
            <h2>config.py</h2>
            <pre><code># apps/worker/ai/config.py
from __future__ import annotations

import os
from dataclasses import dataclass
from typing import List, Optional


def _env(name: str, default: Optional[str] = None) -&gt; Optional[str]:
    v = os.getenv(name)
    return v if v not in (None, &quot;&quot;) else default


@dataclass(frozen=True)
class AIConfig:
    # OCR
    OCR_ENGINE: str = &quot;google&quot;  # google | tesseract | auto
    GOOGLE_APPLICATION_CREDENTIALS: Optional[str] = None  # optional (google sdk default)

    # Segmentation
    QUESTION_SEGMENTATION_ENGINE: str = &quot;auto&quot;  # yolo|opencv|template|auto

    # YOLO (optional)
    YOLO_QUESTION_MODEL_PATH: Optional[str] = None
    YOLO_QUESTION_INPUT_SIZE: int = 640
    YOLO_QUESTION_CONF_THRESHOLD: float = 0.4
    YOLO_QUESTION_IOU_THRESHOLD: float = 0.5

    # Embedding
    EMBEDDING_BACKEND: str = &quot;auto&quot;  # local|openai|auto
    EMBEDDING_LOCAL_MODEL: str = &quot;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&quot;
    EMBEDDING_OPENAI_MODEL: str = &quot;text-embedding-3-small&quot;
    OPENAI_API_KEY: Optional[str] = None

    # Problem generation
    PROBLEM_GEN_MODEL: str = &quot;gpt-4.1-mini&quot;  # default

    @staticmethod
    def load() -&gt; &quot;AIConfig&quot;:
        return AIConfig(
            OCR_ENGINE=_env(&quot;OCR_ENGINE&quot;, &quot;google&quot;) or &quot;google&quot;,
            GOOGLE_APPLICATION_CREDENTIALS=_env(&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;),

            QUESTION_SEGMENTATION_ENGINE=_env(&quot;QUESTION_SEGMENTATION_ENGINE&quot;, &quot;auto&quot;) or &quot;auto&quot;,

            YOLO_QUESTION_MODEL_PATH=_env(&quot;YOLO_QUESTION_MODEL_PATH&quot;),
            YOLO_QUESTION_INPUT_SIZE=int(_env(&quot;YOLO_QUESTION_INPUT_SIZE&quot;, &quot;640&quot;) or &quot;640&quot;),
            YOLO_QUESTION_CONF_THRESHOLD=float(_env(&quot;YOLO_QUESTION_CONF_THRESHOLD&quot;, &quot;0.4&quot;) or &quot;0.4&quot;),
            YOLO_QUESTION_IOU_THRESHOLD=float(_env(&quot;YOLO_QUESTION_IOU_THRESHOLD&quot;, &quot;0.5&quot;) or &quot;0.5&quot;),

            EMBEDDING_BACKEND=_env(&quot;EMBEDDING_BACKEND&quot;, &quot;auto&quot;) or &quot;auto&quot;,
            EMBEDDING_LOCAL_MODEL=_env(
                &quot;EMBEDDING_LOCAL_MODEL&quot;,
                &quot;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&quot;,
            ) or &quot;sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2&quot;,
            EMBEDDING_OPENAI_MODEL=_env(&quot;EMBEDDING_OPENAI_MODEL&quot;, &quot;text-embedding-3-small&quot;) or &quot;text-embedding-3-small&quot;,
            OPENAI_API_KEY=_env(&quot;OPENAI_API_KEY&quot;) or _env(&quot;EMBEDDING_OPENAI_API_KEY&quot;),

            PROBLEM_GEN_MODEL=_env(&quot;PROBLEM_GEN_MODEL&quot;, &quot;gpt-4.1-mini&quot;) or &quot;gpt-4.1-mini&quot;,
        )
</code></pre>
        </section>
        
        <section>
            <h2>detection\__init__.py</h2>
            <pre><code># apps/worker/ai/detection/__init__.py
from __future__ import annotations
</code></pre>
        </section>
        
        <section>
            <h2>detection\segment_dispatcher.py</h2>
            <pre><code># apps/worker/ai/detection/segment_dispatcher.py
from __future__ import annotations

from typing import List, Tuple

from apps.worker.ai.config import AIConfig
from apps.worker.ai.detection.segment_opencv import segment_questions_opencv
from apps.worker.ai.detection.segment_yolo import segment_questions_yolo

BBox = Tuple[int, int, int, int]


def segment_questions(image_path: str) -&gt; List[BBox]:
    &quot;&quot;&quot;
    worker-side segmentation single entrypoint
    &quot;&quot;&quot;
    cfg = AIConfig.load()
    engine = (cfg.QUESTION_SEGMENTATION_ENGINE or &quot;auto&quot;).lower()

    if engine == &quot;opencv&quot;:
        return segment_questions_opencv(image_path)
    if engine == &quot;yolo&quot;:
        return segment_questions_yolo(image_path)

    # auto: yolo -&gt; opencv
    try:
        boxes = segment_questions_yolo(image_path)
        if boxes:
            return boxes
    except Exception:
        pass
    return segment_questions_opencv(image_path)
</code></pre>
        </section>
        
        <section>
            <h2>detection\segment_opencv.py</h2>
            <pre><code># apps/worker/ai/detection/segment_opencv.py
from __future__ import annotations

from typing import List, Tuple
import cv2  # type: ignore

BBox = Tuple[int, int, int, int]


def segment_questions_opencv(image_path: str) -&gt; List[BBox]:
    &quot;&quot;&quot;
    legacy ÏÑûÏó¨ÏûàÎçò opencv segmentation Ï†ïÎ¶¨Î≥∏
    ÏûÖÎ†•: image_path
    Ï∂úÎ†•: [(x,y,w,h), ...]
    &quot;&quot;&quot;
    image_bgr = cv2.imread(image_path)
    if image_bgr is None:
        return []

    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5, 5), 0)

    _, thresh = cv2.threshold(
        blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU
    )

    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))
    dilated = cv2.dilate(thresh, kernel, iterations=1)

    contours, _ = cv2.findContours(
        dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
    )

    h_img, w_img = gray.shape[:2]
    min_area = w_img * h_img * 0.005
    max_area = w_img * h_img * 0.9

    boxes: List[BBox] = []
    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)
        area = w * h
        if area &lt; min_area or area &gt; max_area:
            continue

        aspect = h / (w + 1e-6)
        if aspect &lt; 0.3:
            continue

        boxes.append((x, y, w, h))

    boxes.sort(key=lambda b: (b[1], b[0]))
    return boxes
</code></pre>
        </section>
        
        <section>
            <h2>detection\segment_yolo.py</h2>
            <pre><code># apps/worker/ai/detection/segment_yolo.py
from __future__ import annotations

from functools import lru_cache
from typing import List, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore

from apps.worker.ai.config import AIConfig

BBox = Tuple[int, int, int, int]


class YoloNotConfiguredError(RuntimeError):
    pass


try:
    import onnxruntime as ort  # type: ignore
    _HAS_ORT = True
except Exception:
    _HAS_ORT = False


@lru_cache()
def _get_session():
    cfg = AIConfig.load()

    if not _HAS_ORT:
        raise YoloNotConfiguredError(&quot;onnxruntime not installed&quot;)

    if not cfg.YOLO_QUESTION_MODEL_PATH:
        raise YoloNotConfiguredError(&quot;YOLO_QUESTION_MODEL_PATH not set&quot;)

    providers = [&quot;CPUExecutionProvider&quot;]
    return ort.InferenceSession(str(cfg.YOLO_QUESTION_MODEL_PATH), providers=providers)


def _preprocess(image_bgr, input_size: int):
    h0, w0 = image_bgr.shape[:2]
    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
    resized = cv2.resize(image_rgb, (input_size, input_size))
    resized = resized.astype(np.float32) / 255.0

    tensor = np.transpose(resized, (2, 0, 1))
    tensor = np.expand_dims(tensor, axis=0)

    scale_x = w0 / float(input_size)
    scale_y = h0 / float(input_size)
    return tensor, scale_x, scale_y


def _nms(boxes, scores, iou_threshold: float):
    if len(boxes) == 0:
        return []

    boxes = boxes.astype(np.float32)
    scores = scores.astype(np.float32)

    x1, y1, x2, y2 = boxes.T
    areas = (x2 - x1) * (y2 - y1)
    order = scores.argsort()[::-1]

    keep = []
    while order.size &gt; 0:
        i = int(order[0])
        keep.append(i)

        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])

        w = np.maximum(0.0, xx2 - xx1)
        h = np.maximum(0.0, yy2 - yy1)
        inter = w * h

        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-6)
        idxs = np.where(iou &lt;= iou_threshold)[0]
        order = order[idxs + 1]

    return keep


def segment_questions_yolo(image_path: str) -&gt; List[BBox]:
    cfg = AIConfig.load()
    sess = _get_session()

    image_bgr = cv2.imread(image_path)
    if image_bgr is None:
        return []

    input_tensor, scale_x, scale_y = _preprocess(image_bgr, cfg.YOLO_QUESTION_INPUT_SIZE)

    input_name = sess.get_inputs()[0].name
    outputs = sess.run(None, {input_name: input_tensor})
    preds = outputs[0]
    if preds.ndim == 3:
        preds = preds[0]

    boxes = []
    scores = []

    for det in preds:
        cx, cy, w, h, obj_conf = det[:5]
        cls_scores = det[5:]
        cls_conf = float(cls_scores.max()) if cls_scores.size &gt; 0 else 1.0

        score = float(obj_conf * cls_conf)
        if score &lt; cfg.YOLO_QUESTION_CONF_THRESHOLD:
            continue

        x1 = (cx - w / 2.0) * scale_x
        y1 = (cy - h / 2.0) * scale_y
        x2 = (cx + w / 2.0) * scale_x
        y2 = (cy + h / 2.0) * scale_y

        boxes.append([x1, y1, x2, y2])
        scores.append(score)

    if not boxes:
        return []

    boxes_np = np.array(boxes)
    scores_np = np.array(scores)

    keep_idx = _nms(boxes_np, scores_np, cfg.YOLO_QUESTION_IOU_THRESHOLD)

    final: List[BBox] = []
    for i in keep_idx:
        x1, y1, x2, y2 = boxes_np[i]
        final.append((int(x1), int(y1), int(x2 - x1), int(y2 - y1)))

    final.sort(key=lambda b: (b[1], b[0]))
    return final
</code></pre>
        </section>
        
        <section>
            <h2>embedding\__init__.py</h2>
            <pre><code># apps/worker/ai/embedding/__init__.py
from __future__ import annotations
</code></pre>
        </section>
        
        <section>
            <h2>embedding\service.py</h2>
            <pre><code># apps/worker/ai/embedding/service.py
from __future__ import annotations

from dataclasses import dataclass
from typing import List, Optional, Sequence, Literal
import math

from apps.worker.ai.config import AIConfig

EmbeddingBackendName = Literal[&quot;local&quot;, &quot;openai&quot;]


@dataclass
class EmbeddingBatch:
    vectors: List[List[float]]
    backend: EmbeddingBackendName


def cosine_similarity(a: Sequence[float], b: Sequence[float]) -&gt; float:
    &quot;&quot;&quot;
    cosine similarity normalized to 0~1
    &quot;&quot;&quot;
    if not a or not b:
        return 0.0
    if len(a) != len(b):
        return 0.0

    dot = 0.0
    na = 0.0
    nb = 0.0
    for x, y in zip(a, b):
        dot += x * y
        na += x * x
        nb += y * y

    if na &lt;= 0.0 or nb &lt;= 0.0:
        return 0.0

    sim = dot / (math.sqrt(na) * math.sqrt(nb))
    sim = max(-1.0, min(1.0, sim))
    return (sim + 1.0) / 2.0


# -------- local (sentence-transformers) --------
try:
    from sentence_transformers import SentenceTransformer  # type: ignore
except Exception:
    SentenceTransformer = None  # type: ignore

_local_model: Optional[&quot;SentenceTransformer&quot;] = None


def _get_local_model() -&gt; &quot;SentenceTransformer&quot;:
    global _local_model
    if _local_model is not None:
        return _local_model

    if SentenceTransformer is None:
        raise RuntimeError(&quot;sentence-transformers not installed&quot;)

    cfg = AIConfig.load()
    _local_model = SentenceTransformer(cfg.EMBEDDING_LOCAL_MODEL)
    return _local_model


def _embed_local(texts: List[str]) -&gt; EmbeddingBatch:
    model = _get_local_model()
    vectors = model.encode(texts, convert_to_numpy=False)
    vectors_list = [list(map(float, v)) for v in vectors]
    return EmbeddingBatch(vectors=vectors_list, backend=&quot;local&quot;)


# -------- openai --------
try:
    from openai import OpenAI  # type: ignore
except Exception:
    OpenAI = None  # type: ignore

_openai_client: Optional[&quot;OpenAI&quot;] = None


def _get_openai_client() -&gt; &quot;OpenAI&quot;:
    global _openai_client
    if _openai_client is not None:
        return _openai_client

    if OpenAI is None:
        raise RuntimeError(&quot;openai not installed&quot;)

    cfg = AIConfig.load()
    if not cfg.OPENAI_API_KEY:
        raise RuntimeError(&quot;OPENAI_API_KEY not set&quot;)

    _openai_client = OpenAI(api_key=cfg.OPENAI_API_KEY)
    return _openai_client


def _embed_openai(texts: List[str]) -&gt; EmbeddingBatch:
    cfg = AIConfig.load()
    client = _get_openai_client()
    response = client.embeddings.create(model=cfg.EMBEDDING_OPENAI_MODEL, input=texts)
    vectors = [list(map(float, d.embedding)) for d in response.data]
    return EmbeddingBatch(vectors=vectors, backend=&quot;openai&quot;)


def _choose_backend() -&gt; EmbeddingBackendName:
    cfg = AIConfig.load()
    mode = (cfg.EMBEDDING_BACKEND or &quot;auto&quot;).lower()

    if mode == &quot;local&quot;:
        return &quot;local&quot;
    if mode == &quot;openai&quot;:
        return &quot;openai&quot;

    # auto: local Í∞ÄÎä•ÌïòÎ©¥ local
    if SentenceTransformer is not None:
        try:
            _get_local_model()
            return &quot;local&quot;
        except Exception:
            pass
    return &quot;openai&quot;


def get_embeddings(texts: List[str]) -&gt; EmbeddingBatch:
    if not texts:
        return EmbeddingBatch(vectors=[], backend=_choose_backend())

    backend = _choose_backend()
    norm = [(t or &quot;&quot;).strip() for t in texts]

    if backend == &quot;local&quot;:
        return _embed_local(norm)
    return _embed_openai(norm)
</code></pre>
        </section>
        
        <section>
            <h2>handwriting\__init__.py</h2>
            <pre><code># apps/worker/ai/handwriting/__init__.py
from __future__ import annotations
</code></pre>
        </section>
        
        <section>
            <h2>handwriting\detector.py</h2>
            <pre><code># apps/worker/ai/handwriting/detector.py
from __future__ import annotations

from typing import Dict
import cv2  # type: ignore
import numpy as np  # type: ignore


def analyze_handwriting(image_path: str) -&gt; Dict[str, float]:
    &quot;&quot;&quot;
    legacy(doc_ai/handwriting/handwriting_detector.py) Ïù¥ÏãùÎ≥∏
    - Ìïú Ïù¥ÎØ∏ÏßÄÏóêÏÑú ÌïÑÍ∏∞ ÌùîÏ†Å/Í≥ÑÏÇ∞Ïãù ÌòïÌÉú ÌùîÏ†Å Ïó¨Î∂Ä Ï†êÏàò Î∞òÌôò

    return:
      {
        &quot;writing_score&quot;: 0~1,
        &quot;calculation_score&quot;: 0~1
      }
    &quot;&quot;&quot;
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        return {&quot;writing_score&quot;: 0.0, &quot;calculation_score&quot;: 0.0}

    blur = cv2.GaussianBlur(img, (5, 5), 0)
    edges = cv2.Canny(blur, 50, 150)

    writing_density = float(np.sum(edges &gt; 0) / edges.size)

    sobel_x = cv2.Sobel(blur, cv2.CV_64F, 1, 0, ksize=5)
    sobel_y = cv2.Sobel(blur, cv2.CV_64F, 0, 1, ksize=5)
    grad_mag = (np.mean(np.abs(sobel_x)) + np.mean(np.abs(sobel_y))) / 255.0

    writing_score = min(max(writing_density * 12.0, 0.0), 1.0)
    calculation_score = min(max(grad_mag * 3.0, 0.0), 1.0)

    return {
        &quot;writing_score&quot;: float(writing_score),
        &quot;calculation_score&quot;: float(calculation_score),
    }
</code></pre>
        </section>
        
        <section>
            <h2>homework\__init__.py</h2>
            <pre><code></code></pre>
        </section>
        
        <section>
            <h2>ocr\__init__.py</h2>
            <pre><code># apps/worker/ai/ocr/__init__.py
from __future__ import annotations
</code></pre>
        </section>
        
        <section>
            <h2>ocr\google.py</h2>
            <pre><code># apps/worker/ai/ocr/google.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Optional

# google cloud vision
from google.cloud import vision  # type: ignore


@dataclass
class OCRResult:
    text: str
    confidence: Optional[float] = None
    raw: Optional[Any] = None


def google_ocr(image_path: str) -&gt; OCRResult:
    &quot;&quot;&quot;
    WorkerÏóêÏÑú Ïã§ÌñâÎêòÎäî Google OCR
    - service accountÎäî GOOGLE_APPLICATION_CREDENTIALS ÎòêÎäî Í∏∞Î≥∏ ÌôòÍ≤ΩÏóê Îî∞Î¶Ñ
    &quot;&quot;&quot;
    client = vision.ImageAnnotatorClient()

    with open(image_path, &quot;rb&quot;) as f:
        content = f.read()

    image = vision.Image(content=content)
    response = client.text_detection(image=image)

    if getattr(response, &quot;error&quot;, None) and response.error.message:
        return OCRResult(text=&quot;&quot;, confidence=None, raw={&quot;error&quot;: response.error.message})

    annotations = getattr(response, &quot;text_annotations&quot;, None) or []
    if not annotations:
        return OCRResult(text=&quot;&quot;, confidence=None, raw=None)

    return OCRResult(
        text=annotations[0].description or &quot;&quot;,
        confidence=None,
        raw=None,  # rawÎ•º ÌÜµÏß∏Î°ú ÎÑòÍ∏∞Î©¥ ÏßÅÎ†¨Ìôî Ïù¥ÏäàÍ∞Ä ÏÉùÍ∏∏ Ïàò ÏûàÏñ¥ Í∏∞Î≥∏ None
    )
</code></pre>
        </section>
        
        <section>
            <h2>ocr\tesseract.py</h2>
            <pre><code># apps/worker/ai/ocr/tesseract.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Optional

from PIL import Image  # type: ignore
import pytesseract  # type: ignore


@dataclass
class OCRResult:
    text: str
    confidence: Optional[float] = None
    raw: Optional[Any] = None


def tesseract_ocr(image_path: str) -&gt; OCRResult:
    img = Image.open(image_path)

    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)
    text = &quot;\n&quot;.join(data.get(&quot;text&quot;, [])).strip()

    confs = [c for c in data.get(&quot;conf&quot;, []) if c != -1]
    confidence = (sum(confs) / len(confs)) if confs else None

    # raw=data Îäî ÎÑàÎ¨¥ ÌÅ¥ Ïàò ÏûàÏñ¥ ÌïÑÏöî ÏãúÎßå ÏºúÍ∏∞
    return OCRResult(text=text, confidence=confidence, raw=None)
</code></pre>
        </section>
        
        <section>
            <h2>omr\__init__.py</h2>
            <pre><code></code></pre>
        </section>
        
        <section>
            <h2>omr\engine.py</h2>
            <pre><code># apps/worker/ai/omr/engine.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore

from apps.worker.ai.omr.types import OMRAnswerV1


# ------------------------------------------------------------
# OMR v1 Simple Engine (CPU)
# - ROI(ÎßàÌÇπ ÏòÅÏó≠) Ïù¥ÎØ∏ÏßÄÏóêÏÑú choiceÎ≥Ñ fill Ï†êÏàòÎ•º Í≥ÑÏÇ∞
# - WorkerÎäî &quot;ÌåêÎã®/Ï∂îÏ∂ú&quot;Îßå ÌïòÍ≥†, Ï†ïÎãµ ÎπÑÍµê/Ï†êÏàò Í≥ÑÏÇ∞ÏùÄ API(results)ÏóêÏÑú Ìï®
#
# ÏûÖÎ†•:
#   - image_path: ÏãúÌóòÏßÄ Ï†ÑÏ≤¥ Ïù¥ÎØ∏ÏßÄ
#   - questions: [{question_id, roi: {x,y,w,h}, choices: [&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;]}, ...]
#   - threshold params
#
# Ï∂úÎ†•:
#   - answers: [OMRAnswerV1, ...]
# ------------------------------------------------------------

BBox = Tuple[int, int, int, int]


@dataclass(frozen=True)
class OMRConfigV1:
    # Ïù¥ Í∞íÎì§ÏùÄ v1 baseline. Ïö¥ÏòÅÌïòÎ©¥ÏÑú Ï°∞Ï†ï Í∞ÄÎä•.
    # 0~1 fill Ï†êÏàò (Ïñ¥ÎëêÏö¥ ÌîΩÏÖÄ ÎπÑÏú® Í∏∞Î∞ò)
    blank_threshold: float = 0.08      # Î™®Îëê ÏïΩÌïòÎ©¥ blank
    multi_threshold: float = 0.62      # 2Í∞ú Ïù¥ÏÉÅÏù¥ Ïù¥ Ïù¥ÏÉÅÏù¥Î©¥ multi
    conf_gap_threshold: float = 0.08   # 1Îì±-2Îì± Ï∞®Ïù¥Í∞Ä Ïù¥Î≥¥Îã§ ÏûëÏúºÎ©¥ ambiguous
    low_confidence_threshold: float = 0.70  # API Ï±ÑÏ†ê Ï†ïÏ±ÖÍ≥º ÎßûÏ∂îÍ∏∞ ÏúÑÌï¥ ÎèôÏùº Í∏∞Î≥∏Í∞í


def _safe_int(v: Any, default: int = 0) -&gt; int:
    try:
        return int(v)
    except Exception:
        return default


def _crop(gray: np.ndarray, bbox: BBox) -&gt; np.ndarray:
    x, y, w, h = bbox
    x = max(0, int(x))
    y = max(0, int(y))
    w = max(1, int(w))
    h = max(1, int(h))
    return gray[y:y + h, x:x + w]


def _fill_score(roi_gray: np.ndarray) -&gt; float:
    &quot;&quot;&quot;
    ROIÏóêÏÑú &#x27;Ï±ÑÏõåÏßê(fill)&#x27; Ï†êÏàò Í≥ÑÏÇ∞ (0~1)
    - Îß§Ïö∞ Îã®ÏàúÌïú v1: Î∞ùÍ∏∞ ÏûÑÍ≥ÑÍ∞íÏúºÎ°ú Ïñ¥ÎëêÏö¥ ÌîΩÏÖÄ ÎπÑÏú®
    &quot;&quot;&quot;
    if roi_gray.size == 0:
        return 0.0

    # normalize
    blur = cv2.GaussianBlur(roi_gray, (5, 5), 0)

    # adaptive-ish: OTSU
    _, th = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    # th: Ï±ÑÏõåÏßÑ Î∂ÄÎ∂ÑÏù¥ 255, Î∞∞Í≤ΩÏù¥ 0 (INV)
    filled = float(np.sum(th &gt; 0))
    total = float(th.size)
    if total &lt;= 0:
        return 0.0

    score = filled / total
    # 0~1 clamp
    return float(max(0.0, min(1.0, score)))


def _split_choices_bbox(roi_bbox: BBox, n: int, axis: str = &quot;x&quot;) -&gt; List[BBox]:
    &quot;&quot;&quot;
    ROI bboxÎ•º nÎì±Î∂ÑÌï¥ÏÑú choiceÎ≥Ñ bboxÎ•º ÎßåÎì†Îã§.
    - axis=&quot;x&quot;: Í∞ÄÎ°úÎ°ú nÎ∂ÑÌï† (A B C D EÍ∞Ä Í∞ÄÎ°úÎ∞∞ÏπòÏù∏ Í≤ΩÏö∞)
    - axis=&quot;y&quot;: ÏÑ∏Î°úÎ°ú nÎ∂ÑÌï†
    &quot;&quot;&quot;
    x, y, w, h = roi_bbox
    boxes: List[BBox] = []
    if n &lt;= 0:
        return boxes

    if axis == &quot;y&quot;:
        step = h / float(n)
        for i in range(n):
            yy = y + int(round(i * step))
            hh = int(round(step))
            boxes.append((x, yy, w, max(1, hh)))
        return boxes

    # default x
    step = w / float(n)
    for i in range(n):
        xx = x + int(round(i * step))
        ww = int(round(step))
        boxes.append((xx, y, max(1, ww), h))
    return boxes


def detect_omr_answers_v1(
    *,
    image_path: str,
    questions: List[Dict[str, Any]],
    cfg: Optional[OMRConfigV1] = None,
) -&gt; List[Dict[str, Any]]:
    &quot;&quot;&quot;
    Worker entry: return list of dict payloads (each is OMRAnswerV1.to_dict()).
    &quot;&quot;&quot;
    cfg = cfg or OMRConfigV1()

    img_bgr = cv2.imread(image_path)
    if img_bgr is None:
        # image load fail -&gt; return error answers (best effort)
        out: List[Dict[str, Any]] = []
        for q in questions or []:
            qid = _safe_int(q.get(&quot;question_id&quot;))
            out.append(
                OMRAnswerV1(
                    version=&quot;v1&quot;,
                    question_id=qid,
                    detected=[],
                    marking=&quot;blank&quot;,
                    confidence=0.0,
                    status=&quot;error&quot;,
                    raw={&quot;error&quot;: &quot;cannot read image&quot;},
                ).to_dict()
            )
        return out

    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)

    results: List[Dict[str, Any]] = []

    for q in questions or []:
        qid = _safe_int(q.get(&quot;question_id&quot;))
        roi = q.get(&quot;roi&quot;) or {}
        roi_bbox: BBox = (
            _safe_int(roi.get(&quot;x&quot;)),
            _safe_int(roi.get(&quot;y&quot;)),
            _safe_int(roi.get(&quot;w&quot;), 1),
            _safe_int(roi.get(&quot;h&quot;), 1),
        )
        choices = q.get(&quot;choices&quot;) or [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;]
        axis = (q.get(&quot;axis&quot;) or &quot;x&quot;).lower()

        n = len(choices)
        choice_boxes = _split_choices_bbox(roi_bbox, n=n, axis=axis)

        marks = []
        for idx, cb in enumerate(choice_boxes):
            roi_choice = _crop(gray, cb)
            fill = _fill_score(roi_choice)
            marks.append({&quot;choice&quot;: str(choices[idx]), &quot;fill&quot;: float(fill)})

        # sort by fill desc
        marks_sorted = sorted(marks, key=lambda m: m[&quot;fill&quot;], reverse=True)
        top = marks_sorted[0] if marks_sorted else {&quot;choice&quot;: &quot;&quot;, &quot;fill&quot;: 0.0}
        second = marks_sorted[1] if len(marks_sorted) &gt; 1 else {&quot;choice&quot;: &quot;&quot;, &quot;fill&quot;: 0.0}

        top_fill = float(top.get(&quot;fill&quot;) or 0.0)
        second_fill = float(second.get(&quot;fill&quot;) or 0.0)

        # blank ÌåêÎã®
        if top_fill &lt; cfg.blank_threshold:
            results.append(
                OMRAnswerV1(
                    version=&quot;v1&quot;,
                    question_id=qid,
                    detected=[],
                    marking=&quot;blank&quot;,
                    confidence=0.0,
                    status=&quot;blank&quot;,
                    raw={&quot;marks&quot;: marks_sorted},
                ).to_dict()
            )
            continue

        # multi ÌåêÎã® (v1: 2Í∞ú Ïù¥ÏÉÅÏù¥ multi_threshold Ïù¥ÏÉÅÏù¥Î©¥ multi)
        high = [m for m in marks_sorted if float(m.get(&quot;fill&quot;) or 0.0) &gt;= cfg.multi_threshold]
        if len(high) &gt;= 2:
            detected = [str(m[&quot;choice&quot;]) for m in high]
            results.append(
                OMRAnswerV1(
                    version=&quot;v1&quot;,
                    question_id=qid,
                    detected=detected,
                    marking=&quot;multi&quot;,
                    confidence=float(top_fill),
                    status=&quot;ambiguous&quot;,  # multiÎäî ambiguousÎ°ú Ï≤òÎ¶¨ (v1)
                    raw={&quot;marks&quot;: marks_sorted},
                ).to_dict()
            )
            continue

        # ambiguous ÌåêÎã® (top-2 gapÏù¥ ÎÑàÎ¨¥ ÏûëÏùå)
        gap = top_fill - second_fill
        if gap &lt; cfg.conf_gap_threshold:
            results.append(
                OMRAnswerV1(
                    version=&quot;v1&quot;,
                    question_id=qid,
                    detected=[str(top[&quot;choice&quot;])],
                    marking=&quot;single&quot;,
                    confidence=float(top_fill),
                    status=&quot;ambiguous&quot;,
                    raw={&quot;marks&quot;: marks_sorted, &quot;gap&quot;: float(gap)},
                ).to_dict()
            )
            continue

        # ok / low_confidence
        status = &quot;ok&quot; if top_fill &gt;= cfg.low_confidence_threshold else &quot;low_confidence&quot;

        results.append(
            OMRAnswerV1(
                version=&quot;v1&quot;,
                question_id=qid,
                detected=[str(top[&quot;choice&quot;])],
                marking=&quot;single&quot;,
                confidence=float(top_fill),
                status=status,  # ok ÎòêÎäî low_confidence
                raw={&quot;marks&quot;: marks_sorted, &quot;gap&quot;: float(gap)},
            ).to_dict()
        )

    return results
</code></pre>
        </section>
        
        <section>
            <h2>omr\types.py</h2>
            <pre><code># apps/worker/ai/omr/types.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Literal


OMRStatus = Literal[&quot;ok&quot;, &quot;blank&quot;, &quot;ambiguous&quot;, &quot;low_confidence&quot;, &quot;error&quot;]
OMRMarking = Literal[&quot;blank&quot;, &quot;single&quot;, &quot;multi&quot;]


@dataclass(frozen=True)
class OMRAnswerV1:
    &quot;&quot;&quot;
    Worker-side OMR payload v1 (question-level)
    This should be embedded into API-side SubmissionAnswer.meta[&quot;omr&quot;] later.

    - version: &quot;v1&quot; fixed
    - detected: list[str] ex) [&quot;B&quot;] or [&quot;B&quot;,&quot;D&quot;]
    - marking: blank/single/multi
    - confidence: 0~1 (top mark confidence)
    - status: ok/blank/ambiguous/low_confidence/error
    - raw: debug info (optional)
    &quot;&quot;&quot;
    version: str
    question_id: int

    detected: List[str]
    marking: OMRMarking
    confidence: float
    status: OMRStatus

    raw: Optional[Dict[str, Any]] = None

    def to_dict(self) -&gt; Dict[str, Any]:
        return {
            &quot;version&quot;: self.version,
            &quot;question_id&quot;: int(self.question_id),
            &quot;detected&quot;: list(self.detected or []),
            &quot;marking&quot;: self.marking,
            &quot;confidence&quot;: float(self.confidence or 0.0),
            &quot;status&quot;: self.status,
            &quot;raw&quot;: self.raw,
        }
</code></pre>
        </section>
        
        <section>
            <h2>pipelines\__init__.py</h2>
            <pre><code># apps/worker/ai/pipelines/__init__.py
from __future__ import annotations
</code></pre>
        </section>
        
        <section>
            <h2>pipelines\dispatcher.py</h2>
            <pre><code># apps/worker/ai/pipelines/dispatcher.py
from __future__ import annotations

from typing import Any, Dict

from apps.shared.contracts.ai_job import AIJob
from apps.shared.contracts.ai_result import AIResult

from apps.worker.ai.config import AIConfig
from apps.worker.ai.ocr.google import google_ocr
from apps.worker.ai.ocr.tesseract import tesseract_ocr
from apps.worker.ai.detection.segment_dispatcher import segment_questions
from apps.worker.ai.handwriting.detector import analyze_handwriting
from apps.worker.ai.embedding.service import get_embeddings
from apps.worker.ai.problem.generator import generate_problem_from_ocr
from apps.worker.ai.pipelines.homework_video_analyzer import analyze_homework_video


def handle_ai_job(job: AIJob) -&gt; AIResult:
    &quot;&quot;&quot;
    Worker-side single entrypoint:
      AIJob -&gt; AIResult

    - Ï†ÄÏû•ÌïòÏßÄ ÏïäÏùå
    - ÎèÑÎ©îÏù∏ Í∑úÏπô ÌåêÎã®ÌïòÏßÄ ÏïäÏùå
    - ÌïÑÏöîÌïú Í≥ÑÏÇ∞Îßå ÏàòÌñâÌïòÍ≥† Í≤∞Í≥ºÎ•º result payloadÎ°ú Î∞òÌôò
    &quot;&quot;&quot;
    try:
        cfg = AIConfig.load()
        payload: Dict[str, Any] = job.payload or {}

        if job.type == &quot;ocr&quot;:
            image_path = payload[&quot;image_path&quot;]
            engine = (payload.get(&quot;engine&quot;) or cfg.OCR_ENGINE or &quot;auto&quot;).lower()

            if engine == &quot;tesseract&quot;:
                r = tesseract_ocr(image_path)
            elif engine == &quot;google&quot;:
                r = google_ocr(image_path)
            else:
                # auto: google -&gt; tesseract
                try:
                    r = google_ocr(image_path)
                    if not r.text.strip():
                        r = tesseract_ocr(image_path)
                except Exception:
                    r = tesseract_ocr(image_path)

            return AIResult.done(
                job.id,
                {&quot;text&quot;: r.text, &quot;confidence&quot;: r.confidence},
            )

        if job.type == &quot;question_segmentation&quot;:
            image_path = payload[&quot;image_path&quot;]
            boxes = segment_questions(image_path)
            return AIResult.done(job.id, {&quot;boxes&quot;: boxes})

        if job.type == &quot;handwriting_analysis&quot;:
            image_path = payload[&quot;image_path&quot;]
            scores = analyze_handwriting(image_path)
            return AIResult.done(job.id, scores)

        if job.type == &quot;embedding&quot;:
            texts = payload.get(&quot;texts&quot;) or []
            batch = get_embeddings(list(texts))
            return AIResult.done(
                job.id,
                {&quot;backend&quot;: batch.backend, &quot;vectors&quot;: batch.vectors},
            )

        if job.type == &quot;problem_generation&quot;:
            ocr_text = payload.get(&quot;ocr_text&quot;) or &quot;&quot;
            parsed = generate_problem_from_ocr(ocr_text)
            return AIResult.done(
                job.id,
                {
                    &quot;body&quot;: parsed.body,
                    &quot;choices&quot;: parsed.choices,
                    &quot;answer&quot;: parsed.answer,
                    &quot;difficulty&quot;: parsed.difficulty,
                    &quot;tag&quot;: parsed.tag,
                    &quot;summary&quot;: parsed.summary,
                    &quot;explanation&quot;: parsed.explanation,
                },
            )

        if job.type == &quot;homework_video_analysis&quot;:
            video_path = payload[&quot;video_path&quot;]
            frame_stride = int(payload.get(&quot;frame_stride&quot;) or 10)
            min_frame_count = int(payload.get(&quot;min_frame_count&quot;) or 30)
            analysis = analyze_homework_video(
                video_path=video_path,
                frame_stride=frame_stride,
                min_frame_count=min_frame_count,
            )
            return AIResult.done(job.id, analysis)

        if job.type == &quot;omr_grading&quot;:
            image_path = payload[&quot;image_path&quot;]
            questions = payload.get(&quot;questions&quot;) or []
            # questions: [{question_id, roi:{x,y,w,h}, choices:[...], axis:&quot;x&quot;|&quot;y&quot;}, ...]
            from apps.worker.ai.omr.engine import detect_omr_answers_v1

            answers = detect_omr_answers_v1(
                image_path=image_path,
                questions=list(questions),
            )
            return AIResult.done(
                job.id,
                {
                    &quot;version&quot;: &quot;v1&quot;,
                    &quot;answers&quot;: answers,
                },
            )

        return AIResult.failed(job.id, f&quot;Unsupported job type: {job.type}&quot;)

    except Exception as e:
        return AIResult.failed(job.id, str(e))
</code></pre>
        </section>
        
        <section>
            <h2>pipelines\homework_video_analyzer.py</h2>
            <pre><code># apps/worker/ai/pipelines/homework_video_analyzer.py
from __future__ import annotations

from typing import Dict, Any, List
import cv2  # type: ignore
import numpy as np  # type: ignore


def _estimate_writing_score(gray_roi: np.ndarray) -&gt; float:
    if gray_roi.size == 0:
        return 0.0
    dark = (gray_roi &lt; 220).sum()
    total = gray_roi.size
    return float(dark) / float(total)


def analyze_homework_video(
    video_path: str,
    frame_stride: int = 10,
    min_frame_count: int = 30,
) -&gt; Dict[str, Any]:
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f&quot;cannot open video: {video_path}&quot;)

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_idx = 0
    frame_results: List[Dict[str, Any]] = []

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        if frame_idx % frame_stride != 0:
            frame_idx += 1
            continue

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        score = _estimate_writing_score(gray)

        frame_results.append(
            {
                &quot;index&quot;: frame_idx,
                &quot;writing_score&quot;: round(float(score), 4),
                &quot;has_writing&quot;: bool(score &gt;= 0.05),
            }
        )
        frame_idx += 1

    cap.release()

    sampled = len(frame_results)
    if sampled == 0:
        return {
            &quot;total_frames&quot;: total_frames,
            &quot;sampled_frames&quot;: 0,
            &quot;avg_writing_score&quot;: 0.0,
            &quot;filled_ratio&quot;: 0.0,
            &quot;frames&quot;: [],
            &quot;too_short&quot;: total_frames &lt; min_frame_count,
        }

    avg = sum(fr[&quot;writing_score&quot;] for fr in frame_results) / sampled
    filled = sum(1 for fr in frame_results if fr[&quot;has_writing&quot;])
    ratio = filled / sampled

    return {
        &quot;total_frames&quot;: total_frames,
        &quot;sampled_frames&quot;: sampled,
        &quot;avg_writing_score&quot;: round(float(avg), 4),
        &quot;filled_ratio&quot;: round(float(ratio), 4),
        &quot;frames&quot;: frame_results,
        &quot;too_short&quot;: total_frames &lt; min_frame_count,
    }
</code></pre>
        </section>
        
        <section>
            <h2>problem\__init__.py</h2>
            <pre><code># apps/worker/ai/problem/__init__.py
from __future__ import annotations
</code></pre>
        </section>
        
        <section>
            <h2>problem\generator.py</h2>
            <pre><code># apps/worker/ai/problem/generator.py
from __future__ import annotations

import json
from dataclasses import dataclass
from typing import Optional

from apps.worker.ai.config import AIConfig
from apps.worker.ai.problem.prompt import BASE_PROMPT

try:
    from openai import OpenAI  # type: ignore
except Exception:
    OpenAI = None  # type: ignore


@dataclass
class ParsedProblem:
    body: str
    choices: list
    answer: Optional[str]
    difficulty: int
    tag: str
    summary: str
    explanation: str


_client: Optional[&quot;OpenAI&quot;] = None


def _get_client() -&gt; &quot;OpenAI&quot;:
    global _client
    if _client is not None:
        return _client

    if OpenAI is None:
        raise RuntimeError(&quot;openai not installed&quot;)

    cfg = AIConfig.load()
    if not cfg.OPENAI_API_KEY:
        raise RuntimeError(&quot;OPENAI_API_KEY not set&quot;)

    _client = OpenAI(api_key=cfg.OPENAI_API_KEY)
    return _client


def generate_problem_from_ocr(ocr_text: str) -&gt; ParsedProblem:
    cfg = AIConfig.load()
    prompt = BASE_PROMPT.format(ocr_text=ocr_text)

    client = _get_client()
    response = client.chat.completions.create(
        model=cfg.PROBLEM_GEN_MODEL,
        messages=[
            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;ÎãπÏã†ÏùÄ ÍµêÏú°Ïö© ÏãúÌóò Î¨∏Ï†úÎ•º ÏûêÎèô ÏÉùÏÑ±ÌïòÎäî ÏóîÏßÑÏûÖÎãàÎã§.&quot;},
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt},
        ],
        temperature=0.2,
    )

    # SDK ÌòïÌÉú Ï∞®Ïù¥ Î∞©Ïñ¥
    msg = response.choices[0].message
    content = getattr(msg, &quot;content&quot;, None) or msg.get(&quot;content&quot;)  # type: ignore

    data = json.loads(content)

    return ParsedProblem(
        body=data.get(&quot;body&quot;, &quot;&quot;),
        choices=data.get(&quot;choices&quot;, []),
        answer=data.get(&quot;answer&quot;),
        difficulty=int(data.get(&quot;difficulty&quot;, 3)),
        tag=data.get(&quot;tag&quot;, &quot;&quot;),
        summary=data.get(&quot;summary&quot;, &quot;&quot;),
        explanation=data.get(&quot;explanation&quot;, &quot;&quot;),
    )
</code></pre>
        </section>
        
        <section>
            <h2>problem\prompt.py</h2>
            <pre><code># apps/worker/ai/problem/prompt.py
BASE_PROMPT = &quot;&quot;&quot;
Îã§ÏùåÏùÄ ÏãúÌóò Î¨∏Ï†úÏùò OCR Í≤∞Í≥ºÏûÖÎãàÎã§.
ÏïÑÎûò ÌÖçÏä§Ìä∏Î•º Í∏∞Î∞òÏúºÎ°ú Î¨∏Ï†ú Ï†ïÎ≥¥Î•º JSON ÌòïÏãùÏúºÎ°ú Ï∂îÏ∂úÌïòÏÑ∏Ïöî.

ÏöîÍµ¨ÏÇ¨Ìï≠:
1) Î¨∏Ï†ú Î≥∏Î¨∏ (body)
2) ÏÑ†ÌÉùÏßÄ (choices): ÏóÜÏúºÎ©¥ Îπà Î∞∞Ïó¥
3) Ï†ïÎãµ (answer): Î™ÖÏãúÎêú Ï†ïÎãµÏù¥ ÏóÜÏúºÎ©¥ AIÍ∞Ä Ï∂îÎ°†, Ï∂îÎ°† Î∂àÍ∞Ä Ïãú null
4) ÎÇúÏù¥ÎèÑ (difficulty): 1~5 Ï†ïÏàòÎ°ú Ï∂îÏ†ï
5) ÌÉúÍ∑∏ (tag): ÏàòÌïô/Í≥ºÌïô/Íµ≠Ïñ¥ Îì± Í∞ÑÎã®Ìïú Î∂ÑÎ•ò
6) Î¨∏Ï†ú ÏöîÏïΩ (summary)
7) Ìï¥ÏÑ§ (explanation): Í∞ÑÎã®Î™ÖÎ£åÌïòÍ≤å

Ï∂úÎ†•ÏùÄ Î∞òÎìúÏãú JSON ÌòïÏãùÎßå ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî. Îã§Î•∏ ÌÖçÏä§Ìä∏Îäî Ìè¨Ìï®ÌïòÏßÄ ÎßàÏÑ∏Ïöî.

Ï∂úÎ†• ÌòïÏãù ÏòàÏãú:
{
  &quot;body&quot;: &quot;...&quot;,
  &quot;choices&quot;: [&quot;A...&quot;, &quot;B...&quot;, &quot;C...&quot;, &quot;D...&quot;],
  &quot;answer&quot;: &quot;C&quot;,
  &quot;difficulty&quot;: 3,
  &quot;tag&quot;: &quot;ÏàòÌïô&quot;,
  &quot;summary&quot;: &quot;...&quot;,
  &quot;explanation&quot;: &quot;...&quot;
}

OCR ÌÖçÏä§Ìä∏:
\&quot;\&quot;\&quot;
{ocr_text}
\&quot;\&quot;\&quot;
&quot;&quot;&quot;
</code></pre>
        </section>
        
</body>
</html>
