====================================================================================================
# BACKEND APP: worker
# ROOT PATH: C:\academy\apps\worker
====================================================================================================


==========================================================================================
# FILE: __init__.py
==========================================================================================
#apps/worker/__init__.py

#ÏïÑÎ¨¥ÎÇ¥Ïö©ÏóÜÏùå.


==========================================================================================
# FILE: ai_worker/__init__.py
==========================================================================================



==========================================================================================
# FILE: ai_worker/celery.py
==========================================================================================
# apps/worker/ai_worker/celery.py

from celery import Celery

app = Celery("academy_ai")

app.conf.broker_url = "redis://172.31.32.109:6379/0"
app.conf.result_backend = "redis://172.31.32.109:6379/0"

# ü§ñ AI Ï†ÑÏö©
app.autodiscover_tasks([
    "apps.worker.ai_worker.ai",
])

print("ü§ñ AI Celery READY ü§ñ")


==========================================================================================
# FILE: ai_worker/run.py
==========================================================================================
# apps/worker/run.py
from __future__ import annotations

import os
import sys
import time
import signal
import logging
import requests

from apps.shared.contracts.ai_job import AIJob
from apps.shared.contracts.ai_result import AIResult
from apps.worker.ai_worker.ai.pipelines.dispatcher import handle_ai_job

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] [AI-WORKER] %(message)s",
)
logger = logging.getLogger(__name__)

API_BASE_URL = os.getenv("API_BASE_URL", "http://localhost:8000")
INTERNAL_WORKER_TOKEN = os.getenv("INTERNAL_WORKER_TOKEN", "long-random-secret")
POLL_INTERVAL_SEC = float(os.getenv("AI_WORKER_POLL_INTERVAL", "1.0"))

_running = True


def _shutdown(signum, frame):
    global _running
    logger.warning("Shutdown signal received (%s)", signum)
    _running = False


signal.signal(signal.SIGINT, _shutdown)
signal.signal(signal.SIGTERM, _shutdown)


def fetch_job() -> AIJob | None:
    """
    API ‚Üí Worker
    GET /api/v1/internal/ai/job/next/
    response: { "job": {...} | null }
    """
    url = f"{API_BASE_URL}/api/v1/internal/ai/job/next/"
    headers = {"X-Worker-Token": INTERNAL_WORKER_TOKEN}

    resp = requests.get(url, headers=headers, timeout=10)
    resp.raise_for_status()

    data = resp.json()
    job_data = data.get("job")
    if not job_data:
        return None

    return AIJob.from_dict(job_data)


def submit_result(result: AIResult, submission_id: int) -> None:
    """
    Worker ‚Üí API
    POST /api/v1/internal/ai/job/result/
    """
    url = f"{API_BASE_URL}/api/v1/internal/ai/job/result/"
    headers = {
        "X-Worker-Token": INTERNAL_WORKER_TOKEN,
        "Content-Type": "application/json",
    }

    # ‚úÖ Íµ¨Ï°∞ Í≥†Ï†ï (Ï§ëÏöî)
    payload = {
        "submission_id": submission_id,
        "status": result.status,
        "result": result.result,
        "error": result.error,
    }

    resp = requests.post(
        url,
        json=payload,
        headers=headers,
        timeout=20,
    )
    resp.raise_for_status()


def main():
    logger.info("AI Worker started")

    while _running:
        try:
            job = fetch_job()
            if job is None:
                time.sleep(POLL_INTERVAL_SEC)
                continue

            logger.info("Job received: id=%s type=%s", job.id, job.type)

            # üî• AI Ï≤òÎ¶¨
            result = handle_ai_job(job)

            # üî• Í≤∞Í≥º Ï†ÑÏÜ° (submission_id = job.source_id)
            submit_result(result, int(job.source_id))

            logger.info(
                "Job finished: id=%s status=%s",
                job.id,
                result.status,
            )

        except Exception:
            logger.exception("Worker loop error")
            time.sleep(2.0)

    logger.info("AI Worker shutdown complete")


if __name__ == "__main__":
    sys.exit(main())


==========================================================================================
# FILE: ai_worker/ai/__init__.py
==========================================================================================
# apps/worker/ai/__init__.py
# Celery Ï†úÍ±∞Îê®. run.py Îã®Ïùº ÏßÑÏûÖ


==========================================================================================
# FILE: ai_worker/ai/config.py
==========================================================================================
# apps/worker/ai/config.py
from __future__ import annotations

import os
from dataclasses import dataclass
from typing import List, Optional


def _env(name: str, default: Optional[str] = None) -> Optional[str]:
    v = os.getenv(name)
    return v if v not in (None, "") else default


@dataclass(frozen=True)
class AIConfig:
    # OCR
    OCR_ENGINE: str = "google"  # google | tesseract | auto
    GOOGLE_APPLICATION_CREDENTIALS: Optional[str] = None  # optional (google sdk default)

    # Segmentation
    QUESTION_SEGMENTATION_ENGINE: str = "auto"  # yolo|opencv|template|auto

    # YOLO (optional)
    YOLO_QUESTION_MODEL_PATH: Optional[str] = None
    YOLO_QUESTION_INPUT_SIZE: int = 640
    YOLO_QUESTION_CONF_THRESHOLD: float = 0.4
    YOLO_QUESTION_IOU_THRESHOLD: float = 0.5

    # Embedding
    EMBEDDING_BACKEND: str = "auto"  # local|openai|auto
    EMBEDDING_LOCAL_MODEL: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    EMBEDDING_OPENAI_MODEL: str = "text-embedding-3-small"
    OPENAI_API_KEY: Optional[str] = None

    # Problem generation
    PROBLEM_GEN_MODEL: str = "gpt-4.1-mini"  # default

    @staticmethod
    def load() -> "AIConfig":
        return AIConfig(
            OCR_ENGINE=_env("OCR_ENGINE", "google") or "google",
            GOOGLE_APPLICATION_CREDENTIALS=_env("GOOGLE_APPLICATION_CREDENTIALS"),

            QUESTION_SEGMENTATION_ENGINE=_env("QUESTION_SEGMENTATION_ENGINE", "auto") or "auto",

            YOLO_QUESTION_MODEL_PATH=_env("YOLO_QUESTION_MODEL_PATH"),
            YOLO_QUESTION_INPUT_SIZE=int(_env("YOLO_QUESTION_INPUT_SIZE", "640") or "640"),
            YOLO_QUESTION_CONF_THRESHOLD=float(_env("YOLO_QUESTION_CONF_THRESHOLD", "0.4") or "0.4"),
            YOLO_QUESTION_IOU_THRESHOLD=float(_env("YOLO_QUESTION_IOU_THRESHOLD", "0.5") or "0.5"),

            EMBEDDING_BACKEND=_env("EMBEDDING_BACKEND", "auto") or "auto",
            EMBEDDING_LOCAL_MODEL=_env(
                "EMBEDDING_LOCAL_MODEL",
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            ) or "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            EMBEDDING_OPENAI_MODEL=_env("EMBEDDING_OPENAI_MODEL", "text-embedding-3-small") or "text-embedding-3-small",
            OPENAI_API_KEY=_env("OPENAI_API_KEY") or _env("EMBEDDING_OPENAI_API_KEY"),

            PROBLEM_GEN_MODEL=_env("PROBLEM_GEN_MODEL", "gpt-4.1-mini") or "gpt-4.1-mini",
        )


==========================================================================================
# FILE: ai_worker/ai/detection/__init__.py
==========================================================================================
# apps/worker/ai/detection/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/detection/segment_dispatcher.py
==========================================================================================
# apps/worker/ai/detection/segment_dispatcher.py
from __future__ import annotations

from typing import List, Tuple

from apps.worker.ai_worker.ai.config import AIConfig
from apps.worker.ai_worker.ai.detection.segment_opencv import segment_questions_opencv
from apps.worker.ai_worker.ai.detection.segment_yolo import segment_questions_yolo

BBox = Tuple[int, int, int, int]


def segment_questions(image_path: str) -> List[BBox]:
    """
    worker-side segmentation single entrypoint
    """
    cfg = AIConfig.load()
    engine = (cfg.QUESTION_SEGMENTATION_ENGINE or "auto").lower()

    if engine == "opencv":
        return segment_questions_opencv(image_path)
    if engine == "yolo":
        return segment_questions_yolo(image_path)

    # auto: yolo -> opencv
    try:
        boxes = segment_questions_yolo(image_path)
        if boxes:
            return boxes
    except Exception:
        pass
    return segment_questions_opencv(image_path)


==========================================================================================
# FILE: ai_worker/ai/detection/segment_opencv.py
==========================================================================================
# apps/worker/ai/detection/segment_opencv.py
from __future__ import annotations

from typing import List, Tuple
import cv2  # type: ignore

BBox = Tuple[int, int, int, int]


def segment_questions_opencv(image_path: str) -> List[BBox]:
    """
    legacy ÏÑûÏó¨ÏûàÎçò opencv segmentation Ï†ïÎ¶¨Î≥∏
    ÏûÖÎ†•: image_path
    Ï∂úÎ†•: [(x,y,w,h), ...]
    """
    image_bgr = cv2.imread(image_path)
    if image_bgr is None:
        return []

    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5, 5), 0)

    _, thresh = cv2.threshold(
        blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU
    )

    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))
    dilated = cv2.dilate(thresh, kernel, iterations=1)

    contours, _ = cv2.findContours(
        dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
    )

    h_img, w_img = gray.shape[:2]
    min_area = w_img * h_img * 0.005
    max_area = w_img * h_img * 0.9

    boxes: List[BBox] = []
    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)
        area = w * h
        if area < min_area or area > max_area:
            continue

        aspect = h / (w + 1e-6)
        if aspect < 0.3:
            continue

        boxes.append((x, y, w, h))

    boxes.sort(key=lambda b: (b[1], b[0]))
    return boxes


==========================================================================================
# FILE: ai_worker/ai/detection/segment_yolo.py
==========================================================================================
# apps/worker/ai/detection/segment_yolo.py
from __future__ import annotations

from functools import lru_cache
from typing import List, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore

from apps.worker.ai_worker.ai.config import AIConfig

BBox = Tuple[int, int, int, int]


class YoloNotConfiguredError(RuntimeError):
    pass


try:
    import onnxruntime as ort  # type: ignore
    _HAS_ORT = True
except Exception:
    _HAS_ORT = False


@lru_cache()
def _get_session():
    cfg = AIConfig.load()

    if not _HAS_ORT:
        raise YoloNotConfiguredError("onnxruntime not installed")

    if not cfg.YOLO_QUESTION_MODEL_PATH:
        raise YoloNotConfiguredError("YOLO_QUESTION_MODEL_PATH not set")

    providers = ["CPUExecutionProvider"]
    return ort.InferenceSession(str(cfg.YOLO_QUESTION_MODEL_PATH), providers=providers)


def _preprocess(image_bgr, input_size: int):
    h0, w0 = image_bgr.shape[:2]
    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
    resized = cv2.resize(image_rgb, (input_size, input_size))
    resized = resized.astype(np.float32) / 255.0

    tensor = np.transpose(resized, (2, 0, 1))
    tensor = np.expand_dims(tensor, axis=0)

    scale_x = w0 / float(input_size)
    scale_y = h0 / float(input_size)
    return tensor, scale_x, scale_y


def _nms(boxes, scores, iou_threshold: float):
    if len(boxes) == 0:
        return []

    boxes = boxes.astype(np.float32)
    scores = scores.astype(np.float32)

    x1, y1, x2, y2 = boxes.T
    areas = (x2 - x1) * (y2 - y1)
    order = scores.argsort()[::-1]

    keep = []
    while order.size > 0:
        i = int(order[0])
        keep.append(i)

        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])

        w = np.maximum(0.0, xx2 - xx1)
        h = np.maximum(0.0, yy2 - yy1)
        inter = w * h

        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-6)
        idxs = np.where(iou <= iou_threshold)[0]
        order = order[idxs + 1]

    return keep


def segment_questions_yolo(image_path: str) -> List[BBox]:
    cfg = AIConfig.load()
    sess = _get_session()

    image_bgr = cv2.imread(image_path)
    if image_bgr is None:
        return []

    input_tensor, scale_x, scale_y = _preprocess(image_bgr, cfg.YOLO_QUESTION_INPUT_SIZE)

    input_name = sess.get_inputs()[0].name
    outputs = sess.run(None, {input_name: input_tensor})
    preds = outputs[0]
    if preds.ndim == 3:
        preds = preds[0]

    boxes = []
    scores = []

    for det in preds:
        cx, cy, w, h, obj_conf = det[:5]
        cls_scores = det[5:]
        cls_conf = float(cls_scores.max()) if cls_scores.size > 0 else 1.0

        score = float(obj_conf * cls_conf)
        if score < cfg.YOLO_QUESTION_CONF_THRESHOLD:
            continue

        x1 = (cx - w / 2.0) * scale_x
        y1 = (cy - h / 2.0) * scale_y
        x2 = (cx + w / 2.0) * scale_x
        y2 = (cy + h / 2.0) * scale_y

        boxes.append([x1, y1, x2, y2])
        scores.append(score)

    if not boxes:
        return []

    boxes_np = np.array(boxes)
    scores_np = np.array(scores)

    keep_idx = _nms(boxes_np, scores_np, cfg.YOLO_QUESTION_IOU_THRESHOLD)

    final: List[BBox] = []
    for i in keep_idx:
        x1, y1, x2, y2 = boxes_np[i]
        final.append((int(x1), int(y1), int(x2 - x1), int(y2 - y1)))

    final.sort(key=lambda b: (b[1], b[0]))
    return final


==========================================================================================
# FILE: ai_worker/ai/embedding/__init__.py
==========================================================================================
# apps/worker/ai/embedding/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/embedding/service.py
==========================================================================================
# apps/worker/ai/embedding/service.py
from __future__ import annotations

from dataclasses import dataclass
from typing import List, Optional, Sequence, Literal
import math

from apps.worker.ai_worker.ai.config import AIConfig

EmbeddingBackendName = Literal["local", "openai"]


@dataclass
class EmbeddingBatch:
    vectors: List[List[float]]
    backend: EmbeddingBackendName


def cosine_similarity(a: Sequence[float], b: Sequence[float]) -> float:
    """
    cosine similarity normalized to 0~1
    """
    if not a or not b:
        return 0.0
    if len(a) != len(b):
        return 0.0

    dot = 0.0
    na = 0.0
    nb = 0.0
    for x, y in zip(a, b):
        dot += x * y
        na += x * x
        nb += y * y

    if na <= 0.0 or nb <= 0.0:
        return 0.0

    sim = dot / (math.sqrt(na) * math.sqrt(nb))
    sim = max(-1.0, min(1.0, sim))
    return (sim + 1.0) / 2.0


# -------- local (sentence-transformers) --------
try:
    from sentence_transformers import SentenceTransformer  # type: ignore
except Exception:
    SentenceTransformer = None  # type: ignore

_local_model: Optional["SentenceTransformer"] = None


def _get_local_model() -> "SentenceTransformer":
    global _local_model
    if _local_model is not None:
        return _local_model

    if SentenceTransformer is None:
        raise RuntimeError("sentence-transformers not installed")

    cfg = AIConfig.load()
    _local_model = SentenceTransformer(cfg.EMBEDDING_LOCAL_MODEL)
    return _local_model


def _embed_local(texts: List[str]) -> EmbeddingBatch:
    model = _get_local_model()
    vectors = model.encode(texts, convert_to_numpy=False)
    vectors_list = [list(map(float, v)) for v in vectors]
    return EmbeddingBatch(vectors=vectors_list, backend="local")


# -------- openai --------
try:
    from openai import OpenAI  # type: ignore
except Exception:
    OpenAI = None  # type: ignore

_openai_client: Optional["OpenAI"] = None


def _get_openai_client() -> "OpenAI":
    global _openai_client
    if _openai_client is not None:
        return _openai_client

    if OpenAI is None:
        raise RuntimeError("openai not installed")

    cfg = AIConfig.load()
    if not cfg.OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY not set")

    _openai_client = OpenAI(api_key=cfg.OPENAI_API_KEY)
    return _openai_client


def _embed_openai(texts: List[str]) -> EmbeddingBatch:
    cfg = AIConfig.load()
    client = _get_openai_client()
    response = client.embeddings.create(model=cfg.EMBEDDING_OPENAI_MODEL, input=texts)
    vectors = [list(map(float, d.embedding)) for d in response.data]
    return EmbeddingBatch(vectors=vectors, backend="openai")


def _choose_backend() -> EmbeddingBackendName:
    cfg = AIConfig.load()
    mode = (cfg.EMBEDDING_BACKEND or "auto").lower()

    if mode == "local":
        return "local"
    if mode == "openai":
        return "openai"

    # auto: local Í∞ÄÎä•ÌïòÎ©¥ local
    if SentenceTransformer is not None:
        try:
            _get_local_model()
            return "local"
        except Exception:
            pass
    return "openai"


def get_embeddings(texts: List[str]) -> EmbeddingBatch:
    if not texts:
        return EmbeddingBatch(vectors=[], backend=_choose_backend())

    backend = _choose_backend()
    norm = [(t or "").strip() for t in texts]

    if backend == "local":
        return _embed_local(norm)
    return _embed_openai(norm)


==========================================================================================
# FILE: ai_worker/ai/handwriting/__init__.py
==========================================================================================
# apps/worker/ai/handwriting/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/handwriting/detector.py
==========================================================================================
# apps/worker/ai/handwriting/detector.py
from __future__ import annotations

from typing import Dict
import cv2  # type: ignore
import numpy as np  # type: ignore


def analyze_handwriting(image_path: str) -> Dict[str, float]:
    """
    legacy(doc_ai/handwriting/handwriting_detector.py) Ïù¥ÏãùÎ≥∏
    - Ìïú Ïù¥ÎØ∏ÏßÄÏóêÏÑú ÌïÑÍ∏∞ ÌùîÏ†Å/Í≥ÑÏÇ∞Ïãù ÌòïÌÉú ÌùîÏ†Å Ïó¨Î∂Ä Ï†êÏàò Î∞òÌôò

    return:
      {
        "writing_score": 0~1,
        "calculation_score": 0~1
      }
    """
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        return {"writing_score": 0.0, "calculation_score": 0.0}

    blur = cv2.GaussianBlur(img, (5, 5), 0)
    edges = cv2.Canny(blur, 50, 150)

    writing_density = float(np.sum(edges > 0) / edges.size)

    sobel_x = cv2.Sobel(blur, cv2.CV_64F, 1, 0, ksize=5)
    sobel_y = cv2.Sobel(blur, cv2.CV_64F, 0, 1, ksize=5)
    grad_mag = (np.mean(np.abs(sobel_x)) + np.mean(np.abs(sobel_y))) / 255.0

    writing_score = min(max(writing_density * 12.0, 0.0), 1.0)
    calculation_score = min(max(grad_mag * 3.0, 0.0), 1.0)

    return {
        "writing_score": float(writing_score),
        "calculation_score": float(calculation_score),
    }


==========================================================================================
# FILE: ai_worker/ai/homework/__init__.py
==========================================================================================



==========================================================================================
# FILE: ai_worker/ai/ocr/__init__.py
==========================================================================================
# apps/worker/ai/ocr/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/ocr/engine.py
==========================================================================================
from __future__ import annotations
from typing import List
from .schemas import OCRResultPayload, OMRDetectedAnswer


def run_ocr_engine(
    *,
    image_path: str,
) -> OCRResultPayload:
    """
    Ïã§Ï†ú OCR/OMR ÏóîÏßÑ ÏûêÎ¶¨
    - ÏßÄÍ∏àÏùÄ ÎçîÎØ∏
    - ÎÇòÏ§ëÏóê OpenCV / Tesseract / Ïô∏Î∂Ä API ÍµêÏ≤¥
    """

    answers: List[OMRDetectedAnswer] = [
        {
            "question_number": 1,
            "detected": ["B"],
            "confidence": 0.92,
            "marking": "single",
            "status": "ok",
        },
        {
            "question_number": 2,
            "detected": ["D"],
            "confidence": 0.88,
            "marking": "single",
            "status": "ok",
        },
    ]

    return {
        "version": "v1",
        "answers": answers,
        "raw_text": None,
    }


==========================================================================================
# FILE: ai_worker/ai/ocr/google.py
==========================================================================================
# apps/worker/ai/ocr/google.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Optional

# google cloud vision
from google.cloud import vision  # type: ignore


@dataclass
class OCRResult:
    text: str
    confidence: Optional[float] = None
    raw: Optional[Any] = None


def google_ocr(image_path: str) -> OCRResult:
    """
    WorkerÏóêÏÑú Ïã§ÌñâÎêòÎäî Google OCR
    - service accountÎäî GOOGLE_APPLICATION_CREDENTIALS ÎòêÎäî Í∏∞Î≥∏ ÌôòÍ≤ΩÏóê Îî∞Î¶Ñ
    """
    client = vision.ImageAnnotatorClient()

    with open(image_path, "rb") as f:
        content = f.read()

    image = vision.Image(content=content)
    response = client.text_detection(image=image)

    if getattr(response, "error", None) and response.error.message:
        return OCRResult(text="", confidence=None, raw={"error": response.error.message})

    annotations = getattr(response, "text_annotations", None) or []
    if not annotations:
        return OCRResult(text="", confidence=None, raw=None)

    return OCRResult(
        text=annotations[0].description or "",
        confidence=None,
        raw=None,  # rawÎ•º ÌÜµÏß∏Î°ú ÎÑòÍ∏∞Î©¥ ÏßÅÎ†¨Ìôî Ïù¥ÏäàÍ∞Ä ÏÉùÍ∏∏ Ïàò ÏûàÏñ¥ Í∏∞Î≥∏ None
    )


==========================================================================================
# FILE: ai_worker/ai/ocr/schemas.py
==========================================================================================
from __future__ import annotations
from typing import TypedDict, List, Optional


class OMRDetectedAnswer(TypedDict):
    question_number: int
    detected: List[str]
    confidence: float
    marking: str     # single / multi / blank
    status: str      # ok / error


class OCRResultPayload(TypedDict):
    version: str
    answers: List[OMRDetectedAnswer]
    raw_text: Optional[str]


==========================================================================================
# FILE: ai_worker/ai/ocr/tesseract.py
==========================================================================================
# apps/worker/ai/ocr/tesseract.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Optional

from PIL import Image  # type: ignore
import pytesseract  # type: ignore


@dataclass
class OCRResult:
    text: str
    confidence: Optional[float] = None
    raw: Optional[Any] = None


def tesseract_ocr(image_path: str) -> OCRResult:
    img = Image.open(image_path)

    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)
    text = "\n".join(data.get("text", [])).strip()

    confs = [c for c in data.get("conf", []) if c != -1]
    confidence = (sum(confs) / len(confs)) if confs else None

    # raw=data Îäî ÎÑàÎ¨¥ ÌÅ¥ Ïàò ÏûàÏñ¥ ÌïÑÏöî ÏãúÎßå ÏºúÍ∏∞
    return OCRResult(text=text, confidence=confidence, raw=None)


==========================================================================================
# FILE: ai_worker/ai/omr/__init__.py
==========================================================================================



==========================================================================================
# FILE: ai_worker/ai/omr/engine.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/identifier.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore

from apps.worker.ai_worker.ai.omr.meta_px import build_page_scale_from_meta, PageScale


BBox = Tuple[int, int, int, int]


@dataclass(frozen=True)
class IdentifierConfigV1:
    """
    Identifier OMR v1 (8 digits, each digit 0~9 single mark).

    Principles:
    - ROI based fill score (same philosophy as detect_omr_answers_v1)
    - Robust to scan/photo noise by sampling a square ROI around each bubble
    - No DB, no external calls, worker-only judgement/extraction
    """
    # Ï£ºÎ≥Ä ROI(Î≤ÑÎ∏î Ï§ëÏã¨ Í∏∞Ï§Ä) ÌôïÏû• Í≥ÑÏàò: r * k
    roi_expand_k: float = 1.55

    # blank ÌåêÎã®: Ìï¥Îãπ digitÏóêÏÑú ÏµúÍ≥† fillÏù¥ Ïù¥ Í∞íÎ≥¥Îã§ ÏûëÏúºÎ©¥ blank
    blank_threshold: float = 0.070

    # ambiguous ÌåêÎã®: top-2 gapÏù¥ Ïù¥ Í∞íÎ≥¥Îã§ ÏûëÏúºÎ©¥ ambiguous
    conf_gap_threshold: float = 0.060

    # (Ïö¥ÏòÅ Ìé∏Ïùò) digit-level confidence clamp
    min_confidence: float = 0.0
    max_confidence: float = 1.0


def _clamp(v: int, lo: int, hi: int) -> int:
    return max(lo, min(hi, v))


def _crop(gray: np.ndarray, bbox: BBox) -> np.ndarray:
    x, y, w, h = bbox
    x = _clamp(int(x), 0, gray.shape[1] - 1)
    y = _clamp(int(y), 0, gray.shape[0] - 1)
    w = max(1, int(w))
    h = max(1, int(h))
    w = min(w, gray.shape[1] - x)
    h = min(h, gray.shape[0] - y)
    return gray[y:y + h, x:x + w]


def _fill_score(roi_gray: np.ndarray) -> float:
    """
    Same core idea as OMR v1:
    - blur
    - OTSU + INV
    - filled pixel ratio
    """
    if roi_gray.size == 0:
        return 0.0

    blur = cv2.GaussianBlur(roi_gray, (5, 5), 0)
    _, th = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    filled = float(np.sum(th > 0))
    total = float(th.size) if th.size > 0 else 1.0
    score = filled / total
    return float(max(0.0, min(1.0, score)))


def _bubble_roi_bbox_px(
    *,
    center_px: Tuple[int, int],
    r_px: int,
    cfg: IdentifierConfigV1,
    img_w: int,
    img_h: int,
) -> BBox:
    cx, cy = center_px
    side = int(round(max(2, r_px) * cfg.roi_expand_k)) * 2
    x = int(cx - side // 2)
    y = int(cy - side // 2)
    x = _clamp(x, 0, img_w - 1)
    y = _clamp(y, 0, img_h - 1)
    w = _clamp(side, 1, img_w - x)
    h = _clamp(side, 1, img_h - y)
    return (x, y, w, h)


def detect_identifier_v1(
    *,
    image_bgr: np.ndarray,
    meta: Dict[str, Any],
    cfg: Optional[IdentifierConfigV1] = None,
) -> Dict[str, Any]:
    """
    Extract identifier(8 digits) from aligned full-page image.

    meta requirements:
      meta["identifier"]["bubbles"] list with:
        - digit_index (1..8)
        - number (0..9)
        - center: {x(mm), y(mm)}
        - r(mm)

    return contract:
      {
        "identifier": "12345678" | None,
        "digits": [{"digit_index":1,"value":1,"status":"ok|blank|ambiguous","confidence":0.91,"marks":[...]}...],
        "confidence": 0.0~1.0,
        "status": "ok|ambiguous|blank|error"
      }
    """
    cfg = cfg or IdentifierConfigV1()

    if image_bgr is None or image_bgr.size == 0:
        return {"identifier": None, "digits": [], "confidence": 0.0, "status": "error"}

    h, w = image_bgr.shape[:2]
    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)

    ident = meta.get("identifier") or {}
    bubbles = list(ident.get("bubbles") or [])
    if not bubbles:
        return {"identifier": None, "digits": [], "confidence": 0.0, "status": "error"}

    scale: PageScale = build_page_scale_from_meta(meta=meta, image_size_px=(w, h))

    # group bubbles by digit_index
    by_digit: Dict[int, List[Dict[str, Any]]] = {}
    for b in bubbles:
        try:
            di = int(b.get("digit_index") or 0)
        except Exception:
            continue
        if di <= 0:
            continue
        by_digit.setdefault(di, []).append(b)

    digits_out: List[Dict[str, Any]] = []
    identifier_chars: List[str] = []
    status_rollup = "ok"
    confidences: List[float] = []

    for digit_index in sorted(by_digit.keys()):
        bs = by_digit[digit_index]

        marks: List[Dict[str, Any]] = []
        for b in bs:
            num = int(b.get("number") or 0)
            c = b.get("center") or {}
            r_mm = float(b.get("r") or 0.0)

            cx_mm = float(c.get("x") or 0.0)
            cy_mm = float(c.get("y") or 0.0)

            cx_px, cy_px = scale.mm_to_px_point(cx_mm, cy_mm)

            # radius: use average scale for robustness (mm->px)
            r_px_x = max(1, scale.mm_to_px_len_x(r_mm))
            r_px_y = max(1, scale.mm_to_px_len_y(r_mm))
            r_px = max(1, int(round((r_px_x + r_px_y) / 2.0)))

            bbox = _bubble_roi_bbox_px(
                center_px=(cx_px, cy_px),
                r_px=r_px,
                cfg=cfg,
                img_w=w,
                img_h=h,
            )
            roi = _crop(gray, bbox)
            fill = _fill_score(roi)

            marks.append(
                {
                    "number": int(num),
                    "fill": float(fill),
                    "center_px": {"x": int(cx_px), "y": int(cy_px)},
                    "roi_px": {"x": int(bbox[0]), "y": int(bbox[1]), "w": int(bbox[2]), "h": int(bbox[3])},
                }
            )

        marks_sorted = sorted(marks, key=lambda m: float(m.get("fill") or 0.0), reverse=True)
        top = marks_sorted[0] if marks_sorted else {"number": 0, "fill": 0.0}
        second = marks_sorted[1] if len(marks_sorted) > 1 else {"number": 0, "fill": 0.0}

        top_fill = float(top.get("fill") or 0.0)
        second_fill = float(second.get("fill") or 0.0)
        gap = float(top_fill - second_fill)

        if top_fill < cfg.blank_threshold:
            digits_out.append(
                {
                    "digit_index": int(digit_index),
                    "value": None,
                    "status": "blank",
                    "confidence": 0.0,
                    "marks": marks_sorted,
                }
            )
            identifier_chars.append("?")
            status_rollup = "blank" if status_rollup == "ok" else status_rollup
            continue

        if gap < cfg.conf_gap_threshold:
            digits_out.append(
                {
                    "digit_index": int(digit_index),
                    "value": int(top.get("number") or 0),
                    "status": "ambiguous",
                    "confidence": float(max(cfg.min_confidence, min(cfg.max_confidence, top_fill))),
                    "gap": float(gap),
                    "marks": marks_sorted,
                }
            )
            identifier_chars.append(str(int(top.get("number") or 0)))
            status_rollup = "ambiguous" if status_rollup in ("ok",) else status_rollup
            confidences.append(float(top_fill))
            continue

        # ok
        conf = float(max(cfg.min_confidence, min(cfg.max_confidence, top_fill)))
        digits_out.append(
            {
                "digit_index": int(digit_index),
                "value": int(top.get("number") or 0),
                "status": "ok",
                "confidence": conf,
                "gap": float(gap),
                "marks": marks_sorted,
            }
        )
        identifier_chars.append(str(int(top.get("number") or 0)))
        confidences.append(conf)

    # identifier validity: must be 8 digits all ok/ambiguous (blankÏù¥Î©¥ ? Ìè¨Ìï®)
    identifier = "".join(identifier_chars)
    if "?" in identifier:
        identifier_final: Optional[str] = None
    else:
        identifier_final = identifier

    # overall confidence: conservative (mean of digit conf where available)
    overall_conf = float(sum(confidences) / len(confidences)) if confidences else 0.0

    return {
        "identifier": identifier_final,
        "raw_identifier": identifier,  # '?' Ìè¨Ìï® Í∞ÄÎä• (Ïö¥ÏòÅ ÎîîÎ≤ÑÍ∑∏/Î¶¨Ìä∏ÎùºÏù¥Ïö©)
        "digits": digits_out,
        "confidence": float(max(0.0, min(1.0, overall_conf))),
        "status": status_rollup,
    }


==========================================================================================
# FILE: ai_worker/ai/omr/identifier.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/identifier.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore

from apps.worker.ai_worker.ai.omr.meta_px import build_page_scale_from_meta, PageScale


BBox = Tuple[int, int, int, int]


@dataclass(frozen=True)
class IdentifierConfigV1:
    """
    Identifier OMR v1 (8 digits, each digit 0~9 single mark).

    Principles:
    - ROI based fill score (same philosophy as detect_omr_answers_v1)
    - Robust to scan/photo noise by sampling a square ROI around each bubble
    - No DB, no external calls, worker-only judgement/extraction
    """
    # (OPS DEFAULT) Ï¥¨ÏòÅ/ÏõåÌîÑÏóêÏÑú Ï§ëÏã¨ Ïò§Ï∞®Î•º Ìù°ÏàòÌïòÍ∏∞ ÏúÑÌï¥ ÏÜåÌè≠ ÌôïÏû•
    # Ï£ºÎ≥Ä ROI(Î≤ÑÎ∏î Ï§ëÏã¨ Í∏∞Ï§Ä) ÌôïÏû• Í≥ÑÏàò: r * k
    roi_expand_k: float = 1.60

    # (OPS DEFAULT) blank Í≥ºÎã§ Î∞©ÏßÄ: Ïã§Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Ïó∞ÌïÑ ÎÜçÎèÑ ÎÇÆÏùÄ ÏºÄÏù¥Ïä§ ÎåÄÏùë
    # blank ÌåêÎã®: Ìï¥Îãπ digitÏóêÏÑú ÏµúÍ≥† fillÏù¥ Ïù¥ Í∞íÎ≥¥Îã§ ÏûëÏúºÎ©¥ blank
    blank_threshold: float = 0.055

    # (OPS DEFAULT) ambiguous Í≥ºÎã§ Î∞©ÏßÄ: top-2 gap Í∏∞Ï§Ä ÏÜåÌè≠ ÏôÑÌôî
    # ambiguous ÌåêÎã®: top-2 gapÏù¥ Ïù¥ Í∞íÎ≥¥Îã§ ÏûëÏúºÎ©¥ ambiguous
    conf_gap_threshold: float = 0.050

    # (Ïö¥ÏòÅ Ìé∏Ïùò) digit-level confidence clamp
    min_confidence: float = 0.0
    max_confidence: float = 1.0


def _clamp(v: int, lo: int, hi: int) -> int:
    return max(lo, min(hi, v))


def _crop(gray: np.ndarray, bbox: BBox) -> np.ndarray:
    x, y, w, h = bbox
    x = _clamp(int(x), 0, gray.shape[1] - 1)
    y = _clamp(int(y), 0, gray.shape[0] - 1)
    w = max(1, int(w))
    h = max(1, int(h))
    w = min(w, gray.shape[1] - x)
    h = min(h, gray.shape[0] - y)
    return gray[y:y + h, x:x + w]


def _fill_score(roi_gray: np.ndarray) -> float:
    """
    Same core idea as OMR v1:
    - blur
    - OTSU + INV
    - filled pixel ratio
    """
    if roi_gray.size == 0:
        return 0.0

    blur = cv2.GaussianBlur(roi_gray, (5, 5), 0)
    _, th = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    filled = float(np.sum(th > 0))
    total = float(th.size) if th.size > 0 else 1.0
    score = filled / total
    return float(max(0.0, min(1.0, score)))


def _bubble_roi_bbox_px(
    *,
    center_px: Tuple[int, int],
    r_px: int,
    cfg: IdentifierConfigV1,
    img_w: int,
    img_h: int,
) -> BBox:
    cx, cy = center_px
    side = int(round(max(2, r_px) * cfg.roi_expand_k)) * 2
    x = int(cx - side // 2)
    y = int(cy - side // 2)
    x = _clamp(x, 0, img_w - 1)
    y = _clamp(y, 0, img_h - 1)
    w = _clamp(side, 1, img_w - x)
    h = _clamp(side, 1, img_h - y)
    return (x, y, w, h)


def detect_identifier_v1(
    *,
    image_bgr: np.ndarray,
    meta: Dict[str, Any],
    cfg: Optional[IdentifierConfigV1] = None,
) -> Dict[str, Any]:
    """
    Extract identifier(8 digits) from aligned full-page image.

    meta requirements:
      meta["identifier"]["bubbles"] list with:
        - digit_index (1..8)
        - number (0..9)
        - center: {x(mm), y(mm)}
        - r(mm)

    return contract:
      {
        "identifier": "12345678" | None,
        "digits": [{"digit_index":1,"value":1,"status":"ok|blank|ambiguous","confidence":0.91,"marks":[...]}...],
        "confidence": 0.0~1.0,
        "status": "ok|ambiguous|blank|error"
      }
    """
    cfg = cfg or IdentifierConfigV1()

    if image_bgr is None or image_bgr.size == 0:
        return {"identifier": None, "digits": [], "confidence": 0.0, "status": "error"}

    h, w = image_bgr.shape[:2]
    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)

    ident = meta.get("identifier") or {}
    bubbles = list(ident.get("bubbles") or [])
    if not bubbles:
        return {"identifier": None, "digits": [], "confidence": 0.0, "status": "error"}

    scale: PageScale = build_page_scale_from_meta(meta=meta, image_size_px=(w, h))

    # group bubbles by digit_index
    by_digit: Dict[int, List[Dict[str, Any]]] = {}
    for b in bubbles:
        try:
            di = int(b.get("digit_index") or 0)
        except Exception:
            continue
        if di <= 0:
            continue
        by_digit.setdefault(di, []).append(b)

    digits_out: List[Dict[str, Any]] = []
    identifier_chars: List[str] = []
    status_rollup = "ok"
    confidences: List[float] = []

    for digit_index in sorted(by_digit.keys()):
        bs = by_digit[digit_index]

        marks: List[Dict[str, Any]] = []
        for b in bs:
            num = int(b.get("number") or 0)
            c = b.get("center") or {}
            r_mm = float(b.get("r") or 0.0)

            cx_mm = float(c.get("x") or 0.0)
            cy_mm = float(c.get("y") or 0.0)

            cx_px, cy_px = scale.mm_to_px_point(cx_mm, cy_mm)

            # radius: use average scale for robustness (mm->px)
            r_px_x = max(1, scale.mm_to_px_len_x(r_mm))
            r_px_y = max(1, scale.mm_to_px_len_y(r_mm))
            r_px = max(1, int(round((r_px_x + r_px_y) / 2.0)))

            bbox = _bubble_roi_bbox_px(
                center_px=(cx_px, cy_px),
                r_px=r_px,
                cfg=cfg,
                img_w=w,
                img_h=h,
            )
            roi = _crop(gray, bbox)
            fill = _fill_score(roi)

            marks.append(
                {
                    "number": int(num),
                    "fill": float(fill),
                    "center_px": {"x": int(cx_px), "y": int(cy_px)},
                    "roi_px": {"x": int(bbox[0]), "y": int(bbox[1]), "w": int(bbox[2]), "h": int(bbox[3])},
                }
            )

        marks_sorted = sorted(marks, key=lambda m: float(m.get("fill") or 0.0), reverse=True)
        top = marks_sorted[0] if marks_sorted else {"number": 0, "fill": 0.0}
        second = marks_sorted[1] if len(marks_sorted) > 1 else {"number": 0, "fill": 0.0}

        top_fill = float(top.get("fill") or 0.0)
        second_fill = float(second.get("fill") or 0.0)
        gap = float(top_fill - second_fill)

        if top_fill < cfg.blank_threshold:
            digits_out.append(
                {
                    "digit_index": int(digit_index),
                    "value": None,
                    "status": "blank",
                    "confidence": 0.0,
                    "marks": marks_sorted,
                }
            )
            identifier_chars.append("?")
            status_rollup = "blank" if status_rollup == "ok" else status_rollup
            continue

        if gap < cfg.conf_gap_threshold:
            digits_out.append(
                {
                    "digit_index": int(digit_index),
                    "value": int(top.get("number") or 0),
                    "status": "ambiguous",
                    "confidence": float(max(cfg.min_confidence, min(cfg.max_confidence, top_fill))),
                    "gap": float(gap),
                    "marks": marks_sorted,
                }
            )
            identifier_chars.append(str(int(top.get("number") or 0)))
            status_rollup = "ambiguous" if status_rollup in ("ok",) else status_rollup
            confidences.append(float(top_fill))
            continue

        # ok
        conf = float(max(cfg.min_confidence, min(cfg.max_confidence, top_fill)))
        digits_out.append(
            {
                "digit_index": int(digit_index),
                "value": int(top.get("number") or 0),
                "status": "ok",
                "confidence": conf,
                "gap": float(gap),
                "marks": marks_sorted,
            }
        )
        identifier_chars.append(str(int(top.get("number") or 0)))
        confidences.append(conf)

    # identifier validity: must be 8 digits all ok/ambiguous (blankÏù¥Î©¥ ? Ìè¨Ìï®)
    identifier = "".join(identifier_chars)
    if "?" in identifier:
        identifier_final: Optional[str] = None
    else:
        identifier_final = identifier

    # overall confidence: conservative (mean of digit conf where available)
    overall_conf = float(sum(confidences) / len(confidences)) if confidences else 0.0

    return {
        "identifier": identifier_final,
        "raw_identifier": identifier,  # '?' Ìè¨Ìï® Í∞ÄÎä• (Ïö¥ÏòÅ ÎîîÎ≤ÑÍ∑∏/Î¶¨Ìä∏ÎùºÏù¥Ïö©)
        "digits": digits_out,
        "confidence": float(max(0.0, min(1.0, overall_conf))),
        "status": status_rollup,
    }


==========================================================================================
# FILE: ai_worker/ai/omr/meta_px.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/meta_px.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, Tuple


def _clamp(v: int, lo: int, hi: int) -> int:
    return max(lo, min(hi, v))


@dataclass(frozen=True)
class PageScale:
    """
    Convert meta(mm) -> px.
    Boundary rule (fixed):
      - meta(mm) is assets single truth
      - px conversion is worker responsibility
      - aligned/warped image must represent the full page
    """
    sx: float
    sy: float
    img_w: int
    img_h: int

    def mm_to_px_point(self, x_mm: float, y_mm: float) -> Tuple[int, int]:
        x = int(round(float(x_mm) * self.sx))
        y = int(round(float(y_mm) * self.sy))
        x = _clamp(x, 0, self.img_w - 1)
        y = _clamp(y, 0, self.img_h - 1)
        return x, y

    def mm_to_px_len_x(self, v_mm: float) -> int:
        return max(1, int(round(float(v_mm) * self.sx)))

    def mm_to_px_len_y(self, v_mm: float) -> int:
        return max(1, int(round(float(v_mm) * self.sy)))


def build_page_scale_from_meta(
    *,
    meta: Dict[str, Any],
    image_size_px: Tuple[int, int],
) -> PageScale:
    """
    Build scaler from template meta.
    meta page size is mm. image_size_px is (width, height).
    """
    img_w, img_h = int(image_size_px[0]), int(image_size_px[1])

    page = meta.get("page") or {}
    size = page.get("size") or {}
    page_w_mm = float(size.get("width") or 0.0)
    page_h_mm = float(size.get("height") or 0.0)

    if img_w <= 0 or img_h <= 0:
        raise ValueError("invalid image_size_px")
    if page_w_mm <= 0.0 or page_h_mm <= 0.0:
        raise ValueError("invalid meta page size")

    sx = img_w / page_w_mm
    sy = img_h / page_h_mm
    return PageScale(sx=float(sx), sy=float(sy), img_w=img_w, img_h=img_h)


==========================================================================================
# FILE: ai_worker/ai/omr/template_meta.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/template_meta.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Tuple, Optional

import requests


@dataclass(frozen=True)
class TemplateMeta:
    raw: Dict[str, Any]

    @property
    def units(self) -> str:
        return str(self.raw.get("units") or "mm")

    @property
    def page_size_mm(self) -> Tuple[float, float]:
        page = self.raw.get("page") or {}
        size = page.get("size") or {}
        return float(size.get("width") or 0.0), float(size.get("height") or 0.0)

    @property
    def questions(self) -> List[Dict[str, Any]]:
        return list(self.raw.get("questions") or [])


class TemplateMetaFetchError(RuntimeError):
    pass


def fetch_objective_meta(
    *,
    base_url: str,
    question_count: int,
    # auth options (choose what your deployment uses)
    auth_cookie_header: Optional[str] = None,
    bearer_token: Optional[str] = None,
    worker_token_header: Optional[str] = None,  # e.g. X-Worker-Token (if API allows)
    extra_headers: Optional[Dict[str, str]] = None,
    timeout: int = 10,
) -> TemplateMeta:
    """
    worker -> API (assets meta)

    Stability goals:
    - clear timeouts
    - explicit error messages
    - flexible auth (cookie/bearer/custom header)
    - returns structured exception for caller to gracefully fallback

    NOTE:
    - assets endpoint currently uses IsAuthenticated.
      Production typically uses cookie session OR bearer token.
      worker_token_header is optional if you later add internal auth for workers.
    """
    url = f"{base_url.rstrip('/')}/api/v1/assets/omr/objective/meta/"
    params = {"question_count": str(int(question_count))}

    headers: Dict[str, str] = {"Accept": "application/json"}
    if auth_cookie_header:
        headers["Cookie"] = auth_cookie_header
    if bearer_token:
        headers["Authorization"] = f"Bearer {bearer_token}"
    if worker_token_header:
        headers["X-Worker-Token"] = str(worker_token_header)
    if extra_headers:
        for k, v in extra_headers.items():
            if k and v:
                headers[str(k)] = str(v)

    try:
        r = requests.get(url, params=params, headers=headers, timeout=timeout)
    except Exception as e:
        raise TemplateMetaFetchError(f"meta_fetch_network_error: {e!r}") from e

    if r.status_code >= 400:
        body = (r.text or "")[:2000]
        raise TemplateMetaFetchError(
            f"meta_fetch_http_error: status={r.status_code} body={body}"
        )

    try:
        data = r.json()
    except Exception as e:
        raise TemplateMetaFetchError(f"meta_fetch_invalid_json: {e!r}") from e

    return TemplateMeta(raw=data)


==========================================================================================
# FILE: ai_worker/ai/omr/types.py
==========================================================================================
# apps/worker/ai/omr/types.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Literal


OMRStatus = Literal["ok", "blank", "ambiguous", "low_confidence", "error"]
OMRMarking = Literal["blank", "single", "multi"]


@dataclass(frozen=True)
class OMRAnswerV1:
    """
    Worker-side OMR payload v1 (question-level)
    This should be embedded into API-side SubmissionAnswer.meta["omr"] later.

    - version: "v1" fixed
    - detected: list[str] ex) ["B"] or ["B","D"]
    - marking: blank/single/multi
    - confidence: 0~1 (top mark confidence)
    - status: ok/blank/ambiguous/low_confidence/error
    - raw: debug info (optional)
    """
    version: str
    question_id: int

    detected: List[str]
    marking: OMRMarking
    confidence: float
    status: OMRStatus

    raw: Optional[Dict[str, Any]] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "version": self.version,
            "question_id": int(self.question_id),
            "detected": list(self.detected or []),
            "marking": self.marking,
            "confidence": float(self.confidence or 0.0),
            "status": self.status,
            "raw": self.raw,
        }


==========================================================================================
# FILE: ai_worker/ai/pipelines/__init__.py
==========================================================================================
# apps/worker/ai/pipelines/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/pipelines/dispatcher.py
==========================================================================================
# apps/worker/ai_worker/ai/pipelines/dispatcher.py
from __future__ import annotations

from typing import Any, Dict

from apps.shared.contracts.ai_job import AIJob
from apps.shared.contracts.ai_result import AIResult

from apps.worker.ai_worker.ai.config import AIConfig
from apps.worker.ai_worker.ai.ocr.google import google_ocr
from apps.worker.ai_worker.ai.ocr.tesseract import tesseract_ocr
from apps.worker.ai_worker.ai.detection.segment_dispatcher import segment_questions
from apps.worker.ai_worker.ai.handwriting.detector import analyze_handwriting
from apps.worker.ai_worker.ai.embedding.service import get_embeddings
from apps.worker.ai_worker.ai.problem.generator import generate_problem_from_ocr
from apps.worker.ai_worker.ai.pipelines.homework_video_analyzer import analyze_homework_video
from apps.worker.ai_worker.storage.downloader import download_to_tmp


def handle_ai_job(job: AIJob) -> AIResult:
    try:
        cfg = AIConfig.load()
        payload: Dict[str, Any] = job.payload or {}

        download_url = payload.get("download_url")
        if not download_url:
            return AIResult.failed(job.id, "download_url missing")

        local_path = download_to_tmp(
            download_url=download_url,
            job_id=str(job.id),
        )

        # --------------------------------------------------
        # OCR
        # --------------------------------------------------
        if job.type == "ocr":
            engine = (payload.get("engine") or cfg.OCR_ENGINE or "auto").lower()

            if engine == "tesseract":
                r = tesseract_ocr(local_path)
            elif engine == "google":
                r = google_ocr(local_path)
            else:
                try:
                    r = google_ocr(local_path)
                    if not r.text.strip():
                        r = tesseract_ocr(local_path)
                except Exception:
                    r = tesseract_ocr(local_path)

            return AIResult.done(
                job.id,
                {"text": r.text, "confidence": r.confidence},
            )

        # --------------------------------------------------
        # Question segmentation
        # --------------------------------------------------
        if job.type == "question_segmentation":
            boxes = segment_questions(local_path)
            return AIResult.done(job.id, {"boxes": boxes})

        # --------------------------------------------------
        # Handwriting analysis
        # --------------------------------------------------
        if job.type == "handwriting_analysis":
            scores = analyze_handwriting(local_path)
            return AIResult.done(job.id, scores)

        # --------------------------------------------------
        # Embedding
        # --------------------------------------------------
        if job.type == "embedding":
            texts = payload.get("texts") or []
            batch = get_embeddings(list(texts))
            return AIResult.done(
                job.id,
                {"backend": batch.backend, "vectors": batch.vectors},
            )

        # --------------------------------------------------
        # Problem generation
        # --------------------------------------------------
        if job.type == "problem_generation":
            ocr_text = payload.get("ocr_text") or ""
            parsed = generate_problem_from_ocr(ocr_text)
            return AIResult.done(
                job.id,
                {
                    "body": parsed.body,
                    "choices": parsed.choices,
                    "answer": parsed.answer,
                    "difficulty": parsed.difficulty,
                    "tag": parsed.tag,
                    "summary": parsed.summary,
                    "explanation": parsed.explanation,
                },
            )

        # --------------------------------------------------
        # Homework video analysis
        # --------------------------------------------------
        if job.type == "homework_video_analysis":
            frame_stride = int(payload.get("frame_stride") or 10)
            min_frame_count = int(payload.get("min_frame_count") or 30)
            analysis = analyze_homework_video(
                video_path=local_path,
                frame_stride=frame_stride,
                min_frame_count=min_frame_count,
            )
            return AIResult.done(job.id, analysis)

        # --------------------------------------------------
        # OMR grading (meta-aware) - production final
        # --------------------------------------------------
        if job.type == "omr_grading":
            """
            payload options (recommended):
              - mode: "scan" | "photo" | "auto" (default auto)
              - question_count: 10|20|30 (required if template_fetch is used)
              - template_meta: dict (inject meta directly; no API call)
              - template_fetch:
                    {
                      "base_url": "...",
                      "cookie": "...",
                      "bearer_token": "...",
                      "worker_token": "...",
                      "timeout": 10
                    }

            Output contract:
              {
                "version": "v1",
                "mode": "...",
                "aligned": true|false,
                "identifier": {...},
                "answers": [...],
                "meta_used": true|false
              }
            """
            import cv2  # type: ignore

            from apps.worker.ai_worker.ai.omr.engine import detect_omr_answers_v1, OMRConfigV1
            from apps.worker.ai_worker.ai.omr.roi_builder import build_questions_payload_from_meta
            from apps.worker.ai_worker.ai.omr.warp import warp_to_a4_landscape
            from apps.worker.ai_worker.ai.omr.template_meta import fetch_objective_meta, TemplateMetaFetchError
            from apps.worker.ai_worker.ai.omr.identifier import detect_identifier_v1, IdentifierConfigV1

            mode = str(payload.get("mode") or "auto").lower()
            if mode not in ("scan", "photo", "auto"):
                mode = "auto"

            # 1) meta ÌôïÎ≥¥
            meta = payload.get("template_meta")
            meta_used = False
            meta_fetch_error = None

            if not meta:
                tf = payload.get("template_fetch") or {}
                base_url = tf.get("base_url")
                if base_url:
                    qc = int(payload.get("question_count") or 0)
                    if qc not in (10, 20, 30):
                        return AIResult.failed(job.id, "question_count required (10|20|30) for template_fetch")

                    try:
                        meta_obj = fetch_objective_meta(
                            base_url=str(base_url),
                            question_count=qc,
                            auth_cookie_header=tf.get("cookie"),
                            bearer_token=tf.get("bearer_token"),
                            worker_token_header=tf.get("worker_token"),
                            timeout=int(tf.get("timeout") or 10),
                        )
                        meta = meta_obj.raw
                        meta_used = True
                    except TemplateMetaFetchError as e:
                        meta = None
                        meta_fetch_error = str(e)[:500]

            # 2) Ïù¥ÎØ∏ÏßÄ Î°úÎìú
            img_bgr = cv2.imread(local_path)
            if img_bgr is None:
                return AIResult.failed(job.id, "cannot read image")

            aligned = img_bgr

            # 3) mode Ï†ïÏ±Ö
            if mode == "photo":
                warped = warp_to_a4_landscape(img_bgr)
                if warped is None:
                    return AIResult.failed(job.id, "warp_failed_for_photo_mode")
                aligned = warped

            elif mode == "auto":
                warped = warp_to_a4_landscape(img_bgr)
                if warped is not None:
                    aligned = warped

            # 4) meta ÏóÜÏúºÎ©¥ legacy
            if not meta:
                questions = payload.get("questions") or []
                if not questions:
                    return AIResult.failed(job.id, "template_meta/template_fetch failed and legacy questions missing")

                answers = detect_omr_answers_v1(
                    image_path=local_path,
                    questions=list(questions),
                    cfg=None,
                )
                return AIResult.done(
                    job.id,
                    {
                        "version": "v1",
                        "mode": "legacy_questions",
                        "aligned": False,
                        "identifier": None,
                        "answers": answers,
                        "meta_used": False,
                        "debug": {
                            "meta_fetch_error": meta_fetch_error,
                        },
                    },
                )

            # 5) ROI + identifier
            h, w = aligned.shape[:2]
            questions_payload = build_questions_payload_from_meta(meta, (w, h))

            ident = detect_identifier_v1(
                image_bgr=aligned,
                meta=meta,
                cfg=IdentifierConfigV1(),
            )

            # 6) aligned Ï†ÄÏû• ÌõÑ OMR
            import tempfile, os

            tmp_path = os.path.join(tempfile.gettempdir(), f"omr_aligned_{job.id}.jpg")
            cv2.imwrite(tmp_path, aligned)

            cfg = OMRConfigV1()
            answers = detect_omr_answers_v1(
                image_path=tmp_path,
                questions=list(questions_payload),
                cfg=cfg,
            )

            return AIResult.done(
                job.id,
                {
                    "version": "v1",
                    "mode": mode,
                    "aligned": bool(aligned is not img_bgr),
                    "identifier": ident,
                    "answers": answers,
                    "meta_used": meta_used,
                    "debug": {
                        "meta_fetch_error": meta_fetch_error,
                    },
                },
            )

        return AIResult.failed(job.id, f"Unsupported job type: {job.type}")

    except Exception as e:
        return AIResult.failed(job.id, str(e))


==========================================================================================
# FILE: ai_worker/ai/pipelines/homework_video_analyzer.py
==========================================================================================
# apps/worker/ai/pipelines/homework_video_analyzer.py
from __future__ import annotations

from typing import Dict, Any, List
import cv2  # type: ignore
import numpy as np  # type: ignore


def _estimate_writing_score(gray_roi: np.ndarray) -> float:
    if gray_roi.size == 0:
        return 0.0
    dark = (gray_roi < 220).sum()
    total = gray_roi.size
    return float(dark) / float(total)


def analyze_homework_video(
    video_path: str,
    frame_stride: int = 10,
    min_frame_count: int = 30,
) -> Dict[str, Any]:
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"cannot open video: {video_path}")

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_idx = 0
    frame_results: List[Dict[str, Any]] = []

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        if frame_idx % frame_stride != 0:
            frame_idx += 1
            continue

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        score = _estimate_writing_score(gray)

        frame_results.append(
            {
                "index": frame_idx,
                "writing_score": round(float(score), 4),
                "has_writing": bool(score >= 0.05),
            }
        )
        frame_idx += 1

    cap.release()

    sampled = len(frame_results)
    if sampled == 0:
        return {
            "total_frames": total_frames,
            "sampled_frames": 0,
            "avg_writing_score": 0.0,
            "filled_ratio": 0.0,
            "frames": [],
            "too_short": total_frames < min_frame_count,
        }

    avg = sum(fr["writing_score"] for fr in frame_results) / sampled
    filled = sum(1 for fr in frame_results if fr["has_writing"])
    ratio = filled / sampled

    return {
        "total_frames": total_frames,
        "sampled_frames": sampled,
        "avg_writing_score": round(float(avg), 4),
        "filled_ratio": round(float(ratio), 4),
        "frames": frame_results,
        "too_short": total_frames < min_frame_count,
    }


==========================================================================================
# FILE: ai_worker/ai/problem/__init__.py
==========================================================================================
# apps/worker/ai/problem/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/problem/generator.py
==========================================================================================
# apps/worker/ai/problem/generator.py
from __future__ import annotations

import json
from dataclasses import dataclass
from typing import Optional

from apps.worker.ai_worker.ai.config import AIConfig
from apps.worker.ai_worker.ai.problem.prompt import BASE_PROMPT

try:
    from openai import OpenAI  # type: ignore
except Exception:
    OpenAI = None  # type: ignore


@dataclass
class ParsedProblem:
    body: str
    choices: list
    answer: Optional[str]
    difficulty: int
    tag: str
    summary: str
    explanation: str


_client: Optional["OpenAI"] = None


def _get_client() -> "OpenAI":
    global _client
    if _client is not None:
        return _client

    if OpenAI is None:
        raise RuntimeError("openai not installed")

    cfg = AIConfig.load()
    if not cfg.OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY not set")

    _client = OpenAI(api_key=cfg.OPENAI_API_KEY)
    return _client


def generate_problem_from_ocr(ocr_text: str) -> ParsedProblem:
    cfg = AIConfig.load()
    prompt = BASE_PROMPT.format(ocr_text=ocr_text)

    client = _get_client()
    response = client.chat.completions.create(
        model=cfg.PROBLEM_GEN_MODEL,
        messages=[
            {"role": "system", "content": "ÎãπÏã†ÏùÄ ÍµêÏú°Ïö© ÏãúÌóò Î¨∏Ï†úÎ•º ÏûêÎèô ÏÉùÏÑ±ÌïòÎäî ÏóîÏßÑÏûÖÎãàÎã§."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.2,
    )

    # SDK ÌòïÌÉú Ï∞®Ïù¥ Î∞©Ïñ¥
    msg = response.choices[0].message
    content = getattr(msg, "content", None) or msg.get("content")  # type: ignore

    data = json.loads(content)

    return ParsedProblem(
        body=data.get("body", ""),
        choices=data.get("choices", []),
        answer=data.get("answer"),
        difficulty=int(data.get("difficulty", 3)),
        tag=data.get("tag", ""),
        summary=data.get("summary", ""),
        explanation=data.get("explanation", ""),
    )


==========================================================================================
# FILE: ai_worker/ai/problem/prompt.py
==========================================================================================
# apps/worker/ai/problem/prompt.py
BASE_PROMPT = """
Îã§ÏùåÏùÄ ÏãúÌóò Î¨∏Ï†úÏùò OCR Í≤∞Í≥ºÏûÖÎãàÎã§.
ÏïÑÎûò ÌÖçÏä§Ìä∏Î•º Í∏∞Î∞òÏúºÎ°ú Î¨∏Ï†ú Ï†ïÎ≥¥Î•º JSON ÌòïÏãùÏúºÎ°ú Ï∂îÏ∂úÌïòÏÑ∏Ïöî.

ÏöîÍµ¨ÏÇ¨Ìï≠:
1) Î¨∏Ï†ú Î≥∏Î¨∏ (body)
2) ÏÑ†ÌÉùÏßÄ (choices): ÏóÜÏúºÎ©¥ Îπà Î∞∞Ïó¥
3) Ï†ïÎãµ (answer): Î™ÖÏãúÎêú Ï†ïÎãµÏù¥ ÏóÜÏúºÎ©¥ AIÍ∞Ä Ï∂îÎ°†, Ï∂îÎ°† Î∂àÍ∞Ä Ïãú null
4) ÎÇúÏù¥ÎèÑ (difficulty): 1~5 Ï†ïÏàòÎ°ú Ï∂îÏ†ï
5) ÌÉúÍ∑∏ (tag): ÏàòÌïô/Í≥ºÌïô/Íµ≠Ïñ¥ Îì± Í∞ÑÎã®Ìïú Î∂ÑÎ•ò
6) Î¨∏Ï†ú ÏöîÏïΩ (summary)
7) Ìï¥ÏÑ§ (explanation): Í∞ÑÎã®Î™ÖÎ£åÌïòÍ≤å

Ï∂úÎ†•ÏùÄ Î∞òÎìúÏãú JSON ÌòïÏãùÎßå ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî. Îã§Î•∏ ÌÖçÏä§Ìä∏Îäî Ìè¨Ìï®ÌïòÏßÄ ÎßàÏÑ∏Ïöî.

Ï∂úÎ†• ÌòïÏãù ÏòàÏãú:
{
  "body": "...",
  "choices": ["A...", "B...", "C...", "D..."],
  "answer": "C",
  "difficulty": 3,
  "tag": "ÏàòÌïô",
  "summary": "...",
  "explanation": "..."
}

OCR ÌÖçÏä§Ìä∏:
\"\"\"
{ocr_text}
\"\"\"
"""


==========================================================================================
# FILE: ai_worker/apps/worker/ai_worker/celery.py
==========================================================================================



==========================================================================================
# FILE: ai_worker/storage/__init__.py
==========================================================================================



==========================================================================================
# FILE: ai_worker/storage/downloader.py
==========================================================================================
# ==============================================================================
# PATH: apps/worker/storage/downloader.py
#
# PURPOSE:
# - AI worker Ï†ÑÏö© ÌååÏùº Îã§Ïö¥Î°úÎìú Ïú†Ìã∏
# - presigned GET URL ‚Üí /tmp local file
# - R2 / S3 credential ÏÇ¨Ïö© ‚ùå
# - video_worker ÏΩîÎìúÏôÄ Ï†àÎåÄ Í≥µÏú†ÌïòÏßÄ ÏïäÏùå
# ==============================================================================

from __future__ import annotations

import os
import tempfile
import requests
from pathlib import Path


def download_to_tmp(
    *,
    download_url: str,
    job_id: str,
    suffix: str | None = None,
    timeout: int = 60,
    chunk_size: int = 1024 * 1024,  # 1MB
) -> str:
    """
    presigned GET URLÎ°ú ÌååÏùºÏùÑ Îã§Ïö¥Î°úÎìúÌïòÏó¨ local temp file Í≤ΩÎ°ú Î∞òÌôò

    Í∑úÏπô:
    - AI workerÎäî URLÎßå Ïã†Î¢∞
    - ÌååÏùºÏùÄ /tmp ÎòêÎäî OS temp dirÏóê ÏÉùÏÑ±
    - Ìò∏Ï∂úÏûêÎäî Î∞òÌôòÎêú pathÎßå ÏÇ¨Ïö©
    """

    tmp_dir = Path(tempfile.gettempdir())
    ext = suffix or ""

    filename = f"ai_job_{job_id}{ext}"
    tmp_path = tmp_dir / filename
    part_path = tmp_dir / f"{filename}.part"

    try:
        with requests.get(download_url, stream=True, timeout=timeout) as r:
            r.raise_for_status()

            expected_len = r.headers.get("Content-Length")
            expected_len = int(expected_len) if expected_len and expected_len.isdigit() else None

            written = 0
            with open(part_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=chunk_size):
                    if not chunk:
                        continue
                    f.write(chunk)
                    written += len(chunk)

            if expected_len is not None and written != expected_len:
                raise RuntimeError(
                    f"download size mismatch (expected={expected_len}, got={written})"
                )

        part_path.replace(tmp_path)
        return str(tmp_path)

    except Exception:
        try:
            if part_path.exists():
                part_path.unlink()
        except Exception:
            pass
        raise


==========================================================================================
# FILE: ai_worker/wrong_notes/__init__.py
==========================================================================================
# apps/worker/wrong_notes/__init__.py


==========================================================================================
# FILE: ai_worker/wrong_notes/templates/wrong_note.html
==========================================================================================
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="utf-8" />
    <style>
        body {
            font-family: Arial, Helvetica, sans-serif;
            font-size: 12px;
            line-height: 1.6;
        }
        h1 {
            border-bottom: 2px solid #333;
            padding-bottom: 4px;
        }
        .exam {
            margin-top: 20px;
        }
        .question {
            margin-left: 12px;
        }
        .wrong {
            color: #c0392b;
        }
    </style>
</head>
<body>
    <h1>Ïò§Îãµ ÎÖ∏Ìä∏</h1>

    <p>
        ÌïôÏÉù ID: {{ enrollment_id }}<br/>
        ÏÉùÏÑ±Ïùº: {{ created_at }}
    </p>

    {% for exam_id, items in grouped.items %}
        <div class="exam">
            <h2>ÏãúÌóò {{ exam_id }}</h2>
            {% for item in items %}
                <div class="question">
                    <span class="wrong">
                        Q{{ item.question_id }}
                    </span>
                    / Ï†úÏ∂ú ÎãµÏïà: {{ item.answer }}
                </div>
            {% endfor %}
        </div>
    {% endfor %}
</body>
</html>


==========================================================================================
# FILE: messaging/__init__.py
==========================================================================================



==========================================================================================
# FILE: messaging/kakao/__init__.py
==========================================================================================



==========================================================================================
# FILE: messaging/pipelines/__init__.py
==========================================================================================



==========================================================================================
# FILE: messaging/sms/__init__.py
==========================================================================================



==========================================================================================
# FILE: omr/roi_builder.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/roi_builder.py
from __future__ import annotations

from typing import Any, Dict, List, Tuple

import math


def _clamp(v: int, lo: int, hi: int) -> int:
    return max(lo, min(hi, v))


def build_questions_payload_from_meta(
    *,
    meta: Dict[str, Any],
    image_size_px: Tuple[int, int],
) -> List[Dict[str, Any]]:
    """
    meta(mm) -> questions payload (px) for detect_omr_answers_v1()

    detect_omr_answers_v1 expects:
      questions: [{question_id, roi:{x,y,w,h}, choices:[...], axis:"x"}, ...]

    image_size_px: (width, height)
    """
    img_w, img_h = image_size_px

    page = meta.get("page") or {}
    size = page.get("size") or {}
    page_w_mm = float(size.get("width") or 0.0)
    page_h_mm = float(size.get("height") or 0.0)
    if page_w_mm <= 0.0 or page_h_mm <= 0.0:
        raise ValueError("invalid meta page size")

    # Ï†ïÎ†¨Îêú Ïä§Ï∫î/ÏõåÌîÑ Í≤∞Í≥ºÎäî "ÌéòÏù¥ÏßÄ Ï†ÑÏ≤¥Í∞Ä Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤¥"ÎùºÍ≥† Í∞ÄÏ†ï
    sx = img_w / page_w_mm
    sy = img_h / page_h_mm

    out: List[Dict[str, Any]] = []
    for q in (meta.get("questions") or []):
        qnum = int(q.get("question_number") or 0)
        roi = q.get("roi") or {}

        x_mm = float(roi.get("x") or 0.0)
        y_mm = float(roi.get("y") or 0.0)
        w_mm = float(roi.get("w") or 0.0)
        h_mm = float(roi.get("h") or 0.0)

        x = int(round(x_mm * sx))
        y = int(round(y_mm * sy))
        w = int(round(w_mm * sx))
        h = int(round(h_mm * sy))

        # ÏïàÏ†Ñ ÌÅ¥Îû®ÌîÑ
        x = _clamp(x, 0, img_w - 1)
        y = _clamp(y, 0, img_h - 1)
        w = _clamp(w, 1, img_w - x)
        h = _clamp(h, 1, img_h - y)

        out.append(
            {
                "question_id": qnum,  # worker ÏóîÏßÑÏùÄ question_idÎßå ÏÇ¨Ïö©
                "roi": {"x": x, "y": y, "w": w, "h": h},
                "choices": ["A", "B", "C", "D", "E"],
                "axis": "x",
            }
        )

    # question_id ÏàúÏÑú Î≥¥Ïû•
    out.sort(key=lambda d: int(d.get("question_id") or 0))
    return out


==========================================================================================
# FILE: omr/template_meta.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/template_meta.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Tuple, Optional
import requests


@dataclass(frozen=True)
class TemplateMeta:
    raw: Dict[str, Any]

    @property
    def units(self) -> str:
        return str(self.raw.get("units") or "mm")

    @property
    def page_size_mm(self) -> Tuple[float, float]:
        page = self.raw.get("page") or {}
        size = page.get("size") or {}
        return float(size.get("width") or 0.0), float(size.get("height") or 0.0)

    @property
    def questions(self) -> List[Dict[str, Any]]:
        return list(self.raw.get("questions") or [])


def fetch_objective_meta(
    *,
    base_url: str,
    question_count: int,
    auth_cookie_header: Optional[str] = None,
    timeout: int = 10,
) -> TemplateMeta:
    """
    worker -> API (assets meta)
    - Ïô∏Î∂Ä SaaS Ìò∏Ï∂ú Í∏àÏßÄ: ÎÇ¥Î∂Ä APIÎßå Ìò∏Ï∂ú
    - auth Î∞©ÏãùÏùÄ Ïö¥ÏòÅ ÌôòÍ≤ΩÏóê ÎßûÍ≤å header/cookieÎ•º Ï†ÑÎã¨
    """
    url = f"{base_url.rstrip('/')}/api/v1/assets/omr/objective/meta/"
    params = {"question_count": str(int(question_count))}

    headers: Dict[str, str] = {}
    if auth_cookie_header:
        headers["Cookie"] = auth_cookie_header

    r = requests.get(url, params=params, headers=headers, timeout=timeout)
    r.raise_for_status()
    data = r.json()
    return TemplateMeta(raw=data)


==========================================================================================
# FILE: omr/warp.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/warp.py
from __future__ import annotations

from typing import Optional, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore


def _order_points(pts: np.ndarray) -> np.ndarray:
    # pts: (4,2)
    rect = np.zeros((4, 2), dtype=np.float32)
    s = pts.sum(axis=1)
    rect[0] = pts[np.argmin(s)]  # top-left
    rect[2] = pts[np.argmax(s)]  # bottom-right

    diff = np.diff(pts, axis=1)
    rect[1] = pts[np.argmin(diff)]  # top-right
    rect[3] = pts[np.argmax(diff)]  # bottom-left
    return rect


def warp_to_a4_landscape(
    *,
    image_bgr: np.ndarray,
    out_size_px: Tuple[int, int] = (3508, 2480),  # 300dpi A4 landscape (Í∑ºÏÇ¨)
) -> Optional[np.ndarray]:
    """
    Ï¥¨ÏòÅ/ÌîÑÎ†àÏûÑ Ïù¥ÎØ∏ÏßÄÏóêÏÑú Î¨∏ÏÑú(ÎãµÏïàÏßÄ) Ïô∏Í≥ΩÏùÑ Ï∞æÏïÑ A4 landscapeÎ°ú ÏõåÌîÑ.
    ÏÑ±Í≥µÌïòÎ©¥ "ÌéòÏù¥ÏßÄ Ï†ÑÏ≤¥ = Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤¥"Í∞Ä ÎêòÎØÄÎ°ú meta ROIÎ•º Í∑∏ÎåÄÎ°ú Ï†ÅÏö© Í∞ÄÎä•.

    Ïã§Ìå®ÌïòÎ©¥ None Î∞òÌôò -> callerÍ∞Ä fallback(yolo/opencv segmentation Îì±) Ï≤òÎ¶¨
    """
    if image_bgr is None or image_bgr.size == 0:
        return None

    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5, 5), 0)
    edges = cv2.Canny(blur, 50, 150)

    # Ïú§Í≥Ω Í∞ïÌôî
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))
    edges = cv2.dilate(edges, kernel, iterations=1)

    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        return None

    contours = sorted(contours, key=cv2.contourArea, reverse=True)

    page_cnt = None
    for cnt in contours[:8]:
        peri = cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)
        if len(approx) == 4:
            page_cnt = approx
            break

    if page_cnt is None:
        return None

    pts = page_cnt.reshape(4, 2).astype(np.float32)
    rect = _order_points(pts)

    out_w, out_h = out_size_px
    dst = np.array(
        [
            [0, 0],
            [out_w - 1, 0],
            [out_w - 1, out_h - 1],
            [0, out_h - 1],
        ],
        dtype=np.float32,
    )

    M = cv2.getPerspectiveTransform(rect, dst)
    warped = cv2.warpPerspective(image_bgr, M, (out_w, out_h))
    return warped


==========================================================================================
# FILE: storage/__init__.py
==========================================================================================



==========================================================================================
# FILE: storage/downloader.py
==========================================================================================
# PATH: apps/worker/storage/downloader.py
# Ïó≠Ìï†: Î™®Îì† AI worker ÏûÖÎ†• ÌååÏùº Îã§Ïö¥Î°úÎìú (Í≥µÌÜµ)

import os
import requests
from urllib.parse import urlparse


def download_to_tmp(download_url: str, job_id: str) -> str:
    """
    presigned GET URLÏóêÏÑú ÌååÏùº Îã§Ïö¥Î°úÎìú ‚Üí /tmp
    """
    parsed = urlparse(download_url)
    filename = os.path.basename(parsed.path)

    job_dir = f"/tmp/ai_job_{job_id}"
    os.makedirs(job_dir, exist_ok=True)

    local_path = os.path.join(job_dir, filename)

    with requests.get(download_url, stream=True, timeout=30) as r:
        r.raise_for_status()
        with open(local_path, "wb") as f:
            for chunk in r.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)

    return local_path


==========================================================================================
# FILE: storage/r2_client.py
==========================================================================================



==========================================================================================
# FILE: video_worker/__init__.py
==========================================================================================



==========================================================================================
# FILE: video_worker/celery.py
==========================================================================================
# apps/worker/video_worker/celery.py

from celery import Celery

app = Celery("academy_video")

app.config_from_object(
    "django.conf:settings",
    namespace="CELERY",
)

app.conf.worker_state_db = None

# üé¨ ÎπÑÎîîÏò§ Ï†ÑÏö© taskÎßå
app.autodiscover_tasks([
    "apps.shared.tasks.media",
])

print("üé¨ Video Celery READY üé¨")


==========================================================================================
# FILE: video_worker/r2_uploader.py
==========================================================================================
# apps/worker/video_worker/r2_uploader.py

import boto3
import mimetypes
from pathlib import Path
from django.conf import settings

# ---------------------------------------------------------------------
# R2 S3 Client (Í≥µÏö©)
# ---------------------------------------------------------------------

s3 = boto3.client(
    "s3",
    endpoint_url=settings.R2_ENDPOINT,
    aws_access_key_id=settings.R2_ACCESS_KEY,
    aws_secret_access_key=settings.R2_SECRET_KEY,
    region_name="auto",
)

# ---------------------------------------------------------------------
# Upload helpers (Í∏∞Ï°¥)
# ---------------------------------------------------------------------

def upload_fileobj_to_r2(*, fileobj, key: str, content_type: str | None = None):
    """
    Django UploadedFile -> R2 ÏóÖÎ°úÎìú
    """
    s3.upload_fileobj(
        Fileobj=fileobj,
        Bucket=settings.R2_VIDEO_BUCKET,
        Key=key,
        ExtraArgs={
            "ContentType": content_type or "application/octet-stream"
        },
    )


def upload_dir(local_dir: Path, prefix: str):
    """
    local_dir Ï†ÑÏ≤¥Î•º prefix Í∏∞Ï§ÄÏúºÎ°ú R2Ïóê ÏóÖÎ°úÎìú
    """
    for path in local_dir.rglob("*"):
        if not path.is_file():
            continue

        relative_path = path.relative_to(local_dir).as_posix()
        key = f"{prefix}/{relative_path}"

        content_type, _ = mimetypes.guess_type(path.name)

        s3.upload_file(
            str(path),
            settings.R2_VIDEO_BUCKET,
            key,
            ExtraArgs={
                "ContentType": content_type or "application/octet-stream"
            },
        )

# ---------------------------------------------------------------------
# ‚≠ê STEP 2 ÌïµÏã¨: presigned GET URL ÏÉùÏÑ±
# ---------------------------------------------------------------------

def generate_presigned_get_url(
    *,
    key: str,
    expires_in: int = 3600,
) -> str:
    """
    R2 objectÎ•º ÏûÑÏãúÎ°ú Îã§Ïö¥Î°úÎìúÌï† Ïàò ÏûàÎäî presigned GET URL ÏÉùÏÑ±

    - API ÏÑúÎ≤ÑÏóêÏÑúÎßå Ìò∏Ï∂ú
    - workerÎäî Ïù¥ URLÎßå ÏÇ¨Ïö© (R2 credential Î∂àÌïÑÏöî)
    """
    return s3.generate_presigned_url(
        ClientMethod="get_object",
        Params={
            "Bucket": settings.R2_VIDEO_BUCKET,
            "Key": key,
        },
        ExpiresIn=expires_in,
    )


==========================================================================================
# FILE: video_worker/drm/__init__.py
==========================================================================================



==========================================================================================
# FILE: video_worker/video/__init__.py
==========================================================================================



==========================================================================================
# FILE: video_worker/video/processor.py
==========================================================================================
# apps/worker/media/video/processor.py

from __future__ import annotations

import json
import shutil
import subprocess
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Optional, Union

import requests

from .transcoder import transcode_to_hls


# ---------------------------------------------------------------------
# Errors (processor -> task boundary)
# ---------------------------------------------------------------------

class MediaProcessingError(RuntimeError):
    """
    Processor-level error with structured context.
    Task layer should catch this and mark Video as FAILED.
    """

    def __init__(
        self,
        *,
        stage: str,
        code: str,
        message: str,
        context: Optional[Dict[str, Any]] = None,
    ) -> None:
        super().__init__(message)
        self.stage = stage
        self.code = code
        self.context = context or {}

    def to_dict(self) -> Dict[str, Any]:
        return {
            "stage": self.stage,
            "code": self.code,
            "message": str(self),
            "context": self.context,
        }


# ---------------------------------------------------------------------
# Result contract (processor -> task)
# ---------------------------------------------------------------------

@dataclass(frozen=True)
class ProcessResult:
    video_id: int
    duration_seconds: float
    thumbnail_path: Path
    master_playlist_path: Path
    output_root: Path


# ---------------------------------------------------------------------
# Processor (single responsibility: make one video READY)
# ---------------------------------------------------------------------

class VideoProcessor:
    """
    The single authority that turns one uploaded video into READY artifacts:
      - download source to local
      - duration (ffprobe local)
      - thumbnail (ffmpeg local)
      - HLS (transcoder.transcode_to_hls local)
      - verification (master.m3u8 exists)
    """

    def run(
        self,
        *,
        video_id: int,
        input_url: str,
        output_root: Union[str, Path],
        timeout_download: Optional[int] = 60 * 30,
        timeout_probe: Optional[int] = 60,
        timeout_thumbnail: Optional[int] = 120,
        timeout_hls: Optional[int] = None,
        cleanup_source: bool = False,
    ) -> ProcessResult:
        out_root = Path(output_root)

        # 1) pre-clean
        self._pre_clean_output_root(
            video_id=video_id,
            output_root=out_root,
        )

        # 2) download source to local (Ï†ïÏÑù)
        local_input_path = self._download_source(
            video_id=video_id,
            input_url=input_url,
            output_root=out_root,
            timeout=timeout_download,
        )

        try:
            # 3) probe duration (local)
            duration = self._probe_duration_seconds(
                video_id=video_id,
                input_path=str(local_input_path),
                timeout=timeout_probe,
            )

            # 4) thumbnail (local)
            thumb_path = out_root / "thumbnail.jpg"
            self._generate_thumbnail(
                video_id=video_id,
                input_path=str(local_input_path),
                output_path=thumb_path,
                timeout=timeout_thumbnail,
            )

            # 5) HLS transcode (local)
            master_path = self._transcode_hls(
                video_id=video_id,
                input_path=str(local_input_path),
                output_root=out_root, 
                timeout=timeout_hls,
            )

            # 6) verify
            self._verify_ready(
                video_id=video_id,
                master_path=master_path,
            )

            return ProcessResult(
                video_id=video_id,
                duration_seconds=duration,
                thumbnail_path=thumb_path,
                master_playlist_path=master_path,
                output_root=out_root,
            )

        finally:
            if cleanup_source:
                try:
                    if local_input_path.exists():
                        local_input_path.unlink()
                except Exception:
                    pass

    # -----------------------------------------------------------------
    # Internal steps
    # -----------------------------------------------------------------

    def _pre_clean_output_root(self, *, video_id: int, output_root: Path) -> None:
        """
        Delete output_root entirely if exists, then recreate.
        Prevents stale artifacts from making retries look successful.
        """
        try:
            if output_root.exists():
                shutil.rmtree(output_root)
            output_root.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            raise MediaProcessingError(
                stage="CLEAN",
                code="CLEAN_FAILED",
                message=f"Failed to pre-clean output_root (video_id={video_id})",
                context={
                    "video_id": video_id,
                    "output_root": str(output_root),
                    "error": repr(e),
                },
            ) from e

    def _download_source(
        self,
        *,
        video_id: int,
        input_url: str,
        output_root: Path,
        timeout: Optional[int] = 60 * 30,  # 30m
        chunk_size: int = 1024 * 1024,      # 1MB
    ) -> Path:
        """
        Download source MP4 from presigned GET URL to local disk.
        """
        output_root.mkdir(parents=True, exist_ok=True)

        final_path = output_root / "_source.mp4"
        tmp_path = output_root / "_source.mp4.part"

        try:
            with requests.get(input_url, stream=True, timeout=30) as r:
                r.raise_for_status()

                expected_len = r.headers.get("Content-Length")
                expected_len_int = int(expected_len) if expected_len and expected_len.isdigit() else None

                written = 0
                with open(tmp_path, "wb") as f:
                    for chunk in r.iter_content(chunk_size=chunk_size):
                        if not chunk:
                            continue
                        f.write(chunk)
                        written += len(chunk)

                if expected_len_int is not None and written != expected_len_int:
                    raise RuntimeError(
                        f"download size mismatch (expected={expected_len_int}, got={written})"
                    )

            tmp_path.replace(final_path)
            return final_path

        except Exception as e:
            try:
                if tmp_path.exists():
                    tmp_path.unlink()
            except Exception:
                pass

            raise MediaProcessingError(
                stage="DOWNLOAD",
                code="DOWNLOAD_FAILED",
                message=f"Failed to download source (video_id={video_id})",
                context={
                    "video_id": video_id,
                    "input_url": input_url[:2000],
                    "output_root": str(output_root),
                    "error": repr(e),
                },
            ) from e

    def _probe_duration_seconds(
        self,
        *,
        video_id: int,
        input_path: str,
        timeout: Optional[int],
    ) -> float:
        """
        ffprobe local file.
        """
        cmd = [
            "ffprobe",
            "-v", "error",
            "-print_format", "json",
            "-show_format",
            "-show_streams",
            input_path,
        ]

        try:
            p = subprocess.run(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                timeout=timeout,
                check=False,
            )
        except Exception as e:
            raise MediaProcessingError(
                stage="PROBE",
                code="PROBE_FAILED",
                message=f"ffprobe execution failed (video_id={video_id})",
                context={
                    "video_id": video_id,
                    "cmd": cmd,
                    "error": repr(e),
                },
            ) from e

        if p.returncode != 0:
            raise MediaProcessingError(
                stage="PROBE",
                code="PROBE_FAILED",
                message=f"ffprobe returned non-zero (video_id={video_id})",
                context={
                    "video_id": video_id,
                    "cmd": cmd,
                    "returncode": p.returncode,
                    "stderr": p.stderr,
                },
            )

        try:
            data = json.loads(p.stdout)
            fmt = data.get("format") or {}
            duration_str = fmt.get("duration")
            duration = float(duration_str) if duration_str is not None else 0.0
        except Exception as e:
            raise MediaProcessingError(
                stage="PROBE",
                code="PROBE_FAILED",
                message=f"Failed to parse ffprobe output (video_id={video_id})",
                context={
                    "video_id": video_id,
                    "cmd": cmd,
                    "stdout": p.stdout[:4000],
                    "error": repr(e),
                },
            ) from e

        if duration <= 0:
            raise MediaProcessingError(
                stage="PROBE",
                code="PROBE_FAILED",
                message=f"Invalid duration from ffprobe (video_id={video_id}, duration={duration})",
                context={
                    "video_id": video_id,
                    "cmd": cmd,
                    "duration": duration,
                },
            )

        return duration

    def _generate_thumbnail(
        self,
        *,
        video_id: int,
        input_path: str,
        output_path: Path,
        timeout: Optional[int],
    ) -> None:
        """
        Generate one thumbnail jpg from local file.
        """
        output_path.parent.mkdir(parents=True, exist_ok=True)

        cmd = [
            "ffmpeg",
            "-y",
            "-i", input_path,
            "-vf", "thumbnail,scale=1280:-2",
            "-frames:v", "1",
            str(output_path),
        ]

        try:
            p = subprocess.run(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                timeout=timeout,
                check=False,
            )
        except Exception as e:
            raise MediaProcessingError(
                stage="THUMBNAIL",
                code="THUMBNAIL_FAILED",
                message=f"ffmpeg thumbnail execution failed (video_id={video_id})",
                context={
                    "video_id": video_id,
                    "cmd": cmd,
                    "error": repr(e),
                },
            ) from e

        if p.returncode != 0:
            raise MediaProcessingError(
                stage="THUMBNAIL",
                code="THUMBNAIL_FAILED",
                message=f"ffmpeg thumbnail returned non-zero (video_id={video_id})",
                context={
                    "video_id": video_id,
                    "cmd": cmd,
                    "returncode": p.returncode,
                    "stderr": p.stderr,
                },
            )

        if not output_path.exists():
            raise MediaProcessingError(
                stage="THUMBNAIL",
                code="THUMBNAIL_FAILED",
                message=f"thumbnail file not created (video_id={video_id})",
                context={
                    "video_id": video_id,
                    "output_path": str(output_path),
                },
            )

    def _transcode_hls(
        self,
        *,
        video_id: int,
        input_path: str,
        output_root: Path, 
        timeout: Optional[int],
    ) -> Path:
        """
        Delegate to transcoder (local input).
        """
        try:
            master_path = transcode_to_hls(
                video_id=video_id,
                input_path=input_path,
                output_root=output_root, 
                timeout=timeout,
            )
            return master_path
        except Exception as e:
            raise MediaProcessingError(
                stage="HLS",
                code="HLS_FAILED",
                message=f"HLS transcode failed (video_id={video_id})",
                context={
                    "video_id": video_id,
                    "input_path": input_path,
                    "error": repr(e),
                },
            ) from e

    def _verify_ready(self, *, video_id: int, master_path: Path) -> None:
        """
        READY criterion is strictly: master.m3u8 exists.
        """
        if not master_path.exists():
            raise MediaProcessingError(
                stage="VERIFY",
                code="OUTPUT_INVALID",
                message=f"READY criterion failed: master.m3u8 missing (video_id={video_id})",
                context={
                    "video_id": video_id,
                    "master_path": str(master_path),
                },
            )


# ---------------------------------------------------------------------
# Public entry
# ---------------------------------------------------------------------

def run(
    *,
    video_id: int,
    input_url: str,
    output_root: Union[str, Path],
    timeout_download: Optional[int] = 60 * 30,
    timeout_probe: Optional[int] = 60,
    timeout_thumbnail: Optional[int] = 120,
    timeout_hls: Optional[int] = None,
) -> ProcessResult:
    return VideoProcessor().run(
        video_id=video_id,
        input_url=input_url,
        output_root=output_root,
        timeout_download=timeout_download,
        timeout_probe=timeout_probe,
        timeout_thumbnail=timeout_thumbnail,
        timeout_hls=timeout_hls,
    )


==========================================================================================
# FILE: video_worker/video/transcoder.py
==========================================================================================
# PATH: apps/worker/video_worker/video/transcoder.py

from __future__ import annotations

import json
import subprocess
from pathlib import Path
from typing import List


# ---------------------------------------------------------------------
# HLS Variant Ladder
# ---------------------------------------------------------------------
# ‚ö†Ô∏è nameÏóêÎäî Ï†àÎåÄ 'v' Î∂ôÏù¥ÏßÄ Îßê Í≤É
HLS_VARIANTS = [
    {"name": "1", "width": 426, "height": 240, "video_bitrate": "400k", "audio_bitrate": "64k"},
    {"name": "2", "width": 640, "height": 360, "video_bitrate": "800k", "audio_bitrate": "96k"},
    {"name": "3", "width": 1280, "height": 720, "video_bitrate": "2500k", "audio_bitrate": "128k"},
]


# ---------------------------------------------------------------------
# Directory preparation
# ---------------------------------------------------------------------
def prepare_output_dirs(output_root: Path) -> None:
    """
    storage/media/hls/videos/{video_id}/
      ‚îú‚îÄ master.m3u8
      ‚îú‚îÄ v1/
      ‚îÇ   ‚îú‚îÄ index.m3u8
      ‚îÇ   ‚îî‚îÄ index0.ts ...
      ‚îú‚îÄ v2/
      ‚îî‚îÄ v3/
    """
    output_root.mkdir(parents=True, exist_ok=True)

    # v1, v2, v3 ÎîîÎ†âÌÜ†Î¶¨ ÎØ∏Î¶¨ ÏÉùÏÑ±
    for v in HLS_VARIANTS:
        (output_root / f"v{v['name']}").mkdir(parents=True, exist_ok=True)


# ---------------------------------------------------------------------
# ffprobe: audio stream Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
# ---------------------------------------------------------------------
def has_audio_stream(input_path: str) -> bool:
    """
    ÏûÖÎ†• mp4Ïóê audio streamÏù¥ Ï°¥Ïû¨ÌïòÎäîÏßÄ ÌôïÏù∏ÌïúÎã§.
    - Ïò§ÎîîÏò§Í∞Ä ÏóÜÏúºÎ©¥ var_stream_mapÏóêÏÑú a Ìä∏ÎûôÏùÑ ÏöîÍµ¨ÌïòÎ©¥ ffmpegÍ∞Ä ÌÑ∞Ïßê
    """
    cmd = [
        "ffprobe",
        "-v",
        "error",
        "-print_format",
        "json",
        "-show_streams",
        input_path,
    ]

    p = subprocess.run(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        check=False,
    )

    if p.returncode != 0:
        return False

    try:
        data = json.loads(p.stdout)
        streams = data.get("streams") or []
        return any(s.get("codec_type") == "audio" for s in streams)
    except Exception:
        return False


# ---------------------------------------------------------------------
# ffmpeg filter_complex builder
# ---------------------------------------------------------------------
def build_filter_complex() -> str:
    parts: List[str] = []
    split_count = len(HLS_VARIANTS)

    parts.append("[0:v]split={}".format(split_count) + "".join(f"[v{i}]" for i in range(split_count)))

    for i, v in enumerate(HLS_VARIANTS):
        parts.append(f"[v{i}]scale={v['width']}:{v['height']}[v{i}out]")

    return ";".join(parts)


# ---------------------------------------------------------------------
# ffmpeg command builder (‚≠ê ÌëúÏ§ÄÌòï)
# ---------------------------------------------------------------------
def build_ffmpeg_command(input_path: str, with_audio: bool) -> List[str]:
    """
    ‚ö†Ô∏è Î™®Îì† Ï∂úÎ†• Í≤ΩÎ°úÎäî 'ÏÉÅÎåÄ Í≤ΩÎ°ú'Îßå ÏÇ¨Ïö©
    ‚ö†Ô∏è cwd Í∏∞Ï§ÄÏúºÎ°ú ffmpeg Ïã§ÌñâÎê®

    with_audio:
      - True  -> video+audio variant HLS
      - False -> video-only variant HLS (Ïò§ÎîîÏò§ ÏóÜÎäî ÏûÖÎ†• ÎåÄÏùë)
    """
    cmd: List[str] = [
        "ffmpeg",
        "-y",
        "-i",
        input_path,  # ÏûÖÎ†•ÏùÄ Ï†àÎåÄ Í≤ΩÎ°úÏó¨ÎèÑ OK
        "-filter_complex",
        build_filter_complex(),
    ]

    for i, v in enumerate(HLS_VARIANTS):
        # video mapÏùÄ Ìï≠ÏÉÅ Ï°¥Ïû¨
        cmd += [
            "-map",
            f"[v{i}out]",
        ]

        # ‚úÖ Ïò§ÎîîÏò§Í∞Ä ÏûàÏùÑ ÎïåÎßå audio map
        if with_audio:
            cmd += ["-map", "0:a?"]

        # video encoder
        cmd += [
            f"-c:v:{i}",
            "libx264",
            "-profile:v",
            "main",
            "-pix_fmt",
            "yuv420p",
            f"-b:v:{i}",
            v["video_bitrate"],
            "-g",
            "48",
            "-keyint_min",
            "48",
            "-sc_threshold",
            "0",
        ]

        # ‚úÖ Ïò§ÎîîÏò§ ÏûàÏùÑ ÎïåÎßå aac Ïù∏ÏΩîÎî© ÏòµÏÖò
        if with_audio:
            cmd += [
                f"-c:a:{i}",
                "aac",
                "-ac",
                "2",
                f"-b:a:{i}",
                v["audio_bitrate"],
            ]

    # ‚úÖ var_stream_mapÎèÑ Ïò§ÎîîÏò§ Ïú†Î¨¥Ïóê Îî∞Îùº Î∂ÑÍ∏∞
    if with_audio:
        var_map = " ".join(f"v:{i},a:{i},name:{v['name']}" for i, v in enumerate(HLS_VARIANTS))
    else:
        var_map = " ".join(f"v:{i},name:{v['name']}" for i, v in enumerate(HLS_VARIANTS))

    cmd += [
        "-f",
        "hls",
        "-hls_time",
        "4",
        "-hls_playlist_type",
        "vod",
        "-hls_flags",
        "independent_segments",
        # ‚úÖ ÌëúÏ§Ä: ÏÉÅÎåÄ Í≤ΩÎ°ú + POSIX Ïä¨ÎûòÏãú
        "-hls_segment_filename",
        "v%v/index%d.ts",
        "-master_pl_name",
        "master.m3u8",
        "-var_stream_map",
        var_map,
        # ‚úÖ variant playlistÎèÑ ÏÉÅÎåÄ Í≤ΩÎ°ú
        "v%v/index.m3u8",
    ]

    return cmd


# ---------------------------------------------------------------------
# Public API (‚≠ê ÌëúÏ§ÄÌòï Ïã§ÌñâÎ∂Ä)
# ---------------------------------------------------------------------
def transcode_to_hls(
    *,
    video_id: int,
    input_path: str,
    output_root: Path,
    timeout: int | None = None,
) -> Path:
    """
    Execute HLS transcoding from local mp4 (Best Practice)

    ‚úÖ Ïò§ÎîîÏò§ ÏóÜÎäî ÏûÖÎ†•ÎèÑ Ï≤òÎ¶¨ Í∞ÄÎä•:
    - ffprobeÎ°ú audio stream Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
    - ÏóÜÏúºÎ©¥ video-only HLSÎ°ú ÏÉùÏÑ±
    """
    # 1. Ï∂úÎ†• ÎîîÎ†âÌÜ†Î¶¨ Ï§ÄÎπÑ
    prepare_output_dirs(output_root)

    # 2. Ïò§ÎîîÏò§ Ïä§Ìä∏Î¶º Ï°¥Ïû¨ ÌôïÏù∏
    with_audio = has_audio_stream(input_path)

    # 3. ffmpeg Î™ÖÎ†πÏñ¥ ÏÉùÏÑ±
    cmd = build_ffmpeg_command(input_path, with_audio=with_audio)

    # 4. ‚≠ê ÌïµÏã¨: output_rootÎ•º cwdÎ°ú Ïã§Ìñâ
    process = subprocess.run(
        cmd,
        cwd=str(output_root.resolve()),
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        timeout=timeout,
        check=False,
    )

    if process.returncode != 0:
        raise RuntimeError(
            {
                "video_id": video_id,
                "with_audio": with_audio,
                "cmd": cmd,
                "stderr": process.stderr,
            }
        )

    master_path = output_root / "master.m3u8"
    if not master_path.exists():
        raise RuntimeError("master.m3u8 not created")

    return master_path
