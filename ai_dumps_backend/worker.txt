====================================================================================================
# BACKEND APP: worker
# ROOT PATH: C:\academy\apps\worker
====================================================================================================


==========================================================================================
# FILE: __init__.py
==========================================================================================
#apps/worker/__init__.py

#아무내용없음.


==========================================================================================
# FILE: ai_worker/__init__.py
==========================================================================================



==========================================================================================
# FILE: ai_worker/run.py
==========================================================================================
# PATH: apps/worker/ai_worker/run.py
from __future__ import annotations

import os
import sys
import logging
import requests
import time

from apps.shared.contracts.ai_job import AIJob
from apps.shared.contracts.ai_result import AIResult
from apps.worker.ai_worker.ai.pipelines.dispatcher import handle_ai_job

# ==============================================================================
# AI WORKER – SINGLE RUN MODE (PRODUCTION FINAL)
#
# DESIGN PRINCIPLES (ENTERPRISE STANDARD):
# - Worker is NOT a daemon
# - Bounded polling within fixed lifetime window
# - No infinite loop
# - One execution = at most one job
# - Exit immediately after job 처리 or idle window expiration
#
# NOTE (OPS):
# - Process lifetime is capped to match EC2 billing granularity (default 60s)
# - Polling is allowed ONLY within this window
# ==============================================================================

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] [AI-WORKER] %(message)s",
)
logger = logging.getLogger(__name__)

API_BASE_URL = os.getenv("API_BASE_URL", "http://localhost:8000")
INTERNAL_WORKER_TOKEN = os.getenv("INTERNAL_WORKER_TOKEN", "long-random-secret")

# OPS: worker lifetime & polling
WORKER_MIN_LIFETIME = int(os.getenv("WORKER_MIN_LIFETIME", "60"))
POLL_INTERVAL_SECONDS = int(os.getenv("WORKER_POLL_INTERVAL", "5"))


def _headers() -> dict:
    return {
        "X-Worker-Token": INTERNAL_WORKER_TOKEN,
        "X-Worker-Id": os.getenv("HOSTNAME", "ai-worker"),
    }


def fetch_job() -> AIJob | None:
    """
    Ask API for the next AI job.
    API is the single source of truth.
    """
    url = f"{API_BASE_URL.rstrip('/')}/api/v1/internal/ai/job/next/"
    resp = requests.get(url, headers=_headers(), timeout=10)
    resp.raise_for_status()

    data = resp.json()
    job_data = data.get("job")
    if not job_data:
        return None

    return AIJob.from_dict(job_data)


def submit_result(*, result: AIResult, job: AIJob) -> None:
    """
    Submit job result back to API.
    """
    url = f"{API_BASE_URL.rstrip('/')}/api/v1/internal/ai/job/result/"
    headers = _headers()
    headers["Content-Type"] = "application/json"

    submission_id = None
    try:
        if job.source_id is not None and str(job.source_id).isdigit():
            submission_id = int(str(job.source_id))
    except Exception:
        submission_id = None

    payload = {
        "job_id": job.id,
        "submission_id": submission_id,
        "status": result.status,
        "result": result.result,
        "error": result.error,
    }

    resp = requests.post(url, json=payload, headers=headers, timeout=20)
    resp.raise_for_status()


# ------------------------------------------------------------------------------
# EC2 SELF-STOP (MINIMAL, SAFE)
# ------------------------------------------------------------------------------
def _stop_self_ec2() -> None:
    """
    Stop this EC2 instance itself.

    - Uses IMDSv2
    - Requires IAM role permission: ec2:StopInstances
    """
    try:
        import boto3

        token = requests.put(
            "http://169.254.169.254/latest/api/token",
            headers={"X-aws-ec2-metadata-token-ttl-seconds": "21600"},
            timeout=2,
        ).text

        headers = {"X-aws-ec2-metadata-token": token}

        instance_id = requests.get(
            "http://169.254.169.254/latest/meta-data/instance-id",
            headers=headers,
            timeout=2,
        ).text

        region = requests.get(
            "http://169.254.169.254/latest/meta-data/placement/region",
            headers=headers,
            timeout=2,
        ).text

        ec2 = boto3.client("ec2", region_name=region)
        ec2.stop_instances(InstanceIds=[instance_id])

        logger.info("EC2 self-stop requested (instance_id=%s)", instance_id)

    except Exception as e:
        logger.exception("EC2 self-stop failed (ignored): %s", e)


def main() -> int:
    """
    Single-run worker entrypoint with bounded polling.

    Flow:
    - Poll for job within WORKER_MIN_LIFETIME window
    - If job found → process once → exit
    - If no job until timeout → exit
    """
    start_ts = time.monotonic()
    deadline = start_ts + WORKER_MIN_LIFETIME

    logger.info(
        "AI Worker started (API_BASE_URL=%s, lifetime=%ss)",
        API_BASE_URL,
        WORKER_MIN_LIFETIME,
    )

    try:
        while True:
            now = time.monotonic()
            if now >= deadline:
                logger.info("Idle window expired (%ss). exiting.", WORKER_MIN_LIFETIME)
                return 0

            try:
                job = fetch_job()
            except Exception:
                logger.exception("fetch_job failed")
                return 1

            if job is None:
                sleep_sec = min(POLL_INTERVAL_SECONDS, max(0.0, deadline - now))
                if sleep_sec > 0:
                    time.sleep(sleep_sec)
                continue

            logger.info("Job received: id=%s type=%s", job.id, job.type)

            try:
                result = handle_ai_job(job)
                submit_result(result=result, job=job)
                logger.info("Job finished: id=%s status=%s", job.id, result.status)
                return 0
            except Exception:
                logger.exception("Job processing failed")
                return 1

    finally:
        elapsed = time.monotonic() - start_ts
        remain = WORKER_MIN_LIFETIME - elapsed

        if remain > 0:
            logger.info("Graceful sleep before shutdown: %.1fs", remain)
            time.sleep(remain)

        logger.info("AI Worker shutdown complete")
        _stop_self_ec2()


if __name__ == "__main__":
    sys.exit(main())


==========================================================================================
# FILE: ai_worker/sqs_main_cpu.py
==========================================================================================
"""
AI Worker CPU — Academy 전용 엔트리 (Lite + Basic)

Legacy 제거: academy 전용 엔트리.
academy.framework.workers.ai_sqs_worker 만 사용.
"""
from __future__ import annotations

import os
import sys

if os.environ.get("DJANGO_SETTINGS_MODULE"):
    import django
    django.setup()

if __name__ == "__main__":
    from academy.framework.workers.ai_sqs_worker import run_ai_sqs_worker
    sys.exit(run_ai_sqs_worker())


==========================================================================================
# FILE: ai_worker/sqs_main_gpu.py
==========================================================================================
"""
AI Worker GPU — Academy 전용 엔트리 (Premium)

Legacy 제거: legacy main() 제거.
academy.framework.workers.ai_sqs_worker 만 사용 (AI_WORKER_PREMIUM_ONLY=1).
"""
from __future__ import annotations

import os
import sys

if os.environ.get("DJANGO_SETTINGS_MODULE"):
    import django
    django.setup()

if __name__ == "__main__":
    os.environ["AI_WORKER_PREMIUM_ONLY"] = "1"
    from academy.framework.workers.ai_sqs_worker import run_ai_sqs_worker
    sys.exit(run_ai_sqs_worker())


==========================================================================================
# FILE: ai_worker/ai/__init__.py
==========================================================================================
# apps/worker/ai/__init__.py
# Celery 제거됨. run.py 단일 진입


==========================================================================================
# FILE: ai_worker/ai/config.py
==========================================================================================
# apps/worker/ai/config.py
from __future__ import annotations

import os
from dataclasses import dataclass
from typing import List, Optional


def _env(name: str, default: Optional[str] = None) -> Optional[str]:
    v = os.getenv(name)
    return v if v not in (None, "") else default


@dataclass(frozen=True)
class AIConfig:
    # OCR
    OCR_ENGINE: str = "google"  # google | tesseract | auto
    GOOGLE_APPLICATION_CREDENTIALS: Optional[str] = None  # optional (google sdk default)

    # Segmentation
    QUESTION_SEGMENTATION_ENGINE: str = "auto"  # yolo|opencv|template|auto

    # YOLO (optional)
    YOLO_QUESTION_MODEL_PATH: Optional[str] = None
    YOLO_QUESTION_INPUT_SIZE: int = 640
    YOLO_QUESTION_CONF_THRESHOLD: float = 0.4
    YOLO_QUESTION_IOU_THRESHOLD: float = 0.5

    # Embedding
    EMBEDDING_BACKEND: str = "auto"  # local|openai|auto
    EMBEDDING_LOCAL_MODEL: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    EMBEDDING_OPENAI_MODEL: str = "text-embedding-3-small"
    OPENAI_API_KEY: Optional[str] = None

    # Problem generation
    PROBLEM_GEN_MODEL: str = "gpt-4.1-mini"  # default

    @staticmethod
    def load() -> "AIConfig":
        return AIConfig(
            OCR_ENGINE=_env("OCR_ENGINE", "google") or "google",
            GOOGLE_APPLICATION_CREDENTIALS=_env("GOOGLE_APPLICATION_CREDENTIALS"),

            QUESTION_SEGMENTATION_ENGINE=_env("QUESTION_SEGMENTATION_ENGINE", "auto") or "auto",

            YOLO_QUESTION_MODEL_PATH=_env("YOLO_QUESTION_MODEL_PATH"),
            YOLO_QUESTION_INPUT_SIZE=int(_env("YOLO_QUESTION_INPUT_SIZE", "640") or "640"),
            YOLO_QUESTION_CONF_THRESHOLD=float(_env("YOLO_QUESTION_CONF_THRESHOLD", "0.4") or "0.4"),
            YOLO_QUESTION_IOU_THRESHOLD=float(_env("YOLO_QUESTION_IOU_THRESHOLD", "0.5") or "0.5"),

            EMBEDDING_BACKEND=_env("EMBEDDING_BACKEND", "auto") or "auto",
            EMBEDDING_LOCAL_MODEL=_env(
                "EMBEDDING_LOCAL_MODEL",
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            ) or "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            EMBEDDING_OPENAI_MODEL=_env("EMBEDDING_OPENAI_MODEL", "text-embedding-3-small") or "text-embedding-3-small",
            OPENAI_API_KEY=_env("OPENAI_API_KEY") or _env("EMBEDDING_OPENAI_API_KEY"),

            PROBLEM_GEN_MODEL=_env("PROBLEM_GEN_MODEL", "gpt-4.1-mini") or "gpt-4.1-mini",
        )


==========================================================================================
# FILE: ai_worker/ai/detection/__init__.py
==========================================================================================
# apps/worker/ai/detection/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/detection/segment_dispatcher.py
==========================================================================================
# apps/worker/ai/detection/segment_dispatcher.py
from __future__ import annotations

from typing import List, Tuple

from apps.worker.ai_worker.ai.config import AIConfig
from apps.worker.ai_worker.ai.detection.segment_opencv import segment_questions_opencv
from apps.worker.ai_worker.ai.detection.segment_yolo import segment_questions_yolo

BBox = Tuple[int, int, int, int]


def segment_questions(image_path: str) -> List[BBox]:
    """
    worker-side segmentation single entrypoint
    """
    cfg = AIConfig.load()
    engine = (cfg.QUESTION_SEGMENTATION_ENGINE or "auto").lower()

    if engine == "opencv":
        return segment_questions_opencv(image_path)
    if engine == "yolo":
        return segment_questions_yolo(image_path)

    # auto: yolo -> opencv
    try:
        boxes = segment_questions_yolo(image_path)
        if boxes:
            return boxes
    except Exception:
        pass
    return segment_questions_opencv(image_path)


==========================================================================================
# FILE: ai_worker/ai/detection/segment_opencv.py
==========================================================================================
# apps/worker/ai/detection/segment_opencv.py
from __future__ import annotations

from typing import List, Tuple
import cv2  # type: ignore

BBox = Tuple[int, int, int, int]


def segment_questions_opencv(image_path: str) -> List[BBox]:
    """
    legacy 섞여있던 opencv segmentation 정리본
    입력: image_path
    출력: [(x,y,w,h), ...]
    """
    image_bgr = cv2.imread(image_path)
    if image_bgr is None:
        return []

    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5, 5), 0)

    _, thresh = cv2.threshold(
        blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU
    )

    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))
    dilated = cv2.dilate(thresh, kernel, iterations=1)

    contours, _ = cv2.findContours(
        dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
    )

    h_img, w_img = gray.shape[:2]
    min_area = w_img * h_img * 0.005
    max_area = w_img * h_img * 0.9

    boxes: List[BBox] = []
    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)
        area = w * h
        if area < min_area or area > max_area:
            continue

        aspect = h / (w + 1e-6)
        if aspect < 0.3:
            continue

        boxes.append((x, y, w, h))

    boxes.sort(key=lambda b: (b[1], b[0]))
    return boxes


==========================================================================================
# FILE: ai_worker/ai/detection/segment_yolo.py
==========================================================================================
# apps/worker/ai/detection/segment_yolo.py
from __future__ import annotations

from functools import lru_cache
from typing import List, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore

from apps.worker.ai_worker.ai.config import AIConfig

BBox = Tuple[int, int, int, int]


class YoloNotConfiguredError(RuntimeError):
    pass


try:
    import onnxruntime as ort  # type: ignore
    _HAS_ORT = True
except Exception:
    _HAS_ORT = False


@lru_cache()
def _get_session():
    cfg = AIConfig.load()

    if not _HAS_ORT:
        raise YoloNotConfiguredError("onnxruntime not installed")

    if not cfg.YOLO_QUESTION_MODEL_PATH:
        raise YoloNotConfiguredError("YOLO_QUESTION_MODEL_PATH not set")

    providers = ["CPUExecutionProvider"]
    return ort.InferenceSession(str(cfg.YOLO_QUESTION_MODEL_PATH), providers=providers)


def _preprocess(image_bgr, input_size: int):
    h0, w0 = image_bgr.shape[:2]
    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
    resized = cv2.resize(image_rgb, (input_size, input_size))
    resized = resized.astype(np.float32) / 255.0

    tensor = np.transpose(resized, (2, 0, 1))
    tensor = np.expand_dims(tensor, axis=0)

    scale_x = w0 / float(input_size)
    scale_y = h0 / float(input_size)
    return tensor, scale_x, scale_y


def _nms(boxes, scores, iou_threshold: float):
    if len(boxes) == 0:
        return []

    boxes = boxes.astype(np.float32)
    scores = scores.astype(np.float32)

    x1, y1, x2, y2 = boxes.T
    areas = (x2 - x1) * (y2 - y1)
    order = scores.argsort()[::-1]

    keep = []
    while order.size > 0:
        i = int(order[0])
        keep.append(i)

        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])

        w = np.maximum(0.0, xx2 - xx1)
        h = np.maximum(0.0, yy2 - yy1)
        inter = w * h

        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-6)
        idxs = np.where(iou <= iou_threshold)[0]
        order = order[idxs + 1]

    return keep


def segment_questions_yolo(image_path: str) -> List[BBox]:
    cfg = AIConfig.load()
    sess = _get_session()

    image_bgr = cv2.imread(image_path)
    if image_bgr is None:
        return []

    input_tensor, scale_x, scale_y = _preprocess(image_bgr, cfg.YOLO_QUESTION_INPUT_SIZE)

    input_name = sess.get_inputs()[0].name
    outputs = sess.run(None, {input_name: input_tensor})
    preds = outputs[0]
    if preds.ndim == 3:
        preds = preds[0]

    boxes = []
    scores = []

    for det in preds:
        cx, cy, w, h, obj_conf = det[:5]
        cls_scores = det[5:]
        cls_conf = float(cls_scores.max()) if cls_scores.size > 0 else 1.0

        score = float(obj_conf * cls_conf)
        if score < cfg.YOLO_QUESTION_CONF_THRESHOLD:
            continue

        x1 = (cx - w / 2.0) * scale_x
        y1 = (cy - h / 2.0) * scale_y
        x2 = (cx + w / 2.0) * scale_x
        y2 = (cy + h / 2.0) * scale_y

        boxes.append([x1, y1, x2, y2])
        scores.append(score)

    if not boxes:
        return []

    boxes_np = np.array(boxes)
    scores_np = np.array(scores)

    keep_idx = _nms(boxes_np, scores_np, cfg.YOLO_QUESTION_IOU_THRESHOLD)

    final: List[BBox] = []
    for i in keep_idx:
        x1, y1, x2, y2 = boxes_np[i]
        final.append((int(x1), int(y1), int(x2 - x1), int(y2 - y1)))

    final.sort(key=lambda b: (b[1], b[0]))
    return final


==========================================================================================
# FILE: ai_worker/ai/embedding/__init__.py
==========================================================================================
# apps/worker/ai/embedding/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/embedding/service.py
==========================================================================================
# apps/worker/ai/embedding/service.py
from __future__ import annotations

from dataclasses import dataclass
from typing import List, Optional, Sequence, Literal
import math

from apps.worker.ai_worker.ai.config import AIConfig

EmbeddingBackendName = Literal["local", "openai"]


@dataclass
class EmbeddingBatch:
    vectors: List[List[float]]
    backend: EmbeddingBackendName


def cosine_similarity(a: Sequence[float], b: Sequence[float]) -> float:
    """
    cosine similarity normalized to 0~1
    """
    if not a or not b:
        return 0.0
    if len(a) != len(b):
        return 0.0

    dot = 0.0
    na = 0.0
    nb = 0.0
    for x, y in zip(a, b):
        dot += x * y
        na += x * x
        nb += y * y

    if na <= 0.0 or nb <= 0.0:
        return 0.0

    sim = dot / (math.sqrt(na) * math.sqrt(nb))
    sim = max(-1.0, min(1.0, sim))
    return (sim + 1.0) / 2.0


# -------- local (sentence-transformers) --------
try:
    from sentence_transformers import SentenceTransformer  # type: ignore
except Exception:
    SentenceTransformer = None  # type: ignore

_local_model: Optional["SentenceTransformer"] = None


def _get_local_model() -> "SentenceTransformer":
    global _local_model
    if _local_model is not None:
        return _local_model

    if SentenceTransformer is None:
        raise RuntimeError("sentence-transformers not installed")

    cfg = AIConfig.load()
    _local_model = SentenceTransformer(cfg.EMBEDDING_LOCAL_MODEL)
    return _local_model


def _embed_local(texts: List[str]) -> EmbeddingBatch:
    model = _get_local_model()
    vectors = model.encode(texts, convert_to_numpy=False)
    vectors_list = [list(map(float, v)) for v in vectors]
    return EmbeddingBatch(vectors=vectors_list, backend="local")


# -------- openai --------
try:
    from openai import OpenAI  # type: ignore
except Exception:
    OpenAI = None  # type: ignore

_openai_client: Optional["OpenAI"] = None


def _get_openai_client() -> "OpenAI":
    global _openai_client
    if _openai_client is not None:
        return _openai_client

    if OpenAI is None:
        raise RuntimeError("openai not installed")

    cfg = AIConfig.load()
    if not cfg.OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY not set")

    _openai_client = OpenAI(api_key=cfg.OPENAI_API_KEY)
    return _openai_client


def _embed_openai(texts: List[str]) -> EmbeddingBatch:
    cfg = AIConfig.load()
    client = _get_openai_client()
    response = client.embeddings.create(model=cfg.EMBEDDING_OPENAI_MODEL, input=texts)
    vectors = [list(map(float, d.embedding)) for d in response.data]
    return EmbeddingBatch(vectors=vectors, backend="openai")


def _choose_backend() -> EmbeddingBackendName:
    cfg = AIConfig.load()
    mode = (cfg.EMBEDDING_BACKEND or "auto").lower()

    if mode == "local":
        return "local"
    if mode == "openai":
        return "openai"

    # auto: local 가능하면 local
    if SentenceTransformer is not None:
        try:
            _get_local_model()
            return "local"
        except Exception:
            pass
    return "openai"


def get_embeddings(texts: List[str]) -> EmbeddingBatch:
    if not texts:
        return EmbeddingBatch(vectors=[], backend=_choose_backend())

    backend = _choose_backend()
    norm = [(t or "").strip() for t in texts]

    if backend == "local":
        return _embed_local(norm)
    return _embed_openai(norm)


==========================================================================================
# FILE: ai_worker/ai/handwriting/__init__.py
==========================================================================================
# apps/worker/ai/handwriting/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/handwriting/detector.py
==========================================================================================
# apps/worker/ai/handwriting/detector.py
from __future__ import annotations

from typing import Dict
import cv2  # type: ignore
import numpy as np  # type: ignore


def analyze_handwriting(image_path: str) -> Dict[str, float]:
    """
    legacy(doc_ai/handwriting/handwriting_detector.py) 이식본
    - 한 이미지에서 필기 흔적/계산식 형태 흔적 여부 점수 반환

    return:
      {
        "writing_score": 0~1,
        "calculation_score": 0~1
      }
    """
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        return {"writing_score": 0.0, "calculation_score": 0.0}

    blur = cv2.GaussianBlur(img, (5, 5), 0)
    edges = cv2.Canny(blur, 50, 150)

    writing_density = float(np.sum(edges > 0) / edges.size)

    sobel_x = cv2.Sobel(blur, cv2.CV_64F, 1, 0, ksize=5)
    sobel_y = cv2.Sobel(blur, cv2.CV_64F, 0, 1, ksize=5)
    grad_mag = (np.mean(np.abs(sobel_x)) + np.mean(np.abs(sobel_y))) / 255.0

    writing_score = min(max(writing_density * 12.0, 0.0), 1.0)
    calculation_score = min(max(grad_mag * 3.0, 0.0), 1.0)

    return {
        "writing_score": float(writing_score),
        "calculation_score": float(calculation_score),
    }


==========================================================================================
# FILE: ai_worker/ai/homework/__init__.py
==========================================================================================



==========================================================================================
# FILE: ai_worker/ai/ocr/__init__.py
==========================================================================================
# apps/worker/ai/ocr/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/ocr/engine.py
==========================================================================================
from __future__ import annotations
from typing import List
from .schemas import OCRResultPayload, OMRDetectedAnswer


def run_ocr_engine(
    *,
    image_path: str,
) -> OCRResultPayload:
    """
    실제 OCR/OMR 엔진 자리
    - 지금은 더미
    - 나중에 OpenCV / Tesseract / 외부 API 교체
    """

    answers: List[OMRDetectedAnswer] = [
        {
            "question_number": 1,
            "detected": ["B"],
            "confidence": 0.92,
            "marking": "single",
            "status": "ok",
        },
        {
            "question_number": 2,
            "detected": ["D"],
            "confidence": 0.88,
            "marking": "single",
            "status": "ok",
        },
    ]

    return {
        "version": "v1",
        "answers": answers,
        "raw_text": None,
    }


==========================================================================================
# FILE: ai_worker/ai/ocr/google.py
==========================================================================================
# apps/worker/ai/ocr/google.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Optional

# google cloud vision
from google.cloud import vision  # type: ignore


@dataclass
class OCRResult:
    text: str
    confidence: Optional[float] = None
    raw: Optional[Any] = None


def google_ocr(image_path: str) -> OCRResult:
    """
    Worker에서 실행되는 Google OCR
    - service account는 GOOGLE_APPLICATION_CREDENTIALS 또는 기본 환경에 따름
    """
    client = vision.ImageAnnotatorClient()

    with open(image_path, "rb") as f:
        content = f.read()

    image = vision.Image(content=content)
    response = client.text_detection(image=image)

    if getattr(response, "error", None) and response.error.message:
        return OCRResult(text="", confidence=None, raw={"error": response.error.message})

    annotations = getattr(response, "text_annotations", None) or []
    if not annotations:
        return OCRResult(text="", confidence=None, raw=None)

    return OCRResult(
        text=annotations[0].description or "",
        confidence=None,
        raw=None,  # raw를 통째로 넘기면 직렬화 이슈가 생길 수 있어 기본 None
    )


==========================================================================================
# FILE: ai_worker/ai/ocr/schemas.py
==========================================================================================
from __future__ import annotations
from typing import TypedDict, List, Optional


class OMRDetectedAnswer(TypedDict):
    question_number: int
    detected: List[str]
    confidence: float
    marking: str     # single / multi / blank
    status: str      # ok / error


class OCRResultPayload(TypedDict):
    version: str
    answers: List[OMRDetectedAnswer]
    raw_text: Optional[str]


==========================================================================================
# FILE: ai_worker/ai/ocr/tesseract.py
==========================================================================================
# apps/worker/ai/ocr/tesseract.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Optional

from PIL import Image  # type: ignore
import pytesseract  # type: ignore


@dataclass
class OCRResult:
    text: str
    confidence: Optional[float] = None
    raw: Optional[Any] = None


def tesseract_ocr(image_path: str) -> OCRResult:
    img = Image.open(image_path)

    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)
    text = "\n".join(data.get("text", [])).strip()

    confs = [c for c in data.get("conf", []) if c != -1]
    confidence = (sum(confs) / len(confs)) if confs else None

    # raw=data 는 너무 클 수 있어 필요 시만 켜기
    return OCRResult(text=text, confidence=confidence, raw=None)


==========================================================================================
# FILE: ai_worker/ai/omr/__init__.py
==========================================================================================



==========================================================================================
# FILE: ai_worker/ai/omr/engine.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/identifier.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore

from apps.worker.ai_worker.ai.omr.meta_px import build_page_scale_from_meta, PageScale
from apps.worker.ai_worker.ai.utils.image_resizer import resize_if_large


BBox = Tuple[int, int, int, int]


@dataclass(frozen=True)
class IdentifierConfigV1:
    """
    Identifier OMR v1 (8 digits, each digit 0~9 single mark).

    Principles:
    - ROI based fill score (same philosophy as detect_omr_answers_v1)
    - Robust to scan/photo noise by sampling a square ROI around each bubble
    - No DB, no external calls, worker-only judgement/extraction
    """
    # 주변 ROI(버블 중심 기준) 확장 계수: r * k
    roi_expand_k: float = 1.55

    # blank 판단: 해당 digit에서 최고 fill이 이 값보다 작으면 blank
    blank_threshold: float = 0.070

    # ambiguous 판단: top-2 gap이 이 값보다 작으면 ambiguous
    conf_gap_threshold: float = 0.060

    # (운영 편의) digit-level confidence clamp
    min_confidence: float = 0.0
    max_confidence: float = 1.0


def _clamp(v: int, lo: int, hi: int) -> int:
    return max(lo, min(hi, v))


def _crop(gray: np.ndarray, bbox: BBox) -> np.ndarray:
    x, y, w, h = bbox
    x = _clamp(int(x), 0, gray.shape[1] - 1)
    y = _clamp(int(y), 0, gray.shape[0] - 1)
    w = max(1, int(w))
    h = max(1, int(h))
    w = min(w, gray.shape[1] - x)
    h = min(h, gray.shape[0] - y)
    return gray[y:y + h, x:x + w]


def _fill_score(roi_gray: np.ndarray) -> float:
    """
    Same core idea as OMR v1:
    - blur
    - OTSU + INV
    - filled pixel ratio
    """
    if roi_gray.size == 0:
        return 0.0

    blur = cv2.GaussianBlur(roi_gray, (5, 5), 0)
    _, th = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    filled = float(np.sum(th > 0))
    total = float(th.size) if th.size > 0 else 1.0
    score = filled / total
    return float(max(0.0, min(1.0, score)))


def _bubble_roi_bbox_px(
    *,
    center_px: Tuple[int, int],
    r_px: int,
    cfg: IdentifierConfigV1,
    img_w: int,
    img_h: int,
) -> BBox:
    cx, cy = center_px
    side = int(round(max(2, r_px) * cfg.roi_expand_k)) * 2
    x = int(cx - side // 2)
    y = int(cy - side // 2)
    x = _clamp(x, 0, img_w - 1)
    y = _clamp(y, 0, img_h - 1)
    w = _clamp(side, 1, img_w - x)
    h = _clamp(side, 1, img_h - y)
    return (x, y, w, h)


def detect_identifier_v1(
    *,
    image_bgr: np.ndarray,
    meta: Dict[str, Any],
    cfg: Optional[IdentifierConfigV1] = None,
) -> Dict[str, Any]:
    """
    Extract identifier(8 digits) from aligned full-page image.

    meta requirements:
      meta["identifier"]["bubbles"] list with:
        - digit_index (1..8)
        - number (0..9)
        - center: {x(mm), y(mm)}
        - r(mm)

    return contract:
      {
        "identifier": "12345678" | None,
        "digits": [{"digit_index":1,"value":1,"status":"ok|blank|ambiguous","confidence":0.91,"marks":[...]}...],
        "confidence": 0.0~1.0,
        "status": "ok|ambiguous|blank|error"
      }
    """
    cfg = cfg or IdentifierConfigV1()

    if image_bgr is None or image_bgr.size == 0:
        return {"identifier": None, "digits": [], "confidence": 0.0, "status": "error"}

    h, w = image_bgr.shape[:2]
    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)

    ident = meta.get("identifier") or {}
    bubbles = list(ident.get("bubbles") or [])
    if not bubbles:
        return {"identifier": None, "digits": [], "confidence": 0.0, "status": "error"}

    scale: PageScale = build_page_scale_from_meta(meta=meta, image_size_px=(w, h))

    # group bubbles by digit_index
    by_digit: Dict[int, List[Dict[str, Any]]] = {}
    for b in bubbles:
        try:
            di = int(b.get("digit_index") or 0)
        except Exception:
            continue
        if di <= 0:
            continue
        by_digit.setdefault(di, []).append(b)

    digits_out: List[Dict[str, Any]] = []
    identifier_chars: List[str] = []
    status_rollup = "ok"
    confidences: List[float] = []

    for digit_index in sorted(by_digit.keys()):
        bs = by_digit[digit_index]

        marks: List[Dict[str, Any]] = []
        for b in bs:
            num = int(b.get("number") or 0)
            c = b.get("center") or {}
            r_mm = float(b.get("r") or 0.0)

            cx_mm = float(c.get("x") or 0.0)
            cy_mm = float(c.get("y") or 0.0)

            cx_px, cy_px = scale.mm_to_px_point(cx_mm, cy_mm)

            # radius: use average scale for robustness (mm->px)
            r_px_x = max(1, scale.mm_to_px_len_x(r_mm))
            r_px_y = max(1, scale.mm_to_px_len_y(r_mm))
            r_px = max(1, int(round((r_px_x + r_px_y) / 2.0)))

            bbox = _bubble_roi_bbox_px(
                center_px=(cx_px, cy_px),
                r_px=r_px,
                cfg=cfg,
                img_w=w,
                img_h=h,
            )
            roi = _crop(gray, bbox)
            fill = _fill_score(roi)

            marks.append(
                {
                    "number": int(num),
                    "fill": float(fill),
                    "center_px": {"x": int(cx_px), "y": int(cy_px)},
                    "roi_px": {"x": int(bbox[0]), "y": int(bbox[1]), "w": int(bbox[2]), "h": int(bbox[3])},
                }
            )

        marks_sorted = sorted(marks, key=lambda m: float(m.get("fill") or 0.0), reverse=True)
        top = marks_sorted[0] if marks_sorted else {"number": 0, "fill": 0.0}
        second = marks_sorted[1] if len(marks_sorted) > 1 else {"number": 0, "fill": 0.0}

        top_fill = float(top.get("fill") or 0.0)
        second_fill = float(second.get("fill") or 0.0)
        gap = float(top_fill - second_fill)

        if top_fill < cfg.blank_threshold:
            digits_out.append(
                {
                    "digit_index": int(digit_index),
                    "value": None,
                    "status": "blank",
                    "confidence": 0.0,
                    "marks": marks_sorted,
                }
            )
            identifier_chars.append("?")
            status_rollup = "blank" if status_rollup == "ok" else status_rollup
            continue

        if gap < cfg.conf_gap_threshold:
            digits_out.append(
                {
                    "digit_index": int(digit_index),
                    "value": int(top.get("number") or 0),
                    "status": "ambiguous",
                    "confidence": float(max(cfg.min_confidence, min(cfg.max_confidence, top_fill))),
                    "gap": float(gap),
                    "marks": marks_sorted,
                }
            )
            identifier_chars.append(str(int(top.get("number") or 0)))
            status_rollup = "ambiguous" if status_rollup in ("ok",) else status_rollup
            confidences.append(float(top_fill))
            continue

        # ok
        conf = float(max(cfg.min_confidence, min(cfg.max_confidence, top_fill)))
        digits_out.append(
            {
                "digit_index": int(digit_index),
                "value": int(top.get("number") or 0),
                "status": "ok",
                "confidence": conf,
                "gap": float(gap),
                "marks": marks_sorted,
            }
        )
        identifier_chars.append(str(int(top.get("number") or 0)))
        confidences.append(conf)

    # identifier validity: must be 8 digits all ok/ambiguous (blank이면 ? 포함)
    identifier = "".join(identifier_chars)
    if "?" in identifier:
        identifier_final: Optional[str] = None
    else:
        identifier_final = identifier

    # overall confidence: conservative (mean of digit conf where available)
    overall_conf = float(sum(confidences) / len(confidences)) if confidences else 0.0

    return {
        "identifier": identifier_final,
        "raw_identifier": identifier,  # '?' 포함 가능 (운영 디버그/리트라이용)
        "digits": digits_out,
        "confidence": float(max(0.0, min(1.0, overall_conf))),
        "status": status_rollup,
    }


==========================================================================================
# FILE: ai_worker/ai/omr/identifier.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/identifier.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore

from apps.worker.ai_worker.ai.omr.meta_px import build_page_scale_from_meta, PageScale


BBox = Tuple[int, int, int, int]


@dataclass(frozen=True)
class IdentifierConfigV1:
    """
    Identifier OMR v1 (8 digits, each digit 0~9 single mark).

    Principles:
    - ROI based fill score (same philosophy as detect_omr_answers_v1)
    - Robust to scan/photo noise by sampling a square ROI around each bubble
    - No DB, no external calls, worker-only judgement/extraction
    """
    # (OPS DEFAULT) 촬영/워프에서 중심 오차를 흡수하기 위해 소폭 확장
    # 주변 ROI(버블 중심 기준) 확장 계수: r * k
    roi_expand_k: float = 1.60

    # (OPS DEFAULT) blank 과다 방지: 실데이터에서 연필 농도 낮은 케이스 대응
    # blank 판단: 해당 digit에서 최고 fill이 이 값보다 작으면 blank
    blank_threshold: float = 0.055

    # (OPS DEFAULT) ambiguous 과다 방지: top-2 gap 기준 소폭 완화
    # ambiguous 판단: top-2 gap이 이 값보다 작으면 ambiguous
    conf_gap_threshold: float = 0.050

    # (운영 편의) digit-level confidence clamp
    min_confidence: float = 0.0
    max_confidence: float = 1.0


def _clamp(v: int, lo: int, hi: int) -> int:
    return max(lo, min(hi, v))


def _crop(gray: np.ndarray, bbox: BBox) -> np.ndarray:
    x, y, w, h = bbox
    x = _clamp(int(x), 0, gray.shape[1] - 1)
    y = _clamp(int(y), 0, gray.shape[0] - 1)
    w = max(1, int(w))
    h = max(1, int(h))
    w = min(w, gray.shape[1] - x)
    h = min(h, gray.shape[0] - y)
    return gray[y:y + h, x:x + w]


def _fill_score(roi_gray: np.ndarray) -> float:
    """
    Same core idea as OMR v1:
    - blur
    - OTSU + INV
    - filled pixel ratio
    """
    if roi_gray.size == 0:
        return 0.0

    blur = cv2.GaussianBlur(roi_gray, (5, 5), 0)
    _, th = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    filled = float(np.sum(th > 0))
    total = float(th.size) if th.size > 0 else 1.0
    score = filled / total
    return float(max(0.0, min(1.0, score)))


def _bubble_roi_bbox_px(
    *,
    center_px: Tuple[int, int],
    r_px: int,
    cfg: IdentifierConfigV1,
    img_w: int,
    img_h: int,
) -> BBox:
    cx, cy = center_px
    side = int(round(max(2, r_px) * cfg.roi_expand_k)) * 2
    x = int(cx - side // 2)
    y = int(cy - side // 2)
    x = _clamp(x, 0, img_w - 1)
    y = _clamp(y, 0, img_h - 1)
    w = _clamp(side, 1, img_w - x)
    h = _clamp(side, 1, img_h - y)
    return (x, y, w, h)


def detect_identifier_v1(
    *,
    image_bgr: np.ndarray,
    meta: Dict[str, Any],
    cfg: Optional[IdentifierConfigV1] = None,
) -> Dict[str, Any]:
    """
    Extract identifier(8 digits) from aligned full-page image.

    meta requirements:
      meta["identifier"]["bubbles"] list with:
        - digit_index (1..8)
        - number (0..9)
        - center: {x(mm), y(mm)}
        - r(mm)

    return contract:
      {
        "identifier": "12345678" | None,
        "digits": [{"digit_index":1,"value":1,"status":"ok|blank|ambiguous","confidence":0.91,"marks":[...]}...],
        "confidence": 0.0~1.0,
        "status": "ok|ambiguous|blank|error"
      }
    """
    cfg = cfg or IdentifierConfigV1()

    if image_bgr is None or image_bgr.size == 0:
        return {"identifier": None, "digits": [], "confidence": 0.0, "status": "error"}

    # 대용량 이미지 리사이징 (처리 전)
    image_bgr, _ = resize_if_large(image_bgr, max_megapixels=4.0)
    
    h, w = image_bgr.shape[:2]
    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)

    ident = meta.get("identifier") or {}
    bubbles = list(ident.get("bubbles") or [])
    if not bubbles:
        return {"identifier": None, "digits": [], "confidence": 0.0, "status": "error"}

    scale: PageScale = build_page_scale_from_meta(meta=meta, image_size_px=(w, h))

    # group bubbles by digit_index
    by_digit: Dict[int, List[Dict[str, Any]]] = {}
    for b in bubbles:
        try:
            di = int(b.get("digit_index") or 0)
        except Exception:
            continue
        if di <= 0:
            continue
        by_digit.setdefault(di, []).append(b)

    digits_out: List[Dict[str, Any]] = []
    identifier_chars: List[str] = []
    status_rollup = "ok"
    confidences: List[float] = []

    for digit_index in sorted(by_digit.keys()):
        bs = by_digit[digit_index]

        marks: List[Dict[str, Any]] = []
        for b in bs:
            num = int(b.get("number") or 0)
            c = b.get("center") or {}
            r_mm = float(b.get("r") or 0.0)

            cx_mm = float(c.get("x") or 0.0)
            cy_mm = float(c.get("y") or 0.0)

            cx_px, cy_px = scale.mm_to_px_point(cx_mm, cy_mm)

            # radius: use average scale for robustness (mm->px)
            r_px_x = max(1, scale.mm_to_px_len_x(r_mm))
            r_px_y = max(1, scale.mm_to_px_len_y(r_mm))
            r_px = max(1, int(round((r_px_x + r_px_y) / 2.0)))

            bbox = _bubble_roi_bbox_px(
                center_px=(cx_px, cy_px),
                r_px=r_px,
                cfg=cfg,
                img_w=w,
                img_h=h,
            )
            roi = _crop(gray, bbox)
            fill = _fill_score(roi)

            marks.append(
                {
                    "number": int(num),
                    "fill": float(fill),
                    "center_px": {"x": int(cx_px), "y": int(cy_px)},
                    "roi_px": {"x": int(bbox[0]), "y": int(bbox[1]), "w": int(bbox[2]), "h": int(bbox[3])},
                }
            )

        marks_sorted = sorted(marks, key=lambda m: float(m.get("fill") or 0.0), reverse=True)
        top = marks_sorted[0] if marks_sorted else {"number": 0, "fill": 0.0}
        second = marks_sorted[1] if len(marks_sorted) > 1 else {"number": 0, "fill": 0.0}

        top_fill = float(top.get("fill") or 0.0)
        second_fill = float(second.get("fill") or 0.0)
        gap = float(top_fill - second_fill)

        if top_fill < cfg.blank_threshold:
            digits_out.append(
                {
                    "digit_index": int(digit_index),
                    "value": None,
                    "status": "blank",
                    "confidence": 0.0,
                    "marks": marks_sorted,
                }
            )
            identifier_chars.append("?")
            status_rollup = "blank" if status_rollup == "ok" else status_rollup
            continue

        if gap < cfg.conf_gap_threshold:
            digits_out.append(
                {
                    "digit_index": int(digit_index),
                    "value": int(top.get("number") or 0),
                    "status": "ambiguous",
                    "confidence": float(max(cfg.min_confidence, min(cfg.max_confidence, top_fill))),
                    "gap": float(gap),
                    "marks": marks_sorted,
                }
            )
            identifier_chars.append(str(int(top.get("number") or 0)))
            status_rollup = "ambiguous" if status_rollup in ("ok",) else status_rollup
            confidences.append(float(top_fill))
            continue

        # ok
        conf = float(max(cfg.min_confidence, min(cfg.max_confidence, top_fill)))
        digits_out.append(
            {
                "digit_index": int(digit_index),
                "value": int(top.get("number") or 0),
                "status": "ok",
                "confidence": conf,
                "gap": float(gap),
                "marks": marks_sorted,
            }
        )
        identifier_chars.append(str(int(top.get("number") or 0)))
        confidences.append(conf)

    # identifier validity: must be 8 digits all ok/ambiguous (blank이면 ? 포함)
    identifier = "".join(identifier_chars)
    if "?" in identifier:
        identifier_final: Optional[str] = None
    else:
        identifier_final = identifier

    # overall confidence: conservative (mean of digit conf where available)
    overall_conf = float(sum(confidences) / len(confidences)) if confidences else 0.0

    return {
        "identifier": identifier_final,
        "raw_identifier": identifier,  # '?' 포함 가능 (운영 디버그/리트라이용)
        "digits": digits_out,
        "confidence": float(max(0.0, min(1.0, overall_conf))),
        "status": status_rollup,
    }


==========================================================================================
# FILE: ai_worker/ai/omr/meta_px.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/meta_px.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, Tuple


def _clamp(v: int, lo: int, hi: int) -> int:
    return max(lo, min(hi, v))


@dataclass(frozen=True)
class PageScale:
    """
    Convert meta(mm) -> px.
    Boundary rule (fixed):
      - meta(mm) is assets single truth
      - px conversion is worker responsibility
      - aligned/warped image must represent the full page
    """
    sx: float
    sy: float
    img_w: int
    img_h: int

    def mm_to_px_point(self, x_mm: float, y_mm: float) -> Tuple[int, int]:
        x = int(round(float(x_mm) * self.sx))
        y = int(round(float(y_mm) * self.sy))
        x = _clamp(x, 0, self.img_w - 1)
        y = _clamp(y, 0, self.img_h - 1)
        return x, y

    def mm_to_px_len_x(self, v_mm: float) -> int:
        return max(1, int(round(float(v_mm) * self.sx)))

    def mm_to_px_len_y(self, v_mm: float) -> int:
        return max(1, int(round(float(v_mm) * self.sy)))


def build_page_scale_from_meta(
    *,
    meta: Dict[str, Any],
    image_size_px: Tuple[int, int],
) -> PageScale:
    """
    Build scaler from template meta.
    meta page size is mm. image_size_px is (width, height).
    """
    img_w, img_h = int(image_size_px[0]), int(image_size_px[1])

    page = meta.get("page") or {}
    size = page.get("size") or {}
    page_w_mm = float(size.get("width") or 0.0)
    page_h_mm = float(size.get("height") or 0.0)

    if img_w <= 0 or img_h <= 0:
        raise ValueError("invalid image_size_px")
    if page_w_mm <= 0.0 or page_h_mm <= 0.0:
        raise ValueError("invalid meta page size")

    sx = img_w / page_w_mm
    sy = img_h / page_h_mm
    return PageScale(sx=float(sx), sy=float(sy), img_w=img_w, img_h=img_h)


==========================================================================================
# FILE: ai_worker/ai/omr/template_meta.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/template_meta.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Tuple, Optional

import requests


@dataclass(frozen=True)
class TemplateMeta:
    raw: Dict[str, Any]

    @property
    def units(self) -> str:
        return str(self.raw.get("units") or "mm")

    @property
    def page_size_mm(self) -> Tuple[float, float]:
        page = self.raw.get("page") or {}
        size = page.get("size") or {}
        return float(size.get("width") or 0.0), float(size.get("height") or 0.0)

    @property
    def questions(self) -> List[Dict[str, Any]]:
        return list(self.raw.get("questions") or [])


class TemplateMetaFetchError(RuntimeError):
    pass


def fetch_objective_meta(
    *,
    base_url: str,
    question_count: int,
    # auth options (choose what your deployment uses)
    auth_cookie_header: Optional[str] = None,
    bearer_token: Optional[str] = None,
    worker_token_header: Optional[str] = None,  # e.g. X-Worker-Token (if API allows)
    extra_headers: Optional[Dict[str, str]] = None,
    timeout: int = 10,
) -> TemplateMeta:
    """
    worker -> API (assets meta)

    Stability goals:
    - clear timeouts
    - explicit error messages
    - flexible auth (cookie/bearer/custom header)
    - returns structured exception for caller to gracefully fallback

    NOTE:
    - assets endpoint currently uses IsAuthenticated.
      Production typically uses cookie session OR bearer token.
      worker_token_header is optional if you later add internal auth for workers.
    """
    url = f"{base_url.rstrip('/')}/api/v1/assets/omr/objective/meta/"
    params = {"question_count": str(int(question_count))}

    headers: Dict[str, str] = {"Accept": "application/json"}
    if auth_cookie_header:
        headers["Cookie"] = auth_cookie_header
    if bearer_token:
        headers["Authorization"] = f"Bearer {bearer_token}"
    if worker_token_header:
        headers["X-Worker-Token"] = str(worker_token_header)
    if extra_headers:
        for k, v in extra_headers.items():
            if k and v:
                headers[str(k)] = str(v)

    try:
        r = requests.get(url, params=params, headers=headers, timeout=timeout)
    except Exception as e:
        raise TemplateMetaFetchError(f"meta_fetch_network_error: {e!r}") from e

    if r.status_code >= 400:
        body = (r.text or "")[:2000]
        raise TemplateMetaFetchError(
            f"meta_fetch_http_error: status={r.status_code} body={body}"
        )

    try:
        data = r.json()
    except Exception as e:
        raise TemplateMetaFetchError(f"meta_fetch_invalid_json: {e!r}") from e

    return TemplateMeta(raw=data)


==========================================================================================
# FILE: ai_worker/ai/omr/types.py
==========================================================================================
# apps/worker/ai/omr/types.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Literal


OMRStatus = Literal["ok", "blank", "ambiguous", "low_confidence", "error"]
OMRMarking = Literal["blank", "single", "multi"]


@dataclass(frozen=True)
class OMRAnswerV1:
    """
    Worker-side OMR payload v1 (question-level)
    This should be embedded into API-side SubmissionAnswer.meta["omr"] later.

    - version: "v1" fixed
    - detected: list[str] ex) ["B"] or ["B","D"]
    - marking: blank/single/multi
    - confidence: 0~1 (top mark confidence)
    - status: ok/blank/ambiguous/low_confidence/error
    - raw: debug info (optional)
    """
    version: str
    question_id: int

    detected: List[str]
    marking: OMRMarking
    confidence: float
    status: OMRStatus

    raw: Optional[Dict[str, Any]] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "version": self.version,
            "question_id": int(self.question_id),
            "detected": list(self.detected or []),
            "marking": self.marking,
            "confidence": float(self.confidence or 0.0),
            "status": self.status,
            "raw": self.raw,
        }


==========================================================================================
# FILE: ai_worker/ai/pipelines/__init__.py
==========================================================================================
# apps/worker/ai/pipelines/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/pipelines/dispatcher.py
==========================================================================================
# apps/worker/ai_worker/ai/pipelines/dispatcher.py
from __future__ import annotations

from typing import Any, Dict

from apps.shared.contracts.ai_job import AIJob
from apps.shared.contracts.ai_result import AIResult

from apps.worker.ai_worker.ai.config import AIConfig
from apps.worker.ai_worker.ai.ocr.google import google_ocr
from apps.worker.ai_worker.ai.ocr.tesseract import tesseract_ocr
from apps.worker.ai_worker.ai.detection.segment_dispatcher import segment_questions
from apps.worker.ai_worker.ai.handwriting.detector import analyze_handwriting
from apps.worker.ai_worker.ai.embedding.service import get_embeddings
from apps.worker.ai_worker.ai.problem.generator import generate_problem_from_ocr
from apps.worker.ai_worker.ai.pipelines.homework_video_analyzer import analyze_homework_video
from apps.worker.ai_worker.ai.pipelines.excel_handler import handle_excel_parsing_job
from apps.worker.ai_worker.ai.pipelines.excel_export_handler import (
    handle_attendance_excel_export,
    handle_staff_excel_export,
)
from apps.worker.ai_worker.ai.utils.image_resizer import resize_if_large
from apps.worker.ai_worker.storage.downloader import download_to_tmp


def handle_ai_job(job: AIJob) -> AIResult:
    try:
        # 작업 분기: type / task / job_type 으로 EXCEL_PARSING vs AI 작업 분리
        job_type_lower = (job.type or "").strip().lower()
        if job_type_lower == "excel_parsing":
            return handle_excel_parsing_job(job)
        if job_type_lower == "attendance_excel_export":
            return handle_attendance_excel_export(job)
        if job_type_lower == "staff_excel_export":
            return handle_staff_excel_export(job)

        cfg = AIConfig.load()
        payload: Dict[str, Any] = job.payload or {}

        download_url = payload.get("download_url")
        if not download_url:
            return AIResult.failed(job.id, "download_url missing")

        local_path = download_to_tmp(
            download_url=download_url,
            job_id=str(job.id),
        )

        # --------------------------------------------------
        # OCR
        # --------------------------------------------------
        if job.type == "ocr":
            engine = (payload.get("engine") or cfg.OCR_ENGINE or "auto").lower()

            if engine == "tesseract":
                r = tesseract_ocr(local_path)
            elif engine == "google":
                r = google_ocr(local_path)
            else:
                try:
                    r = google_ocr(local_path)
                    if not r.text.strip():
                        r = tesseract_ocr(local_path)
                except Exception:
                    r = tesseract_ocr(local_path)

            return AIResult.done(
                job.id,
                {"text": r.text, "confidence": r.confidence},
            )

        # --------------------------------------------------
        # Question segmentation
        # --------------------------------------------------
        if job.type == "question_segmentation":
            boxes = segment_questions(local_path)
            return AIResult.done(job.id, {"boxes": boxes})

        # --------------------------------------------------
        # Handwriting analysis
        # --------------------------------------------------
        if job.type == "handwriting_analysis":
            scores = analyze_handwriting(local_path)
            return AIResult.done(job.id, scores)

        # --------------------------------------------------
        # Embedding
        # --------------------------------------------------
        if job.type == "embedding":
            texts = payload.get("texts") or []
            batch = get_embeddings(list(texts))
            return AIResult.done(
                job.id,
                {"backend": batch.backend, "vectors": batch.vectors},
            )

        # --------------------------------------------------
        # Problem generation
        # --------------------------------------------------
        if job.type == "problem_generation":
            ocr_text = payload.get("ocr_text") or ""
            parsed = generate_problem_from_ocr(ocr_text)
            return AIResult.done(
                job.id,
                {
                    "body": parsed.body,
                    "choices": parsed.choices,
                    "answer": parsed.answer,
                    "difficulty": parsed.difficulty,
                    "tag": parsed.tag,
                    "summary": parsed.summary,
                    "explanation": parsed.explanation,
                },
            )

        # --------------------------------------------------
        # Homework video analysis
        # --------------------------------------------------
        if job.type == "homework_video_analysis":
            frame_stride = int(payload.get("frame_stride") or 10)
            min_frame_count = int(payload.get("min_frame_count") or 30)
            use_key_frames = payload.get("use_key_frames", True)  # 기본값: 키 프레임 사용
            max_pages = int(payload.get("max_pages") or 10)
            processing_timeout = int(payload.get("processing_timeout") or 60)
            
            analysis = analyze_homework_video(
                video_path=local_path,
                frame_stride=frame_stride,
                min_frame_count=min_frame_count,
                use_key_frames=use_key_frames,
                max_pages=max_pages,
                processing_timeout=processing_timeout,
            )
            return AIResult.done(job.id, analysis)

        # --------------------------------------------------
        # OMR grading (meta-aware) - production final
        # --------------------------------------------------
        if job.type == "omr_grading":
            """
            payload options (recommended):
              - mode: "scan" | "photo" | "auto" (default auto)
              - question_count: 10|20|30 (required if template_fetch is used)
              - template_meta: dict (inject meta directly; no API call)
              - template_fetch:
                    {
                      "base_url": "...",
                      "cookie": "...",
                      "bearer_token": "...",
                      "worker_token": "...",
                      "timeout": 10
                    }

            Output contract:
              {
                "version": "v1",
                "mode": "...",
                "aligned": true|false,
                "identifier": {...},
                "answers": [...],
                "meta_used": true|false
              }
            """
            import cv2  # type: ignore

            from apps.worker.ai_worker.ai.omr.engine import detect_omr_answers_v1, OMRConfigV1
            from apps.worker.ai_worker.ai.omr.roi_builder import build_questions_payload_from_meta
            from apps.worker.ai_worker.ai.omr.warp import warp_to_a4_landscape
            from apps.worker.ai_worker.ai.omr.template_meta import fetch_objective_meta, TemplateMetaFetchError
            from apps.worker.ai_worker.ai.omr.identifier import detect_identifier_v1, IdentifierConfigV1

            mode = str(payload.get("mode") or "auto").lower()
            if mode not in ("scan", "photo", "auto"):
                mode = "auto"

            # 1) meta 확보
            meta = payload.get("template_meta")
            meta_used = False
            meta_fetch_error = None

            if not meta:
                tf = payload.get("template_fetch") or {}
                base_url = tf.get("base_url")
                if base_url:
                    qc = int(payload.get("question_count") or 0)
                    if qc not in (10, 20, 30):
                        return AIResult.failed(job.id, "question_count required (10|20|30) for template_fetch")

                    try:
                        meta_obj = fetch_objective_meta(
                            base_url=str(base_url),
                            question_count=qc,
                            auth_cookie_header=tf.get("cookie"),
                            bearer_token=tf.get("bearer_token"),
                            worker_token_header=tf.get("worker_token"),
                            timeout=int(tf.get("timeout") or 10),
                        )
                        meta = meta_obj.raw
                        meta_used = True
                    except TemplateMetaFetchError as e:
                        meta = None
                        meta_fetch_error = str(e)[:500]

            # 2) 이미지 로드 및 리사이징
            img_bgr = cv2.imread(local_path)
            if img_bgr is None:
                return AIResult.failed(job.id, "cannot read image")
            
            # 대용량 이미지 리사이징 (처리 전)
            img_bgr, was_resized = resize_if_large(img_bgr, max_megapixels=4.0)

            aligned = img_bgr

            # 3) mode 정책
            if mode == "photo":
                warped = warp_to_a4_landscape(img_bgr)
                if warped is None:
                    return AIResult.failed(job.id, "warp_failed_for_photo_mode")
                aligned = warped

            elif mode == "auto":
                warped = warp_to_a4_landscape(img_bgr)
                if warped is not None:
                    aligned = warped

            # 4) meta 없으면 legacy
            if not meta:
                questions = payload.get("questions") or []
                if not questions:
                    return AIResult.failed(job.id, "template_meta/template_fetch failed and legacy questions missing")

                answers = detect_omr_answers_v1(
                    image_path=local_path,
                    questions=list(questions),
                    cfg=None,
                )
                return AIResult.done(
                    job.id,
                    {
                        "version": "v1",
                        "mode": "legacy_questions",
                        "aligned": False,
                        "identifier": None,
                        "answers": answers,
                        "meta_used": False,
                        "debug": {
                            "meta_fetch_error": meta_fetch_error,
                        },
                    },
                )

            # 5) ROI + identifier
            h, w = aligned.shape[:2]
            questions_payload = build_questions_payload_from_meta(meta, (w, h))

            ident = detect_identifier_v1(
                image_bgr=aligned,
                meta=meta,
                cfg=IdentifierConfigV1(),
            )

            # 6) aligned 저장 후 OMR
            import tempfile, os

            tmp_path = os.path.join(tempfile.gettempdir(), f"omr_aligned_{job.id}.jpg")
            cv2.imwrite(tmp_path, aligned)

            cfg = OMRConfigV1()
            answers = detect_omr_answers_v1(
                image_path=tmp_path,
                questions=list(questions_payload),
                cfg=cfg,
            )

            return AIResult.done(
                job.id,
                {
                    "version": "v1",
                    "mode": mode,
                    "aligned": bool(aligned is not img_bgr),
                    "identifier": ident,
                    "answers": answers,
                    "meta_used": meta_used,
                    "debug": {
                        "meta_fetch_error": meta_fetch_error,
                    },
                },
            )

        return AIResult.failed(job.id, f"Unsupported job type: {job.type}")

    except Exception as e:
        return AIResult.failed(job.id, str(e))


==========================================================================================
# FILE: ai_worker/ai/pipelines/excel_export_handler.py
==========================================================================================
# PATH: apps/worker/ai_worker/ai/pipelines/excel_export_handler.py
# 엑셀 내보내기(출석/직원) — 워커에서 DB 조회 → openpyxl 생성 → R2 업로드 → download_url 반환

from __future__ import annotations

import logging
from io import BytesIO

from apps.shared.contracts.ai_result import AIResult
from apps.shared.contracts.ai_job import AIJob

logger = logging.getLogger(__name__)

EXCEL_EXPORT_EXPIRES_IN = 3600  # 1시간


def _upload_and_presign(
    *,
    job_id: str,
    tenant_id: str,
    fileobj: BytesIO,
    filename: str,
) -> str:
    """R2 엑셀 버킷에 업로드 후 presigned GET URL 반환."""
    from apps.infrastructure.storage.r2 import (
        upload_fileobj_to_r2_excel,
        generate_presigned_get_url_excel,
    )

    key = f"exports/{tenant_id}/{job_id}_{filename}"
    content_type = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    upload_fileobj_to_r2_excel(
        fileobj=fileobj,
        key=key,
        content_type=content_type,
    )
    return generate_presigned_get_url_excel(key=key, expires_in=EXCEL_EXPORT_EXPIRES_IN)


def handle_attendance_excel_export(job: AIJob) -> AIResult:
    """출석 엑셀 내보내기: lecture_id → DB 조회 → openpyxl → R2 → download_url."""
    payload = job.payload or {}
    lecture_id = payload.get("lecture_id")
    tenant_id = str(payload.get("tenant_id") or job.tenant_id or "")

    if not lecture_id or not tenant_id:
        return AIResult.failed(job.id, "payload.lecture_id and tenant_id required")

    try:
        from academy.adapters.db.django.repositories_enrollment import get_lecture_by_id_and_tenant_id
        from apps.domains.attendance.utils.excel import build_attendance_excel

        lecture = get_lecture_by_id_and_tenant_id(lecture_id, tenant_id)
        if not lecture:
            return AIResult.failed(job.id, "lecture not found")

        workbook, filename = build_attendance_excel(lecture)
        buffer = BytesIO()
        workbook.save(buffer)
        buffer.seek(0)

        download_url = _upload_and_presign(
            job_id=job.id,
            tenant_id=tenant_id,
            fileobj=buffer,
            filename=filename,
        )
        logger.info(
            "ATTENDANCE_EXCEL_EXPORT done job_id=%s tenant_id=%s lecture_id=%s",
            job.id,
            tenant_id,
            lecture_id,
        )
        return AIResult.done(
            job.id,
            {"download_url": download_url, "filename": filename},
        )
    except Exception as e:
        logger.exception(
            "ATTENDANCE_EXCEL_EXPORT failed job_id=%s tenant_id=%s lecture_id=%s: %s",
            job.id,
            tenant_id,
            lecture_id,
            e,
        )
        return AIResult.failed(job.id, str(e)[:2000])


def handle_staff_excel_export(job: AIJob) -> AIResult:
    """직원 급여 엑셀 내보내기: year, month → DB 조회 → openpyxl → R2 → download_url."""
    payload = job.payload or {}
    year = payload.get("year")
    month = payload.get("month")
    tenant_id = str(payload.get("tenant_id") or job.tenant_id or "")

    if not year or not month or not tenant_id:
        return AIResult.failed(job.id, "payload.year, month and tenant_id required")

    try:
        from academy.adapters.db.django.repositories_staffs import get_payroll_snapshots_for_excel

        qs = get_payroll_snapshots_for_excel(tenant_id, year, month)

        from openpyxl import Workbook
        from openpyxl.styles import Font, Alignment

        wb = Workbook()
        ws = wb.active
        ws.title = f"{year}-{month} 급여정산"

        headers = [
            "직원명", "연도", "월", "근무시간",
            "급여", "승인된 비용", "총 지급액", "확정자", "확정일시",
        ]
        ws.append(headers)

        for c in ws[1]:
            c.font = Font(bold=True)
            c.alignment = Alignment(horizontal="center")

        for s in qs:
            ws.append([
                s.staff.name,
                s.year,
                s.month,
                float(s.work_hours),
                s.work_amount,
                s.approved_expense_amount,
                s.total_amount,
                getattr(s.generated_by, "username", "") if s.generated_by else "",
                s.created_at.strftime("%Y-%m-%d %H:%M:%S"),
            ])

        filename = f"payroll_{year}_{month}.xlsx"
        buffer = BytesIO()
        wb.save(buffer)
        buffer.seek(0)

        download_url = _upload_and_presign(
            job_id=job.id,
            tenant_id=tenant_id,
            fileobj=buffer,
            filename=filename,
        )
        logger.info(
            "STAFF_EXCEL_EXPORT done job_id=%s tenant_id=%s year=%s month=%s",
            job.id,
            tenant_id,
            year,
            month,
        )
        return AIResult.done(
            job.id,
            {"download_url": download_url, "filename": filename},
        )
    except Exception as e:
        logger.exception(
            "STAFF_EXCEL_EXPORT failed job_id=%s tenant_id=%s: %s",
            job.id,
            tenant_id,
            e,
        )
        return AIResult.failed(job.id, str(e)[:2000])


==========================================================================================
# FILE: ai_worker/ai/pipelines/excel_handler.py
==========================================================================================
# PATH: apps/worker/ai_worker/ai/pipelines/excel_handler.py
# EXCEL_PARSING 작업 처리 — R2 다운로드 → 파싱·등록 → 로컬/R2 자원 정리

from __future__ import annotations

import logging
import os

from apps.shared.contracts.ai_result import AIResult
from apps.shared.contracts.ai_job import AIJob
from src.application.services.excel_parsing_service import ExcelParsingService
from src.infrastructure.storage import R2ObjectStorageAdapter

logger = logging.getLogger(__name__)

# 엑셀 버킷: 호출 시점 환경변수 참조 (하드코딩 금지)
def _excel_bucket(payload: dict) -> str:
    return (
        payload.get("bucket")
        or os.environ.get("EXCEL_BUCKET_NAME")
        or "academy-excel"
    )


def _record_progress(job_id: str, step: str, percent: int) -> None:
    """Redis 진행률 기록 (우하단 실시간 프로그래스바용)."""
    try:
        from src.infrastructure.cache.redis_progress_adapter import RedisProgressAdapter
        RedisProgressAdapter().record_progress(job_id, step, {"percent": percent})
    except Exception as e:
        logger.debug("Redis progress record skip: %s", e)


def handle_excel_parsing_job(job: AIJob) -> AIResult:
    """
    EXCEL_PARSING 작업: R2에서 Get → ExcelParsingService(비즈니스 핵심) → 수강등록.
    어떤 상황(성공/예외)에서도 finally에서 R2 원본 객체 삭제 수행.
    """
    payload = job.payload or {}
    file_key = payload.get("file_key")
    if not file_key:
        return AIResult.failed(job.id, "payload.file_key required")

    bucket = _excel_bucket(payload)
    storage = R2ObjectStorageAdapter()
    _record_progress(job.id, "downloading", 10)

    try:
        service = ExcelParsingService(storage)
        _record_progress(job.id, "parsing", 40)
        result = service.run(job.id, payload)
        _record_progress(job.id, "done", 100)
        result["processed_by"] = "worker"
        logger.info(
            "EXCEL_PARSING processed_by=worker job_id=%s enrolled=%s",
            job.id,
            result.get("enrolled_count"),
        )
        return AIResult.done(job.id, result)
    except Exception as e:
        logger.exception(
            "EXCEL_PARSING failed job_id=%s tenant_id=%s lecture_id=%s: %s",
            job.id,
            payload.get("tenant_id"),
            payload.get("lecture_id"),
            e,
        )
        return AIResult.failed(job.id, str(e)[:2000])
    finally:
        # 더블 체크: 성공/실패/예외와 관계없이 R2 원본 삭제 (로컬 tmp는 ExcelParsingService.run finally에서 정리)
        try:
            storage.delete_object(bucket, file_key)
            logger.debug("EXCEL_PARSING R2 cleanup done bucket=%s key=%s", bucket, file_key)
        except Exception as e:
            logger.warning("R2 delete_object after EXCEL_PARSING bucket=%s key=%s: %s", bucket, file_key, e)


==========================================================================================
# FILE: ai_worker/ai/pipelines/homework_video_analyzer.py
==========================================================================================
# apps/worker/ai/pipelines/homework_video_analyzer.py
from __future__ import annotations

from typing import Dict, Any, List
import cv2  # type: ignore
import numpy as np  # type: ignore

from apps.worker.ai_worker.ai.pipelines.video_frame_extractor import (
    extract_key_frames,
    extract_frame_at_index,
)
from apps.worker.ai_worker.ai.utils.image_resizer import resize_if_large


def _estimate_writing_score(gray_roi: np.ndarray) -> float:
    if gray_roi.size == 0:
        return 0.0
    dark = (gray_roi < 220).sum()
    total = gray_roi.size
    return float(dark) / float(total)


def analyze_homework_video(
    video_path: str,
    frame_stride: int = 10,
    min_frame_count: int = 30,
    use_key_frames: bool = True,  # 키 프레임 추출 사용 여부
    max_pages: int = 10,  # 최대 페이지 수
    processing_timeout: int = 60,  # 처리 타임아웃 (초)
) -> Dict[str, Any]:
    # 키 프레임 추출 사용 (권장)
    if use_key_frames:
        try:
            key_frames_info = extract_key_frames(
                video_path=video_path,
                target_fps=2.0,  # 1-3 fps 권장
                max_pages=max_pages,
                processing_timeout=processing_timeout,
            )
            
            # 키 프레임에서만 분석
            frame_results: List[Dict[str, Any]] = []
            
            for key_frame in key_frames_info["key_frames"]:
                frame_idx = key_frame["frame_index"]
                frame = extract_frame_at_index(video_path, frame_idx)
                
                if frame is None:
                    continue
                
                # 이미지 리사이징 (대용량 처리 전)
                frame_resized, was_resized = resize_if_large(frame, max_megapixels=4.0)
                gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)
                score = _estimate_writing_score(gray)
                
                frame_results.append({
                    "index": frame_idx,
                    "timestamp": key_frame["timestamp"],
                    "page_number": key_frame["page_number"],
                    "writing_score": round(float(score), 4),
                    "has_writing": bool(score >= 0.05),
                    "was_resized": was_resized,
                })
            
            sampled = len(frame_results)
            if sampled == 0:
                return {
                    "total_frames": key_frames_info["total_frames"],
                    "sampled_frames": 0,
                    "avg_writing_score": 0.0,
                    "filled_ratio": 0.0,
                    "frames": [],
                    "pages_detected": 0,
                    "too_short": key_frames_info["total_frames"] < min_frame_count,
                }
            
            avg = sum(fr["writing_score"] for fr in frame_results) / sampled
            filled = sum(1 for fr in frame_results if fr["has_writing"])
            ratio = filled / sampled
            
            return {
                "total_frames": key_frames_info["total_frames"],
                "sampled_frames": sampled,
                "pages_detected": key_frames_info["pages_detected"],
                "avg_writing_score": round(float(avg), 4),
                "filled_ratio": round(float(ratio), 4),
                "frames": frame_results,
                "too_short": key_frames_info["total_frames"] < min_frame_count,
                "key_frames_extraction": True,
            }
        except Exception as e:
            # 키 프레임 추출 실패 시 레거시 방식으로 fallback
            import logging
            logger = logging.getLogger(__name__)
            logger.warning("Key frame extraction failed, using legacy method: %s", e)
    
    # 레거시 방식 (전체 프레임 샘플링)
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"cannot open video: {video_path}")

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_idx = 0
    frame_results: List[Dict[str, Any]] = []

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        if frame_idx % frame_stride != 0:
            frame_idx += 1
            continue

        # 이미지 리사이징 (대용량 처리 전)
        frame_resized, was_resized = resize_if_large(frame, max_megapixels=4.0)
        gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)
        score = _estimate_writing_score(gray)

        frame_results.append(
            {
                "index": frame_idx,
                "writing_score": round(float(score), 4),
                "has_writing": bool(score >= 0.05),
                "was_resized": was_resized,
            }
        )
        frame_idx += 1

    cap.release()

    sampled = len(frame_results)
    if sampled == 0:
        return {
            "total_frames": total_frames,
            "sampled_frames": 0,
            "avg_writing_score": 0.0,
            "filled_ratio": 0.0,
            "frames": [],
            "too_short": total_frames < min_frame_count,
            "key_frames_extraction": False,
        }

    avg = sum(fr["writing_score"] for fr in frame_results) / sampled
    filled = sum(1 for fr in frame_results if fr["has_writing"])
    ratio = filled / sampled

    return {
        "total_frames": total_frames,
        "sampled_frames": sampled,
        "avg_writing_score": round(float(avg), 4),
        "filled_ratio": round(float(ratio), 4),
        "frames": frame_results,
        "too_short": total_frames < min_frame_count,
        "key_frames_extraction": False,
    }


==========================================================================================
# FILE: ai_worker/ai/pipelines/tier_enforcer.py
==========================================================================================
"""
Tier별 처리 제한 강제

비즈니스 규칙:
- Lite: CPU OCR만 허용
- Basic: CPU 기반 OMR/status detection + 개선된 CPU OCR
- Premium: GPU 기반 전체 OCR + 고급 분석 (향후)
"""

from __future__ import annotations

import logging
from typing import Optional

logger = logging.getLogger(__name__)


def enforce_tier_limits(
    *,
    tier: str,
    job_type: str,
) -> tuple[bool, Optional[str]]:
    """
    Tier별 처리 제한 강제
    
    Args:
        tier: Tier ("lite" | "basic" | "premium")
        job_type: 작업 타입
        
    Returns:
        tuple: (허용 여부, 에러 메시지)
    """
    tier = tier.lower()
    job_type_lower = job_type.lower()
    
    # Lite: OCR + 엑셀 파싱 (경량)
    if tier == "lite":
        if job_type_lower not in ("ocr", "excel_parsing"):
            return False, f"Tier 'lite' only allows 'ocr', 'excel_parsing', got '{job_type}'"
        return True, None

    # Basic: OCR + OMR/status detection + 엑셀 파싱
    if tier == "basic":
        allowed_types = ("ocr", "omr_grading", "homework_video_analysis", "excel_parsing")
        if job_type_lower not in allowed_types:
            return False, f"Tier 'basic' only allows {allowed_types}, got '{job_type}'"
        return True, None
    
    # Premium: 모든 작업 허용
    if tier == "premium":
        return True, None
    
    return False, f"Unknown tier: {tier}"


==========================================================================================
# FILE: ai_worker/ai/pipelines/video_frame_extractor.py
==========================================================================================
"""
비디오 프레임 추출 모듈

요구사항:
- 전체 비디오 프레임 분석 금지
- 1-3 fps로 샘플링
- 페이지 전환 감지 (SSIM/Frame Difference)
- 페이지당 대표 프레임 1개만 추출
- 최대 페이지 수 제한
- 처리 타임아웃 강제
"""

from __future__ import annotations

import logging
import time
from typing import List, Dict, Any, Optional, Tuple
import cv2  # type: ignore
import numpy as np  # type: ignore

try:
    from skimage.metrics import structural_similarity as ssim  # type: ignore
    HAS_SSIM = True
except ImportError:
    HAS_SSIM = False

logger = logging.getLogger(__name__)


def _calculate_ssim(img1: np.ndarray, img2: np.ndarray) -> float:
    """
    두 이미지 간 SSIM 계산
    
    Args:
        img1: 첫 번째 이미지 (grayscale)
        img2: 두 번째 이미지 (grayscale)
        
    Returns:
        float: SSIM 값 (0.0 ~ 1.0)
    """
    if not HAS_SSIM:
        # SSIM이 없으면 Frame Difference 사용
        return 1.0 - _calculate_frame_diff(img1, img2)
    
    try:
        # 이미지 크기 맞추기
        if img1.shape != img2.shape:
            img2 = cv2.resize(img2, (img1.shape[1], img1.shape[0]))
        
        # SSIM 계산
        score = ssim(img1, img2, data_range=255)
        return float(score)
    except Exception as e:
        logger.warning("SSIM calculation failed: %s", e)
        # Fallback to frame diff
        return 1.0 - _calculate_frame_diff(img1, img2)


def _calculate_frame_diff(img1: np.ndarray, img2: np.ndarray) -> float:
    """
    두 프레임 간 차이 계산 (간단한 방법)
    
    Args:
        img1: 첫 번째 프레임 (grayscale)
        img2: 두 번째 프레임 (grayscale)
        
    Returns:
        float: 차이 비율 (0.0 ~ 1.0)
    """
    try:
        # 이미지 크기 맞추기
        if img1.shape != img2.shape:
            img2 = cv2.resize(img2, (img1.shape[1], img1.shape[0]))
        
        # 절대 차이 계산
        diff = cv2.absdiff(img1, img2)
        diff_ratio = np.sum(diff > 30) / diff.size  # 임계값 30
        return float(diff_ratio)
    except Exception as e:
        logger.warning("Frame diff calculation failed: %s", e)
        return 0.0


def extract_key_frames(
    video_path: str,
    target_fps: float = 2.0,  # 1-3 fps 권장
    max_pages: int = 10,  # 최대 페이지 수 제한
    page_change_threshold: float = 0.3,  # 페이지 전환 임계값 (SSIM 또는 diff)
    use_ssim: bool = True,  # SSIM 사용 여부 (False면 frame diff 사용)
    processing_timeout: int = 60,  # 처리 타임아웃 (초)
) -> Dict[str, Any]:
    """
    비디오에서 키 프레임 추출 (페이지당 1개)
    
    Args:
        video_path: 비디오 파일 경로
        target_fps: 목표 FPS (1-3 권장)
        max_pages: 최대 페이지 수
        page_change_threshold: 페이지 전환 감지 임계값
        use_ssim: SSIM 사용 여부
        processing_timeout: 처리 타임아웃 (초)
        
    Returns:
        dict: 추출된 키 프레임 정보
    """
    start_time = time.time()
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"cannot open video: {video_path}")
    
    try:
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0
        
        # 샘플링 간격 계산 (target_fps에 맞춤)
        frame_interval = int(fps / target_fps) if fps > 0 else 1
        frame_interval = max(1, frame_interval)  # 최소 1프레임
        
        logger.info(
            "Video info: fps=%.2f, total_frames=%d, duration=%.2fs, frame_interval=%d",
            fps,
            total_frames,
            duration,
            frame_interval,
        )
        
        key_frames: List[Dict[str, Any]] = []
        prev_frame: Optional[np.ndarray] = None
        current_page_frames: List[Tuple[int, np.ndarray]] = []  # (frame_idx, frame)
        frame_idx = 0
        
        while True:
            # 타임아웃 체크
            if time.time() - start_time > processing_timeout:
                logger.warning("Processing timeout reached: %ds", processing_timeout)
                break
            
            ret, frame = cap.read()
            if not ret:
                break
            
            # 샘플링: target_fps에 맞춰 프레임 선택
            if frame_idx % frame_interval != 0:
                frame_idx += 1
                continue
            
            # Grayscale 변환
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # 첫 프레임이거나 페이지 전환 감지
            is_page_change = False
            
            if prev_frame is None:
                # 첫 프레임
                is_page_change = True
            else:
                # 페이지 전환 감지
                if use_ssim:
                    similarity = _calculate_ssim(prev_frame, gray)
                    is_page_change = similarity < (1.0 - page_change_threshold)
                else:
                    diff_ratio = _calculate_frame_diff(prev_frame, gray)
                    is_page_change = diff_ratio > page_change_threshold
            
            if is_page_change:
                # 이전 페이지의 대표 프레임 선택 (중간 프레임)
                if current_page_frames:
                    mid_idx = len(current_page_frames) // 2
                    rep_frame_idx, rep_frame = current_page_frames[mid_idx]
                    
                    key_frames.append({
                        "frame_index": rep_frame_idx,
                        "timestamp": rep_frame_idx / fps if fps > 0 else 0,
                        "page_number": len(key_frames) + 1,
                    })
                    
                    logger.debug(
                        "Page %d detected: frame_idx=%d, frames_in_page=%d",
                        len(key_frames),
                        rep_frame_idx,
                        len(current_page_frames),
                    )
                
                # 최대 페이지 수 체크
                if len(key_frames) >= max_pages:
                    logger.warning("Max pages reached: %d", max_pages)
                    break
                
                # 새 페이지 시작
                current_page_frames = [(frame_idx, gray)]
            else:
                # 같은 페이지에 프레임 추가
                current_page_frames.append((frame_idx, gray))
            
            prev_frame = gray
            frame_idx += 1
        
        # 마지막 페이지 처리
        if current_page_frames and len(key_frames) < max_pages:
            mid_idx = len(current_page_frames) // 2
            rep_frame_idx, rep_frame = current_page_frames[mid_idx]
            
            key_frames.append({
                "frame_index": rep_frame_idx,
                "timestamp": rep_frame_idx / fps if fps > 0 else 0,
                "page_number": len(key_frames) + 1,
            })
        
        processing_time = time.time() - start_time
        
        return {
            "total_frames": total_frames,
            "video_fps": fps,
            "duration": duration,
            "key_frames": key_frames,
            "pages_detected": len(key_frames),
            "processing_time": processing_time,
            "frame_interval": frame_interval,
        }
        
    finally:
        cap.release()


def extract_frame_at_index(
    video_path: str,
    frame_index: int,
) -> Optional[np.ndarray]:
    """
    특정 인덱스의 프레임 추출
    
    Args:
        video_path: 비디오 파일 경로
        frame_index: 프레임 인덱스
        
    Returns:
        np.ndarray: 프레임 이미지 (BGR) 또는 None
    """
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return None
    
    try:
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
        ret, frame = cap.read()
        return frame if ret else None
    finally:
        cap.release()


==========================================================================================
# FILE: ai_worker/ai/problem/__init__.py
==========================================================================================
# apps/worker/ai/problem/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/problem/generator.py
==========================================================================================
# apps/worker/ai/problem/generator.py
from __future__ import annotations

import json
from dataclasses import dataclass
from typing import Optional

from apps.worker.ai_worker.ai.config import AIConfig
from apps.worker.ai_worker.ai.problem.prompt import BASE_PROMPT

try:
    from openai import OpenAI  # type: ignore
except Exception:
    OpenAI = None  # type: ignore


@dataclass
class ParsedProblem:
    body: str
    choices: list
    answer: Optional[str]
    difficulty: int
    tag: str
    summary: str
    explanation: str


_client: Optional["OpenAI"] = None


def _get_client() -> "OpenAI":
    global _client
    if _client is not None:
        return _client

    if OpenAI is None:
        raise RuntimeError("openai not installed")

    cfg = AIConfig.load()
    if not cfg.OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY not set")

    _client = OpenAI(api_key=cfg.OPENAI_API_KEY)
    return _client


def generate_problem_from_ocr(ocr_text: str) -> ParsedProblem:
    cfg = AIConfig.load()
    prompt = BASE_PROMPT.format(ocr_text=ocr_text)

    client = _get_client()
    response = client.chat.completions.create(
        model=cfg.PROBLEM_GEN_MODEL,
        messages=[
            {"role": "system", "content": "당신은 교육용 시험 문제를 자동 생성하는 엔진입니다."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.2,
    )

    # SDK 형태 차이 방어
    msg = response.choices[0].message
    content = getattr(msg, "content", None) or msg.get("content")  # type: ignore

    data = json.loads(content)

    return ParsedProblem(
        body=data.get("body", ""),
        choices=data.get("choices", []),
        answer=data.get("answer"),
        difficulty=int(data.get("difficulty", 3)),
        tag=data.get("tag", ""),
        summary=data.get("summary", ""),
        explanation=data.get("explanation", ""),
    )


==========================================================================================
# FILE: ai_worker/ai/problem/prompt.py
==========================================================================================
# apps/worker/ai/problem/prompt.py
BASE_PROMPT = """
다음은 시험 문제의 OCR 결과입니다.
아래 텍스트를 기반으로 문제 정보를 JSON 형식으로 추출하세요.

요구사항:
1) 문제 본문 (body)
2) 선택지 (choices): 없으면 빈 배열
3) 정답 (answer): 명시된 정답이 없으면 AI가 추론, 추론 불가 시 null
4) 난이도 (difficulty): 1~5 정수로 추정
5) 태그 (tag): 수학/과학/국어 등 간단한 분류
6) 문제 요약 (summary)
7) 해설 (explanation): 간단명료하게

출력은 반드시 JSON 형식만 사용하세요. 다른 텍스트는 포함하지 마세요.

출력 형식 예시:
{
  "body": "...",
  "choices": ["A...", "B...", "C...", "D..."],
  "answer": "C",
  "difficulty": 3,
  "tag": "수학",
  "summary": "...",
  "explanation": "..."
}

OCR 텍스트:
\"\"\"
{ocr_text}
\"\"\"
"""


==========================================================================================
# FILE: ai_worker/ai/utils/image_resizer.py
==========================================================================================
"""
이미지 리사이징 유틸리티

대용량 이미지 처리 전 리사이징으로 성능 향상 및 메모리 사용량 감소
"""

from __future__ import annotations

import logging
from typing import Optional, Tuple
import cv2  # type: ignore
import numpy as np  # type: ignore

logger = logging.getLogger(__name__)


def resize_if_large(
    image: np.ndarray,
    max_width: int = 1920,
    max_height: int = 1920,
    max_megapixels: float = 4.0,  # 4MP (약 2000x2000)
) -> Tuple[np.ndarray, bool]:
    """
    이미지가 크면 리사이징
    
    Args:
        image: 입력 이미지 (BGR 또는 Grayscale)
        max_width: 최대 너비
        max_height: 최대 높이
        max_megapixels: 최대 메가픽셀 수
        
    Returns:
        tuple: (리사이징된 이미지, 리사이징 여부)
    """
    h, w = image.shape[:2]
    current_mp = (w * h) / 1_000_000
    
    # 메가픽셀 체크
    if current_mp <= max_megapixels and w <= max_width and h <= max_height:
        return image, False
    
    # 리사이징 비율 계산
    scale_w = max_width / w if w > max_width else 1.0
    scale_h = max_height / h if h > max_height else 1.0
    scale_mp = np.sqrt(max_megapixels / current_mp) if current_mp > max_megapixels else 1.0
    
    # 가장 작은 스케일 사용 (가장 제한적인 조건)
    scale = min(scale_w, scale_h, scale_mp)
    
    new_w = int(w * scale)
    new_h = int(h * scale)
    
    logger.info(
        "Resizing image: %dx%d -> %dx%d (scale=%.2f, mp=%.2f -> %.2f)",
        w,
        h,
        new_w,
        new_h,
        scale,
        current_mp,
        (new_w * new_h) / 1_000_000,
    )
    
    resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)
    return resized, True


def resize_to_fit(
    image: np.ndarray,
    target_width: Optional[int] = None,
    target_height: Optional[int] = None,
    maintain_aspect: bool = True,
) -> np.ndarray:
    """
    이미지를 지정된 크기에 맞게 리사이징
    
    Args:
        image: 입력 이미지
        target_width: 목표 너비
        target_height: 목표 높이
        maintain_aspect: 종횡비 유지 여부
        
    Returns:
        np.ndarray: 리사이징된 이미지
    """
    h, w = image.shape[:2]
    
    if target_width is None and target_height is None:
        return image
    
    if maintain_aspect:
        if target_width and target_height:
            # 둘 다 지정된 경우, 작은 쪽에 맞춤
            scale_w = target_width / w
            scale_h = target_height / h
            scale = min(scale_w, scale_h)
        elif target_width:
            scale = target_width / w
        elif target_height:
            scale = target_height / h
        else:
            return image
        
        new_w = int(w * scale)
        new_h = int(h * scale)
    else:
        new_w = target_width or w
        new_h = target_height or h
    
    if new_w == w and new_h == h:
        return image
    
    return cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)


==========================================================================================
# FILE: ai_worker/apps/worker/ai_worker/celery.py
==========================================================================================



==========================================================================================
# FILE: ai_worker/storage/__init__.py
==========================================================================================
# apps.worker.ai_worker.storage


==========================================================================================
# FILE: ai_worker/storage/downloader.py
==========================================================================================
# ==============================================================================
# PATH: apps/worker/storage/downloader.py
#
# PURPOSE:
# - AI worker 전용 파일 다운로드 유틸
# - presigned GET URL → /tmp local file
# - R2 / S3 credential 사용 ❌
# - video_worker 코드와 절대 공유하지 않음
# ==============================================================================

from __future__ import annotations

import os
import tempfile
import requests
from pathlib import Path


def download_to_tmp(
    *,
    download_url: str,
    job_id: str,
    suffix: str | None = None,
    timeout: int = 60,
    chunk_size: int = 1024 * 1024,  # 1MB
) -> str:
    """
    presigned GET URL로 파일을 다운로드하여 local temp file 경로 반환

    규칙:
    - AI worker는 URL만 신뢰
    - 파일은 /tmp 또는 OS temp dir에 생성
    - 호출자는 반환된 path만 사용
    """

    tmp_dir = Path(tempfile.gettempdir())
    ext = suffix or ""

    filename = f"ai_job_{job_id}{ext}"
    tmp_path = tmp_dir / filename
    part_path = tmp_dir / f"{filename}.part"

    try:
        with requests.get(download_url, stream=True, timeout=timeout) as r:
            r.raise_for_status()

            expected_len = r.headers.get("Content-Length")
            expected_len = int(expected_len) if expected_len and expected_len.isdigit() else None

            written = 0
            with open(part_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=chunk_size):
                    if not chunk:
                        continue
                    f.write(chunk)
                    written += len(chunk)

            if expected_len is not None and written != expected_len:
                raise RuntimeError(
                    f"download size mismatch (expected={expected_len}, got={written})"
                )

        part_path.replace(tmp_path)
        return str(tmp_path)

    except Exception:
        try:
            if part_path.exists():
                part_path.unlink()
        except Exception:
            pass
        raise


==========================================================================================
# FILE: ai_worker/wrong_notes/__init__.py
==========================================================================================
# apps/worker/wrong_notes/__init__.py


==========================================================================================
# FILE: ai_worker/wrong_notes/templates/wrong_note.html
==========================================================================================
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="utf-8" />
    <style>
        body {
            font-family: Arial, Helvetica, sans-serif;
            font-size: 12px;
            line-height: 1.6;
        }
        h1 {
            border-bottom: 2px solid #333;
            padding-bottom: 4px;
        }
        .exam {
            margin-top: 20px;
        }
        .question {
            margin-left: 12px;
        }
        .wrong {
            color: #c0392b;
        }
    </style>
</head>
<body>
    <h1>오답 노트</h1>

    <p>
        학생 ID: {{ enrollment_id }}<br/>
        생성일: {{ created_at }}
    </p>

    {% for exam_id, items in grouped.items %}
        <div class="exam">
            <h2>시험 {{ exam_id }}</h2>
            {% for item in items %}
                <div class="question">
                    <span class="wrong">
                        Q{{ item.question_id }}
                    </span>
                    / 제출 답안: {{ item.answer }}
                </div>
            {% endfor %}
        </div>
    {% endfor %}
</body>
</html>


==========================================================================================
# FILE: messaging_worker/README.md
==========================================================================================
# Messaging Worker (SQS + Solapi)

SQS `academy-messaging-jobs` 수신 → 알림톡 우선 시도 → 실패 시 SMS 폴백. 예약 취소 시 발송 직전 Double Check로 스킵.

---

## 1. 인프라 (비용 절감)

- **인스턴스**: **t3.nano**(또는 가장 싼 스펙). CPU보다 네트워크 I/O만 쓰므로 고사양 불필요.
- **반드시 스팟 인스턴스(Spot Instance)** 로 구성 → 일반 온디맨드 대비 **70% 이상 절감**.

---

## 2. 환경변수

| 변수 | 필수 | 설명 |
|------|------|------|
| `SOLAPI_API_KEY` | ✅ | Solapi API 키 |
| `SOLAPI_API_SECRET` | ✅ | Solapi API 시크릿 |
| `SOLAPI_SENDER` | ✅ | 발신 번호 (예: 01012345678) |
| `SOLAPI_KAKAO_PF_ID` | - | 카카오 비즈니스 채널 ID (알림톡 사용 시) |
| `SOLAPI_KAKAO_TEMPLATE_ID` | - | 카카오 검수 완료 템플릿 ID. **ENV로 관리해 코드 수정 없이 교체** |
| `MESSAGING_SQS_QUEUE_NAME` | - | 기본값 `academy-messaging-jobs` |
| `AWS_REGION` | - | 기본값 `ap-northeast-2` |
| `MESSAGING_SQS_WAIT_SECONDS` | - | Long Polling 대기(기본 20) |
| `DJANGO_SETTINGS_MODULE` | - | 예약 취소 Double Check 시 설정 |

---

## 3. SQS 운용 전략

- **Visibility Timeout**: 솔라피 타임아웃(약 5~10초)보다 넉넉히 **30~60초** (기본 60). 짧으면 중복 발송 대참사.
- **Dead Letter Queue (DLQ)**: **3번 재시도** 후 실패 메시지는 DLQ로 격리 → "왜 문자 안 왔냐" 민원 확인용 로그.
- **Long Polling**: `WaitTimeSeconds` **20초** → SQS 빈 쿼리 비용 최소화.

---

## 4. 알림톡 / SMS 하이브리드 (Fallback)

1. **1단계 (알림톡)**: 단가 저렴 → `use_alimtalk_first=True` 이고 카카오 pf_id/template_id 설정 시 **최우선 발송**.
2. **2단계 (SMS 전환)**: 알림톡 발송 실패 또는 수신 거부 시, 워커가 **즉시 일반 문자로 재발송**.
3. **템플릿 관리**: 카카오 검수 끝난 **템플릿 ID만 ENV**(`SOLAPI_KAKAO_TEMPLATE_ID`)로 관리 → 코드 수정 없이 교체.

---

## 5. 예약 취소 대응 (Revoke + Double Check)

- 사용자가 예약을 취소하면, 해당 예약의 발송은 **SQS에 이미 들어간 뒤**일 수 있음.
- **발송 로직 입구에서 Double Check 필수**: `reservation_id`가 있으면 `is_reservation_cancelled(reservation_id)` 호출. **취소 상태면 발송 스킵**하고 메시지 삭제.
- 워커 실행 시 `DJANGO_SETTINGS_MODULE` 설정 시 `django.setup()` 호출 → `apps.support.messaging.services.is_reservation_cancelled()` 에서 `Reservation` 모델의 `status == 'CANCELLED'` 여부 조회.

---

## 6. 실행

```bash
pip install -r requirements/worker-messaging.txt
python scripts/create_sqs_resources.py ap-northeast-2

# 예약 취소 체크 필요 시
export DJANGO_SETTINGS_MODULE=apps.api.config.settings.prod
python -m apps.worker.messaging_worker.sqs_main
```

---

## 7. API/서비스에서 발송 요청

- **비동기 (권장)**  
  `enqueue_sms(tenant_id=request.tenant.id, to="01012345678", text="내용", reservation_id=123, use_alimtalk_first=True)`
  → SQS 적재, 워커가 tenant 잔액 검증·차감 후 알림톡(테넌트 PFID) 우선 → 실패 시 롤백·SMS, 예약 취소 시 스킵.
- **동기**  
  `send_sms(to="01012345678", text="내용")` (API 서버에 solapi 설치 필요).

API 키/시크릿은 **환경변수**로만 설정하고 코드에 노출하지 마세요.

---

## 8. 발송 ID 저장 (민원 대응)

Solapi 응답의 **group_id**(및 messageId)를 발송 로그/예약 테이블에 저장해 두면, "문자 안 왔어요" 민원 시 Solapi 콘솔에서 해당 ID로 조회해 원인 파악이 가능합니다.

---

## 9. Fake Solapi (DEBUG / 테스트)

- **DEBUG=True** 또는 **SOLAPI_MOCK=true** 이면 실제 API를 호출하지 않고, 발송될 JSON만 콘솔에 예쁘게 로그합니다.
- 잔액 차감·템플릿 미승인 에러를 피하려면 개발/스트레스 테스트 시 이 모드로 실행하세요.
- `apps.support.messaging.solapi_mock.MockSolapiMessageService` 사용.

## 10. 스트레스 테스트

```bash
# 100건 enqueue (워커는 별도 터미널에서 DEBUG=True 로 실행)
python scripts/stress_test_messaging_worker.py

# 100건 enqueue 후 워커 자동 실행, 큐가 비워질 때까지 대기
python scripts/stress_test_messaging_worker.py --run-worker
```

## 11. 알림톡 템플릿 변수 (미리 확정)

- `apps.support.messaging.alimtalk_templates` 에 템플릿 변수명 상수 및 치환 헬퍼 정의.
- **변수명**: `name`, `date`, `time`, `clinic_name`, `place`, `link`, `title` (DB 필드와 매칭).
- 솔라피/카카오 콘솔에 등록할 템플릿 문구 예: `#{name}님, #{date} #{time} #{clinic_name} 예약이 완료되었습니다.`
- 치환 데이터 생성: `build_replacements(context)` / `template_context_from_reservation(reservation)` 사용.


==========================================================================================
# FILE: messaging_worker/__init__.py
==========================================================================================
# apps/worker/messaging_worker — SQS 기반 메시지 발송 워커


==========================================================================================
# FILE: messaging_worker/config.py
==========================================================================================
# PATH: apps/worker/messaging_worker/config.py
"""메시지 발송 워커 설정 — 환경변수만 사용 (Django 불필요)"""

from __future__ import annotations

import os
import sys
from dataclasses import dataclass


def _require(name: str) -> str:
    v = os.environ.get(name)
    if not v:
        raise RuntimeError(f"Missing required env: {name}")
    return v


def _optional(name: str, default: str = "") -> str:
    return os.environ.get(name, default).strip()


@dataclass(frozen=True)
class Config:
    SOLAPI_API_KEY: str
    SOLAPI_API_SECRET: str
    SOLAPI_SENDER: str
    MESSAGING_SQS_QUEUE_NAME: str
    AWS_REGION: str
    SQS_WAIT_TIME_SECONDS: int
    # 알림톡 (카카오) — 템플릿 ENV로 관리, 코드 수정 없이 교체 가능
    SOLAPI_KAKAO_PF_ID: str
    SOLAPI_KAKAO_TEMPLATE_ID: str


def load_config() -> Config:
    try:
        return Config(
            SOLAPI_API_KEY=_require("SOLAPI_API_KEY"),
            SOLAPI_API_SECRET=_require("SOLAPI_API_SECRET"),
            SOLAPI_SENDER=_require("SOLAPI_SENDER"),
            MESSAGING_SQS_QUEUE_NAME=os.environ.get("MESSAGING_SQS_QUEUE_NAME", "academy-messaging-jobs"),
            AWS_REGION=os.environ.get("AWS_REGION", "ap-northeast-2"),
            SQS_WAIT_TIME_SECONDS=int(os.environ.get("MESSAGING_SQS_WAIT_SECONDS", "20")),
            # 알림톡: 미설정이면 SMS만 사용 (빈 문자열 허용)
            SOLAPI_KAKAO_PF_ID=os.environ.get("SOLAPI_KAKAO_PF_ID", "").strip(),
            SOLAPI_KAKAO_TEMPLATE_ID=os.environ.get("SOLAPI_KAKAO_TEMPLATE_ID", "").strip(),
        )
    except Exception as e:
        import logging
        logging.basicConfig(level=logging.INFO)
        logging.getLogger(__name__).critical("config error: %s", e)
        sys.exit(1)


==========================================================================================
# FILE: messaging_worker/sqs_main.py
==========================================================================================
"""
Messaging Worker - SQS 기반 메시지 발송

SQS academy-messaging-jobs 에서 수신 → Solapi SMS/LMS 발송
video_worker sqs_main 과 동일한 패턴 (Long Polling, Graceful shutdown)
"""

from __future__ import annotations

import json
import logging
import os
import signal
import sys
import time
from typing import Optional

from libs.queue import get_queue_client, QueueUnavailableError
from libs.redis.idempotency import acquire_job_lock, release_job_lock

from apps.worker.messaging_worker.config import load_config

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] [MESSAGING-WORKER] %(message)s",
)
logger = logging.getLogger("messaging_worker")

_shutdown = False
_current_receipt_handle: Optional[str] = None


def _handle_signal(sig, frame):
    global _shutdown, _current_receipt_handle
    logger.info(
        "Received signal, initiating graceful shutdown... | current_job=%s",
        "processing" if _current_receipt_handle else "idle",
    )
    _shutdown = True


def _get_solapi_client(cfg):
    """DEBUG=True 또는 SOLAPI_MOCK=true 이면 Mock (로그만), 아니면 실제 Solapi."""
    if os.environ.get("SOLAPI_MOCK", "").lower() in ("true", "1", "yes") or os.environ.get("DEBUG", "").lower() in ("true", "1", "yes"):
        from apps.support.messaging.solapi_mock import MockSolapiMessageService
        return MockSolapiMessageService(api_key=cfg.SOLAPI_API_KEY, api_secret=cfg.SOLAPI_API_SECRET)
    from solapi import SolapiMessageService
    return SolapiMessageService(api_key=cfg.SOLAPI_API_KEY, api_secret=cfg.SOLAPI_API_SECRET)


def send_one_alimtalk(
    cfg,
    to: str,
    sender: str,
    pf_id: str,
    template_id: str,
    replacements: Optional[list] = None,
) -> dict:
    """
    Solapi 알림톡 1건 발송. 실패/수신거부 시 caller가 SMS로 fallback.
    replacements: [{"key": "name", "value": "홍길동"}, ...] — 템플릿 #{name}, #{date}, #{clinic_name} 등 치환.
    """
    try:
        from solapi.model import RequestMessage
        from solapi.model.kakao.kakao_option import KakaoOption
    except ImportError:
        return {"status": "error", "reason": "solapi_not_installed"}
    client = _get_solapi_client(cfg)
    to = (to or "").replace("-", "").strip()
    if not to or not pf_id or not template_id:
        return {"status": "error", "reason": "to_pf_template_required"}
    try:
        kakao_option = KakaoOption(pf_id=pf_id, template_id=template_id)
        message = RequestMessage(
            from_=sender,
            to=to,
            kakao_options=kakao_option,
            replacements=replacements or None,
        )
        response = client.send(message)
        group_id = getattr(getattr(response, "group_info", None), "group_id", None)
        count = getattr(getattr(response, "group_info", None), "count", None)
        if count is not None and getattr(count, "registered_success", 0) == 0:
            reason = "alimtalk_failed_or_rejected"
            logger.warning("alimtalk no success to=%s****", to[:4])
            return {"status": "error", "reason": reason, "group_id": group_id}
        logger.info("send_alimtalk ok to=%s**** group_id=%s", to[:4], group_id)
        return {"status": "ok", "group_id": group_id}
    except Exception as e:
        logger.warning("alimtalk failed to=%s****: %s", to[:4], e)
        return {"status": "error", "reason": str(e)[:500]}


def send_one_sms(cfg, to: str, text: str, sender: str) -> dict:
    """
    Solapi로 SMS 1건 발송.
    Returns: {"status": "ok"|"error", "group_id"?, "reason"?}
    """
    try:
        from solapi.model import RequestMessage
    except ImportError as e:
        logger.error("solapi SDK not installed: %s", e)
        return {"status": "error", "reason": "solapi_not_installed"}
    client = _get_solapi_client(cfg)
    sender = (sender or cfg.SOLAPI_SENDER or "").strip()
    if not sender:
        return {"status": "error", "reason": "sender_required"}

    to = (to or "").replace("-", "").strip()
    text = (text or "").strip()
    if not to or not text:
        return {"status": "error", "reason": "to_and_text_required"}

    try:
        message = RequestMessage(from_=sender, to=to, text=text)
        response = client.send(message)
        group_id = getattr(getattr(response, "group_info", None), "group_id", None)
        # 발송 로그 테이블이 있으면 group_id(및 messageId) 저장 권장 → "문자 안 왔어요" 민원 시 Solapi 콘솔 조회용
        logger.info("send_sms ok to=%s**** group_id=%s", to[:4], group_id)
        return {"status": "ok", "group_id": group_id}
    except Exception as e:
        logger.exception("send_sms failed to=%s****", to[:4])
        return {"status": "error", "reason": str(e)[:500]}


def main() -> int:
    signal.signal(signal.SIGTERM, _handle_signal)
    signal.signal(signal.SIGINT, _handle_signal)

    # Django context: 예약/유저 등 DB 조회가 필요할 때 ORM 사용 가능하도록
    if os.environ.get("DJANGO_SETTINGS_MODULE"):
        import django
        django.setup()
        logger.info("Django setup done (ORM available)")

    cfg = load_config()
    queue_client = get_queue_client()

    # Long Polling 10~20초: 빈 큐에 반복 요청 방지 → AWS 비용·CPU 절약
    logger.info(
        "Messaging Worker started | queue=%s | wait_time=%ss",
        cfg.MESSAGING_SQS_QUEUE_NAME,
        cfg.SQS_WAIT_TIME_SECONDS,
    )

    consecutive_errors = 0
    max_consecutive_errors = 10

    try:
        while not _shutdown:
            try:
                try:
                    raw = queue_client.receive_message(
                        queue_name=cfg.MESSAGING_SQS_QUEUE_NAME,
                        wait_time_seconds=cfg.SQS_WAIT_TIME_SECONDS,
                    )
                except QueueUnavailableError as e:
                    logger.warning(
                        "SQS unavailable (AWS credentials invalid or missing?). Waiting 60s. %s",
                        e,
                    )
                    time.sleep(60)
                    continue
                if not raw:
                    continue

                body = raw.get("Body", "")
                receipt_handle = raw.get("ReceiptHandle")
                message_id = raw.get("MessageId") or receipt_handle
                if not receipt_handle:
                    logger.error("Message missing ReceiptHandle")
                    continue

                job_id = f"messaging:{message_id}"
                if not acquire_job_lock(job_id):
                    queue_client.delete_message(
                        queue_name=cfg.MESSAGING_SQS_QUEUE_NAME,
                        receipt_handle=receipt_handle,
                    )
                    continue

                global _current_receipt_handle
                try:
                    if isinstance(body, str):
                        try:
                            data = json.loads(body)
                        except json.JSONDecodeError:
                            logger.error("Invalid JSON in message body")
                            queue_client.delete_message(
                                queue_name=cfg.MESSAGING_SQS_QUEUE_NAME,
                                receipt_handle=receipt_handle,
                            )
                            continue
                    else:
                        data = body

                    if not isinstance(data, dict) or "to" not in data or "text" not in data:
                        logger.error("Invalid message format: %s", data)
                        queue_client.delete_message(
                            queue_name=cfg.MESSAGING_SQS_QUEUE_NAME,
                            receipt_handle=receipt_handle,
                        )
                        continue

                    tenant_id = data.get("tenant_id")
                    if tenant_id is None and os.environ.get("DJANGO_SETTINGS_MODULE"):
                        logger.warning("Message missing tenant_id, skipping (legacy message)")
                        queue_client.delete_message(
                            queue_name=cfg.MESSAGING_SQS_QUEUE_NAME,
                            receipt_handle=receipt_handle,
                        )
                        continue

                    # 예약 취소 Double Check: 발송 직전 한 번 더 확인
                    reservation_id = data.get("reservation_id")
                    if reservation_id is not None and os.environ.get("DJANGO_SETTINGS_MODULE"):
                        try:
                            from apps.support.messaging.services import is_reservation_cancelled
                            if is_reservation_cancelled(int(reservation_id)):
                                logger.info("reservation_id=%s cancelled, skip send", reservation_id)
                                queue_client.delete_message(
                                    queue_name=cfg.MESSAGING_SQS_QUEUE_NAME,
                                    receipt_handle=receipt_handle,
                                )
                                _current_receipt_handle = None
                                continue
                        except Exception as e:
                            logger.warning("reservation check failed: %s", e)

                    _current_receipt_handle = receipt_handle

                    to = str(data.get("to", ""))
                    text = str(data.get("text", ""))
                    sender = (data.get("sender") or "").strip() or cfg.SOLAPI_SENDER
                    use_alimtalk_first = bool(data.get("use_alimtalk_first"))
                    alimtalk_replacements = data.get("alimtalk_replacements") or []
                    template_id_msg = data.get("template_id") or ""

                    # 테넌트별 잔액·PFID·단가 (Django 있을 때만)
                    info = None
                    base_price = "0"
                    pf_id_tenant = ""
                    if tenant_id is not None and os.environ.get("DJANGO_SETTINGS_MODULE"):
                        try:
                            from apps.support.messaging.credit_services import (
                                get_tenant_messaging_info,
                                deduct_credits,
                                rollback_credits,
                            )
                            from apps.support.messaging.models import NotificationLog
                            from apps.core.models import Tenant
                            info = get_tenant_messaging_info(int(tenant_id))
                            if info:
                                base_price = info["base_price"]
                                pf_id_tenant = (info["kakao_pfid"] or "").strip()
                        except Exception as e:
                            logger.warning("get_tenant_messaging_info failed: %s", e)

                    # 알림톡 사용 시: 테넌트 PFID 또는 워커 기본 PFID
                    pf_id = pf_id_tenant or cfg.SOLAPI_KAKAO_PF_ID
                    template_id = (template_id_msg or "").strip() or cfg.SOLAPI_KAKAO_TEMPLATE_ID

                    # 잔액 검증 및 차감 (Django + info 있을 때, 단가 > 0)
                    deducted = False
                    try:
                        if info and float(base_price) > 0 and tenant_id is not None:
                            from decimal import Decimal
                            from apps.support.messaging.credit_services import deduct_credits
                            from academy.adapters.db.django.repositories_messaging import create_notification_log
                            bal = info.get("credit_balance", "0")
                            if float(bal) < float(base_price):
                                logger.warning(
                                    "tenant_id=%s insufficient_balance balance=%s base_price=%s, skip send",
                                    tenant_id, bal, base_price,
                                )
                                create_notification_log(
                                    tenant_id=int(tenant_id),
                                    success=False,
                                    amount_deducted=Decimal("0"),
                                    recipient_summary=to[:4] + "****",
                                    failure_reason="insufficient_balance",
                                )
                                queue_client.delete_message(
                                    queue_name=cfg.MESSAGING_SQS_QUEUE_NAME,
                                    receipt_handle=receipt_handle,
                                )
                                _current_receipt_handle = None
                                continue
                            deduct_credits(int(tenant_id), base_price)
                            deducted = True
                    except Exception as e:
                        logger.exception("deduct_credits failed: %s", e)
                        _current_receipt_handle = None
                        consecutive_errors += 1
                        continue

                    # 알림톡 → SMS 폴백: 알림톡 우선 시도, 실패 시 즉시 SMS
                    result = None
                    if use_alimtalk_first and pf_id and template_id:
                        result = send_one_alimtalk(
                            cfg, to=to, sender=sender,
                            pf_id=pf_id,
                            template_id=template_id,
                            replacements=alimtalk_replacements if isinstance(alimtalk_replacements, list) else None,
                        )
                        if result.get("status") != "ok":
                            logger.info("alimtalk failed, fallback to SMS")
                            result = send_one_sms(cfg, to=to, text=text, sender=sender)
                    else:
                        result = send_one_sms(cfg, to=to, text=text, sender=sender)

                    # 성공 시 로그, 실패 시 롤백 + 로그
                    if tenant_id is not None and os.environ.get("DJANGO_SETTINGS_MODULE") and info:
                        try:
                            from decimal import Decimal
                            from apps.support.messaging.credit_services import rollback_credits
                            from academy.adapters.db.django.repositories_messaging import create_notification_log
                            if result.get("status") == "ok":
                                create_notification_log(
                                    tenant_id=int(tenant_id),
                                    success=True,
                                    amount_deducted=Decimal(str(base_price)),
                                    recipient_summary=to[:4] + "****",
                                    template_summary=template_id or "SMS",
                                )
                            else:
                                if deducted:
                                    rollback_credits(int(tenant_id), base_price)
                                create_notification_log(
                                    tenant_id=int(tenant_id),
                                    success=False,
                                    amount_deducted=Decimal("0"),
                                    recipient_summary=to[:4] + "****",
                                    failure_reason=result.get("reason", "send_failed")[:500],
                                )
                        except Exception as e:
                            logger.exception("NotificationLog/rollback failed: %s", e)
                            if deducted and result.get("status") != "ok":
                                try:
                                    rollback_credits(int(tenant_id), base_price)
                                except Exception:
                                    pass

                    if result.get("status") == "ok":
                        queue_client.delete_message(
                            queue_name=cfg.MESSAGING_SQS_QUEUE_NAME,
                            receipt_handle=receipt_handle,
                        )
                        consecutive_errors = 0
                    else:
                        logger.warning("send failed, message will retry: %s", result.get("reason"))
                        consecutive_errors += 1
                        if consecutive_errors >= max_consecutive_errors:
                            logger.error("Too many consecutive errors (%s), exiting", consecutive_errors)
                            return 1

                    _current_receipt_handle = None

                    if _shutdown:
                        logger.info("Graceful shutdown: exiting")
                        break
                finally:
                    release_job_lock(job_id)

            except KeyboardInterrupt:
                break
            except QueueUnavailableError:
                # 이미 내부 try에서 처리하지만, 다른 경로로 올 수 있음
                time.sleep(60)
                continue
            except Exception as e:
                logger.exception("Unexpected error in main loop: %s", e)
                consecutive_errors += 1
                if consecutive_errors >= max_consecutive_errors:
                    return 1
                time.sleep(5)

        logger.info("Messaging Worker shutdown complete")
        return 0

    except Exception:
        logger.exception("Fatal error in Messaging Worker")
        return 1


if __name__ == "__main__":
    sys.exit(main())


==========================================================================================
# FILE: omr/roi_builder.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/roi_builder.py
from __future__ import annotations

from typing import Any, Dict, List, Tuple

import math


def _clamp(v: int, lo: int, hi: int) -> int:
    return max(lo, min(hi, v))


def build_questions_payload_from_meta(
    *,
    meta: Dict[str, Any],
    image_size_px: Tuple[int, int],
) -> List[Dict[str, Any]]:
    """
    meta(mm) -> questions payload (px) for detect_omr_answers_v1()

    detect_omr_answers_v1 expects:
      questions: [{question_id, roi:{x,y,w,h}, choices:[...], axis:"x"}, ...]

    image_size_px: (width, height)
    """
    img_w, img_h = image_size_px

    page = meta.get("page") or {}
    size = page.get("size") or {}
    page_w_mm = float(size.get("width") or 0.0)
    page_h_mm = float(size.get("height") or 0.0)
    if page_w_mm <= 0.0 or page_h_mm <= 0.0:
        raise ValueError("invalid meta page size")

    # 정렬된 스캔/워프 결과는 "페이지 전체가 이미지 전체"라고 가정
    sx = img_w / page_w_mm
    sy = img_h / page_h_mm

    out: List[Dict[str, Any]] = []
    for q in (meta.get("questions") or []):
        qnum = int(q.get("question_number") or 0)
        roi = q.get("roi") or {}

        x_mm = float(roi.get("x") or 0.0)
        y_mm = float(roi.get("y") or 0.0)
        w_mm = float(roi.get("w") or 0.0)
        h_mm = float(roi.get("h") or 0.0)

        x = int(round(x_mm * sx))
        y = int(round(y_mm * sy))
        w = int(round(w_mm * sx))
        h = int(round(h_mm * sy))

        # 안전 클램프
        x = _clamp(x, 0, img_w - 1)
        y = _clamp(y, 0, img_h - 1)
        w = _clamp(w, 1, img_w - x)
        h = _clamp(h, 1, img_h - y)

        out.append(
            {
                "question_id": qnum,  # worker 엔진은 question_id만 사용
                "roi": {"x": x, "y": y, "w": w, "h": h},
                "choices": ["A", "B", "C", "D", "E"],
                "axis": "x",
            }
        )

    # question_id 순서 보장
    out.sort(key=lambda d: int(d.get("question_id") or 0))
    return out


==========================================================================================
# FILE: omr/template_meta.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/template_meta.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Tuple, Optional
import requests


@dataclass(frozen=True)
class TemplateMeta:
    raw: Dict[str, Any]

    @property
    def units(self) -> str:
        return str(self.raw.get("units") or "mm")

    @property
    def page_size_mm(self) -> Tuple[float, float]:
        page = self.raw.get("page") or {}
        size = page.get("size") or {}
        return float(size.get("width") or 0.0), float(size.get("height") or 0.0)

    @property
    def questions(self) -> List[Dict[str, Any]]:
        return list(self.raw.get("questions") or [])


def fetch_objective_meta(
    *,
    base_url: str,
    question_count: int,
    auth_cookie_header: Optional[str] = None,
    timeout: int = 10,
) -> TemplateMeta:
    """
    worker -> API (assets meta)
    - 외부 SaaS 호출 금지: 내부 API만 호출
    - auth 방식은 운영 환경에 맞게 header/cookie를 전달
    """
    url = f"{base_url.rstrip('/')}/api/v1/assets/omr/objective/meta/"
    params = {"question_count": str(int(question_count))}

    headers: Dict[str, str] = {}
    if auth_cookie_header:
        headers["Cookie"] = auth_cookie_header

    r = requests.get(url, params=params, headers=headers, timeout=timeout)
    r.raise_for_status()
    data = r.json()
    return TemplateMeta(raw=data)


==========================================================================================
# FILE: omr/warp.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/warp.py
from __future__ import annotations

from typing import Optional, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore


def _order_points(pts: np.ndarray) -> np.ndarray:
    # pts: (4,2)
    rect = np.zeros((4, 2), dtype=np.float32)
    s = pts.sum(axis=1)
    rect[0] = pts[np.argmin(s)]  # top-left
    rect[2] = pts[np.argmax(s)]  # bottom-right

    diff = np.diff(pts, axis=1)
    rect[1] = pts[np.argmin(diff)]  # top-right
    rect[3] = pts[np.argmax(diff)]  # bottom-left
    return rect


def warp_to_a4_landscape(
    *,
    image_bgr: np.ndarray,
    out_size_px: Tuple[int, int] = (3508, 2480),  # 300dpi A4 landscape (근사)
) -> Optional[np.ndarray]:
    """
    촬영/프레임 이미지에서 문서(답안지) 외곽을 찾아 A4 landscape로 워프.
    성공하면 "페이지 전체 = 이미지 전체"가 되므로 meta ROI를 그대로 적용 가능.

    실패하면 None 반환 -> caller가 fallback(yolo/opencv segmentation 등) 처리
    """
    if image_bgr is None or image_bgr.size == 0:
        return None

    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5, 5), 0)
    edges = cv2.Canny(blur, 50, 150)

    # 윤곽 강화
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))
    edges = cv2.dilate(edges, kernel, iterations=1)

    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        return None

    contours = sorted(contours, key=cv2.contourArea, reverse=True)

    page_cnt = None
    for cnt in contours[:8]:
        peri = cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)
        if len(approx) == 4:
            page_cnt = approx
            break

    if page_cnt is None:
        return None

    pts = page_cnt.reshape(4, 2).astype(np.float32)
    rect = _order_points(pts)

    out_w, out_h = out_size_px
    dst = np.array(
        [
            [0, 0],
            [out_w - 1, 0],
            [out_w - 1, out_h - 1],
            [0, out_h - 1],
        ],
        dtype=np.float32,
    )

    M = cv2.getPerspectiveTransform(rect, dst)
    warped = cv2.warpPerspective(image_bgr, M, (out_w, out_h))
    return warped


==========================================================================================
# FILE: video_worker/config.py
==========================================================================================
# PATH: apps/worker/video_worker/config.py
from __future__ import annotations

import os
import sys
from dataclasses import dataclass


def _require(name: str) -> str:
    v = os.environ.get(name)
    if not v:
        raise RuntimeError(f"Missing required env: {name}")
    return v


def _require_any(*names: str) -> str:
    for name in names:
        v = os.environ.get(name)
        if v:
            return v
    raise RuntimeError(f"Missing required env (any of): {', '.join(names)}")


def _float(name: str, default: str) -> float:
    try:
        return float(os.environ.get(name, default))
    except Exception:
        return float(default)


def _int(name: str, default: str) -> int:
    try:
        return int(os.environ.get(name, default))
    except Exception:
        return int(default)


@dataclass(frozen=True)
class Config:
    # API / Auth
    API_BASE_URL: str
    INTERNAL_WORKER_TOKEN: str
    WORKER_ID: str

    # HTTP / retry
    HTTP_TIMEOUT_SECONDS: float
    RETRY_MAX_ATTEMPTS: int
    BACKOFF_BASE_SECONDS: float
    BACKOFF_CAP_SECONDS: float

    # Temp / Lock
    TEMP_DIR: str
    LOCK_DIR: str
    LOCK_STALE_SECONDS: int

    # Heartbeat
    HEARTBEAT_INTERVAL_SECONDS: int

    # ffmpeg / ffprobe
    FFMPEG_BIN: str
    FFPROBE_BIN: str
    FFPROBE_TIMEOUT_SECONDS: int
    FFMPEG_TIMEOUT_SECONDS: int

    # HLS / thumb
    HLS_TIME_SECONDS: int
    THUMBNAIL_AT_SECONDS: float
    MIN_SEGMENTS_PER_VARIANT: int

    # R2
    R2_BUCKET: str
    R2_PREFIX: str
    R2_ENDPOINT: str
    R2_ACCESS_KEY: str
    R2_SECRET_KEY: str
    R2_REGION: str
    UPLOAD_MAX_CONCURRENCY: int

    # download
    DOWNLOAD_TIMEOUT_SECONDS: float
    DOWNLOAD_CHUNK_BYTES: int


def load_config() -> Config:
    try:
        return Config(
            API_BASE_URL=_require("API_BASE_URL").rstrip("/"),
            INTERNAL_WORKER_TOKEN=_require("INTERNAL_WORKER_TOKEN"),
            WORKER_ID=os.environ.get("WORKER_ID", "video-worker"),

            HTTP_TIMEOUT_SECONDS=_float("VIDEO_WORKER_HTTP_TIMEOUT", "10.0"),
            RETRY_MAX_ATTEMPTS=_int("VIDEO_WORKER_RETRY_MAX", "6"),
            BACKOFF_BASE_SECONDS=_float("VIDEO_WORKER_BACKOFF_BASE", "0.5"),
            BACKOFF_CAP_SECONDS=_float("VIDEO_WORKER_BACKOFF_CAP", "10.0"),

            TEMP_DIR=os.environ.get("VIDEO_WORKER_TEMP_DIR", "/tmp/video-worker"),
            LOCK_DIR=os.environ.get("VIDEO_WORKER_LOCK_DIR", "/tmp/video-worker-locks"),
            LOCK_STALE_SECONDS=_int("VIDEO_WORKER_LOCK_STALE_SECONDS", "3600"),

            HEARTBEAT_INTERVAL_SECONDS=_int("VIDEO_WORKER_HEARTBEAT_INTERVAL", "20"),

            FFMPEG_BIN=os.environ.get("FFMPEG_BIN", "ffmpeg"),
            FFPROBE_BIN=os.environ.get("FFPROBE_BIN", "ffprobe"),
            FFPROBE_TIMEOUT_SECONDS=_int("FFPROBE_TIMEOUT_SECONDS", "60"),
            FFMPEG_TIMEOUT_SECONDS=_int("FFMPEG_TIMEOUT_SECONDS", "3600"),

            HLS_TIME_SECONDS=_int("HLS_TIME_SECONDS", "4"),
            THUMBNAIL_AT_SECONDS=_float("THUMBNAIL_AT_SECONDS", "1.0"),
            MIN_SEGMENTS_PER_VARIANT=_int("MIN_SEGMENTS_PER_VARIANT", "1"),

            R2_BUCKET=_require_any("R2_BUCKET", "R2_VIDEO_BUCKET"),
            R2_PREFIX=os.environ.get("R2_PREFIX", "media/hls/videos"),
            R2_ENDPOINT=_require("R2_ENDPOINT"),
            R2_ACCESS_KEY=_require("R2_ACCESS_KEY"),
            R2_SECRET_KEY=_require("R2_SECRET_KEY"),
            R2_REGION=os.environ.get("R2_REGION", "auto"),
            UPLOAD_MAX_CONCURRENCY=_int("UPLOAD_MAX_CONCURRENCY", "8"),

            DOWNLOAD_TIMEOUT_SECONDS=_float("DOWNLOAD_TIMEOUT_SECONDS", "30.0"),
            DOWNLOAD_CHUNK_BYTES=_int("DOWNLOAD_CHUNK_BYTES", str(1024 * 1024)),
        )
    except Exception as e:
        import logging
        logging.basicConfig(level=logging.INFO)
        logging.getLogger(__name__).critical("config error: %s", e)
        sys.exit(1)


==========================================================================================
# FILE: video_worker/download.py
==========================================================================================
from __future__ import annotations

import logging
from pathlib import Path

import requests

from apps.worker.video_worker.config import Config
from apps.worker.video_worker.utils import backoff_sleep, ensure_dir, trim_tail

logger = logging.getLogger("video_worker")


class DownloadError(RuntimeError):
    pass


def download_to_file(*, url: str, dst: Path, cfg: Config) -> None:
    """
    안정적 다운로드:
    - stream chunk
    - retry with backoff
    - tmp(.part) -> atomic rename
    """
    ensure_dir(dst.parent)

    attempt = 0
    while True:
        try:
            with requests.get(url, stream=True, timeout=cfg.DOWNLOAD_TIMEOUT_SECONDS) as r:
                r.raise_for_status()

                tmp = dst.with_suffix(dst.suffix + ".part")
                bytes_written = 0

                with open(tmp, "wb") as f:
                    for chunk in r.iter_content(chunk_size=cfg.DOWNLOAD_CHUNK_BYTES):
                        if chunk:
                            f.write(chunk)
                            bytes_written += len(chunk)

                if bytes_written <= 0:
                    raise DownloadError("downloaded file is empty")

                tmp.replace(dst)
                return

        except Exception as e:
            attempt += 1
            if attempt >= cfg.RETRY_MAX_ATTEMPTS:
                raise DownloadError(f"download failed: {trim_tail(str(e))}") from e
            logger.warning("download retry attempt=%s err=%s", attempt, e)
            backoff_sleep(attempt, cfg.BACKOFF_BASE_SECONDS, cfg.BACKOFF_CAP_SECONDS)


==========================================================================================
# FILE: video_worker/sqs_main.py
==========================================================================================
"""
Video Worker - SQS 기반 메인 엔트리포인트

기존 HTTP polling 방식에서 SQS Long Polling으로 전환
"""

from __future__ import annotations

import os
import sys

# Django 설정이 있으면 앱 로드 — 아래 import들이 Django 모델을 쓰므로 setup을 먼저 호출
if os.environ.get("DJANGO_SETTINGS_MODULE"):
    import django
    django.setup()

import json
import logging
import signal
import time
import uuid
from typing import Optional

import boto3
import requests

from apps.worker.video_worker.config import load_config
from libs.queue import QueueUnavailableError
from src.infrastructure.video import VideoSQSAdapter
from src.infrastructure.video.processor import process_video
from academy.adapters.db.django.repositories_video import DjangoVideoRepository
from src.infrastructure.cache.redis_idempotency_adapter import RedisIdempotencyAdapter
from src.infrastructure.cache.redis_progress_adapter import RedisProgressAdapter
from src.application.video.handler import ProcessVideoJobHandler

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] [VIDEO-WORKER-SQS] %(message)s",
)
logger = logging.getLogger("video_worker_sqs")

_shutdown = False
_current_job_receipt_handle: Optional[str] = None  # Graceful shutdown: 현재 처리 중인 작업 추적
_current_job_start_time: Optional[float] = None  # 로그 가시성: 작업 시작 시간

# SQS Long Polling 설정
SQS_WAIT_TIME_SECONDS = 20  # 최대 대기 시간 (Long Polling)
# 작업 시작 시 ChangeMessageVisibility로 연장 (3시간 영상 등 장시간 대비)
VIDEO_VISIBILITY_EXTEND_SECONDS = int(os.getenv("VIDEO_SQS_VISIBILITY_EXTEND", "10800"))  # 3시간
SQS_VISIBILITY_TIMEOUT = 300  # 로그 비교용 (실제는 VIDEO_VISIBILITY_EXTEND_SECONDS 사용)

# EC2 Self-Stop 설정 (비용 최적화)
IDLE_STOP_THRESHOLD = int(os.getenv("EC2_IDLE_STOP_THRESHOLD", "5"))  # 연속 빈 폴링 5회 = 100초


def _handle_signal(sig, frame):
    """
    Graceful shutdown 핸들러
    
    50명 원장 확장 대비: 현재 처리 중인 작업 완료 후 종료
    """
    global _shutdown, _current_job_receipt_handle
    signal_name = signal.Signals(sig).name
    logger.info(
        "Received %s, initiating graceful shutdown... | current_job=%s",
        signal_name,
        "processing" if _current_job_receipt_handle else "idle",
    )
    _shutdown = True
    # 현재 작업이 있으면 완료될 때까지 대기 (메인 루프에서 처리)


def _stop_self_ec2() -> None:
    """
    SQS 큐가 연속으로 비어있을 때 EC2 인스턴스 자동 종료
    
    비용 최적화: idle 상태 인스턴스 자동 종료로 월 $30-50 절감
    IMDSv2를 사용하여 안전하게 인스턴스 메타데이터 조회
    """
    try:
        # EC2 메타데이터에서 인스턴스 정보 가져오기 (IMDSv2)
        token = requests.put(
            "http://169.254.169.254/latest/api/token",
            headers={"X-aws-ec2-metadata-token-ttl-seconds": "21600"},
            timeout=2,
        ).text
        
        headers = {"X-aws-ec2-metadata-token": token}
        instance_id = requests.get(
            "http://169.254.169.254/latest/meta-data/instance-id",
            headers=headers,
            timeout=2,
        ).text
        
        region = requests.get(
            "http://169.254.169.254/latest/meta-data/placement/region",
            headers=headers,
            timeout=2,
        ).text
        
        ec2 = boto3.client("ec2", region_name=region)
        ec2.stop_instances(InstanceIds=[instance_id])
        
        logger.info("EC2 instance stopped due to idle queues: instance_id=%s (video worker)", instance_id)
        
    except Exception as e:
        logger.exception("EC2 self-stop failed (ignored): %s", e)


def main() -> int:
    """
    SQS 기반 Video Worker 메인 루프
    
    Flow:
    1. SQS에서 메시지 Long Polling
    2. 메시지 수신 시 비디오 처리
    3. 성공 시 메시지 삭제
    4. 실패 시 메시지는 SQS가 자동으로 재시도 (DLQ로 전송 전까지)
    """
    signal.signal(signal.SIGTERM, _handle_signal)
    signal.signal(signal.SIGINT, _handle_signal)
    
    cfg = load_config()
    queue = VideoSQSAdapter()
    repo = DjangoVideoRepository()
    idempotency = RedisIdempotencyAdapter()
    progress = RedisProgressAdapter()
    handler = ProcessVideoJobHandler(
        repo=repo,
        idempotency=idempotency,
        progress=progress,
        process_fn=process_video,
    )

    logger.info(
        "Video Worker (SQS) started | queue=%s | wait_time=%ss",
        queue._get_queue_name(),
        SQS_WAIT_TIME_SECONDS,
    )
    
    consecutive_errors = 0
    max_consecutive_errors = 10
    consecutive_empty_polls = 0  # 비용 최적화: 빈 폴링 카운터
    
    try:
        while not _shutdown:
            try:
                # SQS Long Polling으로 메시지 수신
                try:
                    message = queue.receive_message(wait_time_seconds=SQS_WAIT_TIME_SECONDS)
                except QueueUnavailableError as e:
                    # 로컬 등 AWS 자격 증명 없을 때: 로그 한 번, 60초 대기 후 재시도 (empty로 세지 않음 → EC2 종료 안 함)
                    logger.warning(
                        "SQS unavailable (AWS credentials invalid or missing?). Waiting 60s before retry. %s",
                        e,
                    )
                    time.sleep(60)
                    continue

                if not message:
                    consecutive_empty_polls += 1
                    consecutive_errors = 0

                    # 연속 빈 폴링이 임계값을 초과하면 EC2 인스턴스 종료 (실제 큐가 비었을 때만)
                    if consecutive_empty_polls >= IDLE_STOP_THRESHOLD:
                        logger.info(
                            "Queue empty for %d consecutive polls (threshold=%d), stopping EC2 instance in 10s",
                            consecutive_empty_polls,
                            IDLE_STOP_THRESHOLD,
                        )
                        time.sleep(10)  # 500 plan: Dead zone 완화를 위해 Stop 직전 대기
                        logger.info("EC2 self-stop initiating (video worker)")
                        _stop_self_ec2()
                        return 0

                    continue

                # 메시지가 있으면 카운터 리셋
                consecutive_empty_polls = 0
                
                receipt_handle = message.get("receipt_handle")
                if not receipt_handle:
                    logger.error("Message missing receipt_handle: %s", message)
                    continue

                # ----- R2 삭제 작업 (비동기 삭제) -----
                if message.get("action") == "delete_r2":
                    video_id = message.get("video_id")
                    file_key = (message.get("file_key") or "").strip()
                    hls_prefix = (message.get("hls_prefix") or "").strip()
                    # delete_r2 전용 visibility 900초. 장시간 삭제 시 배치마다 재연장.
                    DELETE_R2_VISIBILITY = 900
                    queue.change_message_visibility(receipt_handle, DELETE_R2_VISIBILITY)
                    # action별 멱등: 삭제 중복 처리 방지
                    if not idempotency.acquire_lock(f"delete_r2:{video_id}"):
                        logger.info("R2 delete skip (lock) video_id=%s", video_id)
                        queue.delete_message(receipt_handle)
                        continue
                    try:
                        from apps.infrastructure.storage.r2 import delete_object_r2_video, delete_prefix_r2_video
                        if file_key:
                            delete_object_r2_video(key=file_key)
                            logger.info("R2 raw deleted video_id=%s key=%s", video_id, file_key)
                        if hls_prefix:
                            def _extend_visibility(_):
                                queue.change_message_visibility(receipt_handle, DELETE_R2_VISIBILITY)
                            n = delete_prefix_r2_video(
                                prefix=hls_prefix,
                                on_batch_deleted=_extend_visibility,
                            )
                            logger.info("R2 HLS prefix deleted video_id=%s prefix=%s count=%d", video_id, hls_prefix, n)
                    except Exception as e:
                        logger.exception("R2 delete job failed video_id=%s: %s", video_id, e)
                    finally:
                        idempotency.release_lock(f"delete_r2:{video_id}")
                    queue.delete_message(receipt_handle)
                    continue

                # ----- 인코딩 작업 -----
                video_id = message.get("video_id")
                file_key = message.get("file_key")
                tenant_id = message.get("tenant_id")
                tenant_code = message.get("tenant_code")
                message_created_at = message.get("created_at")

                if not video_id or tenant_id is None:
                    logger.error("Invalid message format (video_id, tenant_id required): %s", message)
                    queue.delete_message(receipt_handle)
                    continue

                # Retry로 이미 완료된 영상이면 visibility 연장 없이 메시지만 삭제 (중복·3시간 묶임 방지)
                from academy.adapters.db.django.repositories_video import get_video_status
                if get_video_status(video_id) == "READY":
                    queue.delete_message(receipt_handle)
                    logger.info("VIDEO_ALREADY_READY_SKIP | video_id=%s (retry 등으로 이미 완료)", video_id)
                    continue

                # 장시간 인코딩 시 재노출 방지 (작업 시작 직후 visibility 연장)
                queue.change_message_visibility(receipt_handle, VIDEO_VISIBILITY_EXTEND_SECONDS)

                request_id = str(uuid.uuid4())[:8]
                message_received_at = time.time()
                queue_wait_time = message_received_at - (float(message_created_at) if message_created_at else message_received_at)

                logger.info(
                    "SQS_MESSAGE_RECEIVED | request_id=%s | video_id=%s | tenant_id=%s | queue_wait_sec=%.2f | created_at=%s",
                    request_id,
                    video_id,
                    tenant_id,
                    queue_wait_time,
                    message_created_at or "unknown",
                )

                global _current_job_receipt_handle, _current_job_start_time
                _current_job_receipt_handle = receipt_handle
                _current_job_start_time = time.time()

                job = {
                    "video_id": int(video_id),
                    "file_key": str(file_key or ""),
                    "tenant_id": int(tenant_id),
                    "tenant_code": str(tenant_code or ""),
                }

                try:
                    result = handler.handle(job, cfg)
                except Exception as e:
                    # handler.handle() 예외 시에도 반드시 즉시 재노출 (3시간 묶임 방지)
                    queue.change_message_visibility(receipt_handle, 0)
                    logger.exception("Handler exception (visibility 0 applied): video_id=%s: %s", video_id, e)
                    consecutive_errors += 1
                    _current_job_receipt_handle = None
                    _current_job_start_time = None
                    if consecutive_errors >= max_consecutive_errors:
                        logger.error("Too many consecutive errors (%s), shutting down", consecutive_errors)
                        return 1
                    time.sleep(5)
                    continue

                processing_duration = time.time() - _current_job_start_time
                _current_job_receipt_handle = None
                _current_job_start_time = None

                if result == "ok":
                    # 인코딩 성공 → HLS 업로드·DB 완료 후 raw 삭제 (실패해도 DB 롤백 안 함, 재시도만)
                    file_key_for_raw = job.get("file_key") or ""
                    if file_key_for_raw.strip():
                        from apps.infrastructure.storage.r2 import delete_object_r2_video
                        for attempt in range(3):
                            try:
                                delete_object_r2_video(key=file_key_for_raw.strip())
                                logger.info("R2 raw deleted after encode video_id=%s key=%s", video_id, file_key_for_raw[:80])
                                break
                            except Exception as e:
                                logger.warning(
                                    "R2 raw delete after encode failed video_id=%s attempt=%s: %s",
                                    video_id, attempt + 1, e,
                                )
                                if attempt < 2:
                                    time.sleep(2**attempt)  # 1s → 2s → 4s exponential backoff
                    queue.delete_message(receipt_handle)
                    logger.info(
                        "SQS_JOB_COMPLETED | request_id=%s | video_id=%s | tenant_code=%s | processing_duration=%.2f | queue_wait_sec=%.2f",
                        request_id,
                        video_id,
                        tenant_code,
                        processing_duration,
                        queue_wait_time,
                    )
                    # 평균 인코딩 시간 모니터링용 (로그 파싱·메트릭 수집)
                    logger.info(
                        "VIDEO_ENCODING_DURATION | video_id=%s | duration_sec=%.2f",
                        video_id,
                        processing_duration,
                    )
                    consecutive_errors = 0

                    if _shutdown:
                        logger.info("Graceful shutdown: current job completed, exiting")
                        break

                elif result == "skip":
                    queue.delete_message(receipt_handle)
                    consecutive_errors = 0

                else:
                    # handler 실패 시 즉시 재노출 → 다른 워커가 곧바로 처리 (3시간 묶임 방지)
                    queue.change_message_visibility(receipt_handle, 0)
                    logger.exception(
                        "SQS_JOB_FAILED | request_id=%s | video_id=%s | tenant_code=%s | processing_duration=%.2f | queue_wait_sec=%.2f",
                        request_id,
                        video_id,
                        tenant_code,
                        processing_duration,
                        queue_wait_time,
                    )
                    if processing_duration > SQS_VISIBILITY_TIMEOUT:
                        logger.warning(
                            "SQS_VISIBILITY_TIMEOUT_EXCEEDED | request_id=%s | video_id=%s | processing_duration=%.2f | visibility_timeout=%d",
                            request_id,
                            video_id,
                            processing_duration,
                            SQS_VISIBILITY_TIMEOUT,
                        )
                    consecutive_errors += 1

                    if consecutive_errors >= max_consecutive_errors:
                        logger.error(
                            "Too many consecutive errors (%s), shutting down",
                            consecutive_errors,
                        )
                        return 1
                
            except KeyboardInterrupt:
                logger.info("Keyboard interrupt received")
                break
            except Exception as e:
                # 예외 발생 시에도 visibility 0 시도 (이미 delete된 메시지면 API 오류는 무시)
                try:
                    queue.change_message_visibility(receipt_handle, 0)
                except Exception:
                    pass
                logger.exception("Unexpected error in main loop: %s", e)
                consecutive_errors += 1
                if consecutive_errors >= max_consecutive_errors:
                    logger.error(
                        "Too many consecutive errors (%s), shutting down",
                        consecutive_errors,
                    )
                    return 1
                time.sleep(5)
        
        # Graceful shutdown: 현재 작업이 있으면 완료 대기
        if _current_job_receipt_handle:
            logger.info(
                "Graceful shutdown: waiting for current job to complete | receipt_handle=%s",
                _current_job_receipt_handle[:20] + "...",
            )
        
        logger.info("Video Worker shutdown complete")
        return 0
        
    except Exception:
        logger.exception("Fatal error in Video Worker")
        return 1


if __name__ == "__main__":
    sys.exit(main())


==========================================================================================
# FILE: video_worker/utils.py
==========================================================================================
from __future__ import annotations

import logging
import random
import shutil
import tempfile
import time
from contextlib import contextmanager
from pathlib import Path

logger = logging.getLogger("video_worker")


@contextmanager
def temp_workdir(base_dir: str, prefix: str):
    """
    Temporary working directory context manager.
    
    Ensures cleanup on exit (success or failure).
    Logs errors if cleanup fails.
    """
    Path(base_dir).mkdir(parents=True, exist_ok=True)
    path = Path(tempfile.mkdtemp(prefix=prefix, dir=base_dir))
    try:
        yield path
    finally:
        try:
            if path.exists():
                shutil.rmtree(path, ignore_errors=False)
                logger.debug("Cleaned up temp dir: %s", path)
        except Exception as e:
            logger.error("Failed to cleanup temp dir: %s, error: %s", path, e)
            # Don't re-raise - allow processing to continue, but log error for monitoring


def ensure_dir(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


def backoff_sleep(attempt: int, base: float, cap: float) -> None:
    raw = min(cap, base * (2 ** attempt))
    jitter = random.uniform(0.5, 1.5)
    time.sleep(raw * jitter)


def trim_tail(s: str, limit: int = 2000) -> str:
    if not s:
        return ""
    return s[-limit:] if len(s) > limit else s


def guess_content_type(name: str) -> str:
    n = name.lower()
    if n.endswith(".m3u8"):
        return "application/vnd.apple.mpegurl"
    if n.endswith(".ts"):
        return "video/MP2T"
    if n.endswith(".mp4"):
        return "video/mp4"
    if n.endswith(".jpg") or n.endswith(".jpeg"):
        return "image/jpeg"
    if n.endswith(".png"):
        return "image/png"
    if n.endswith(".json"):
        return "application/json"
    return "application/octet-stream"


def cache_control_for_object(name: str) -> str:
    """
    R2 Cache-Control 전략 (요구사항 반영)

    - HLS playlist (.m3u8): 서명 정책/쿠키 기반 접근을 전제로 "no-cache"
      (플레이리스트는 재생 정책/토큰 갱신 영향 받음)
    - Segment (.ts): immutable (콘텐츠 주소가 prefix/video_id 고정이라도,
      세그먼트는 VOD 생성 후 변경되지 않는 것이 정상)
    - Thumbnail: 7d 캐시
    """
    n = name.lower()
    if n.endswith(".m3u8"):
        return "no-cache"
    if n.endswith(".ts"):
        return "public, max-age=31536000, immutable"
    if n.endswith(".jpg") or n.endswith(".jpeg") or n.endswith(".png"):
        return "public, max-age=604800"
    return "public, max-age=3600"


==========================================================================================
# FILE: video_worker/video/duration.py
==========================================================================================
# PATH: apps/worker/video_worker/video/duration.py
#
# PURPOSE:
# - 로컬 영상 파일에서 ffprobe로 duration(초) 추출
# - 실패해도 worker 전체 작업을 fail 시키지 않음 (best-effort)

from __future__ import annotations

import subprocess
from typing import Optional


class DurationProbeError(RuntimeError):
    pass


def probe_duration_seconds(
    *,
    input_path: str,
    ffprobe_bin: str,
    timeout: int,
) -> Optional[int]:
    if not input_path:
        return None

    cmd = [
        ffprobe_bin,
        "-v", "error",
        "-show_entries", "format=duration",
        "-of", "default=noprint_wrappers=1:nokey=1",
        input_path,
    ]

    try:
        p = subprocess.run(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=timeout,
            check=False,
        )
    except Exception:
        return None

    if p.returncode != 0:
        return None

    raw = (p.stdout or "").strip()
    if not raw:
        return None

    try:
        sec = float(raw)
        if sec < 0:
            return None
        return int(sec)
    except Exception:
        return None


==========================================================================================
# FILE: video_worker/video/r2_uploader.py
==========================================================================================
from __future__ import annotations

import os
from pathlib import Path

import boto3
from boto3.s3.transfer import TransferConfig

from apps.worker.video_worker.utils import guess_content_type, cache_control_for_object, trim_tail, backoff_sleep


class UploadError(RuntimeError):
    pass


def upload_directory(
    *,
    local_dir: Path,
    bucket: str,
    prefix: str,
    endpoint_url: str,
    access_key: str,
    secret_key: str,
    region: str,
    max_concurrency: int,
    retry_max: int = 5,
    backoff_base: float = 0.5,
    backoff_cap: float = 10.0,
) -> None:
    """
    업로드 정책 (요구사항 반영):
    - Content-Type 정확히
    - Cache-Control 전략 포함
      - .m3u8 : no-cache
      - .ts   : public, max-age=31536000, immutable
      - thumb : 7d
    - 부분 업로드 방지:
      - boto3 multipart 실패 시 예외 / retry
      - 동일 Key에 overwrite는 허용 (idempotent)
    """
    s3 = boto3.client(
        "s3",
        endpoint_url=endpoint_url,
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key,
        region_name=region,
    )

    transfer_cfg = TransferConfig(
        max_concurrency=max_concurrency,
        multipart_threshold=8 * 1024 * 1024,
        multipart_chunksize=8 * 1024 * 1024,
        use_threads=True,
    )

    local_dir = local_dir.resolve()

    for root, _, files in os.walk(local_dir):
        for name in files:
            full_path = Path(root) / name
            rel = full_path.relative_to(local_dir)
            key = f"{prefix.rstrip('/')}/{rel.as_posix()}"

            extra = {
                "ContentType": guess_content_type(name),
                "CacheControl": cache_control_for_object(name),
            }

            attempt = 0
            while True:
                try:
                    s3.upload_file(
                        Filename=str(full_path),
                        Bucket=bucket,
                        Key=key,
                        ExtraArgs=extra,
                        Config=transfer_cfg,
                    )
                    break
                except Exception as e:
                    attempt += 1
                    if attempt >= retry_max:
                        raise UploadError(f"upload failed key={key} err={trim_tail(str(e))}") from e
                    backoff_sleep(attempt, backoff_base, backoff_cap)


==========================================================================================
# FILE: video_worker/video/thumbnail.py
==========================================================================================
from __future__ import annotations

import subprocess
from pathlib import Path

from apps.worker.video_worker.utils import ensure_dir, trim_tail


class ThumbnailError(RuntimeError):
    pass


def generate_thumbnail(
    *,
    input_path: str,
    output_path: Path,
    ffmpeg_bin: str,
    at_seconds: float,
    timeout: int,
) -> None:
    ensure_dir(output_path.parent)

    cmd = [
        ffmpeg_bin,
        "-y",
        "-ss", f"{at_seconds:.3f}",
        "-i", input_path,
        "-frames:v", "1",
        "-q:v", "2",
        str(output_path),
    ]

    try:
        p = subprocess.run(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=timeout,
            check=False,
        )
    except subprocess.TimeoutExpired as e:
        raise ThumbnailError(f"thumbnail timeout ({timeout}s)") from e

    if p.returncode != 0:
        raise ThumbnailError(f"thumbnail ffmpeg failed: {trim_tail(p.stderr)}")


==========================================================================================
# FILE: video_worker/video/transcoder.py
==========================================================================================
# PATH: apps/worker/video_worker/video/transcoder.py

from __future__ import annotations

import json
import subprocess
from pathlib import Path
from typing import List, Optional

from apps.worker.video_worker.utils import ensure_dir, trim_tail

# preset 유지 (순서 중요)
HLS_VARIANTS = [
    {"name": "1", "width": 426, "height": 240, "video_bitrate": "400k", "audio_bitrate": "64k"},
    {"name": "2", "width": 640, "height": 360, "video_bitrate": "800k", "audio_bitrate": "96k"},
    {"name": "3", "width": 1280, "height": 720, "video_bitrate": "2500k", "audio_bitrate": "128k"},
]


class TranscodeError(RuntimeError):
    pass


def _probe_resolution(input_path: str, ffprobe_bin: str, timeout: int) -> tuple[int, int]:
    cmd = [
        ffprobe_bin,
        "-v", "error",
        "-select_streams", "v:0",
        "-show_entries", "stream=width,height",
        "-of", "json",
        input_path,
    ]
    try:
        p = subprocess.run(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=timeout,
            check=False,
        )
    except subprocess.TimeoutExpired:
        return 0, 0

    if p.returncode != 0:
        return 0, 0

    try:
        data = json.loads(p.stdout)
        s = (data.get("streams") or [{}])[0]
        return int(s.get("width") or 0), int(s.get("height") or 0)
    except Exception:
        return 0, 0


def _select_variants(input_w: int, input_h: int) -> List[dict]:
    """
    입력 해상도 상한 반영:
    - 원본보다 큰 variant는 제외
    """
    selected = []
    for v in HLS_VARIANTS:
        if v["width"] <= input_w and v["height"] <= input_h:
            selected.append(v)
    # 안전장치: 최소 1개
    if not selected:
        selected.append(HLS_VARIANTS[0])
    return selected


def prepare_output_dirs(output_root: Path, variants: List[dict]) -> None:
    ensure_dir(output_root)
    for v in variants:
        ensure_dir(output_root / f"v{v['name']}")


def has_audio_stream(*, input_path: str, ffprobe_bin: str, timeout: int) -> bool:
    cmd = [
        ffprobe_bin,
        "-v", "error",
        "-print_format", "json",
        "-show_streams",
        input_path,
    ]
    try:
        p = subprocess.run(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=timeout,
            check=False,
        )
    except subprocess.TimeoutExpired:
        return False

    if p.returncode != 0:
        return False

    try:
        data = json.loads(p.stdout)
        streams = data.get("streams") or []
        return any(s.get("codec_type") == "audio" for s in streams)
    except Exception:
        return False


def build_filter_complex(variants: List[dict]) -> str:
    parts: List[str] = []
    split_count = len(variants)
    parts.append("[0:v]split={}".format(split_count) + "".join(f"[v{i}]" for i in range(split_count)))
    for i, v in enumerate(variants):
        parts.append(f"[v{i}]scale={v['width']}:{v['height']}[v{i}out]")
    return ";".join(parts)


def build_ffmpeg_command(
    *,
    input_path: str,
    variants: List[dict],
    with_audio: bool,
    ffmpeg_bin: str,
    hls_time: int,
) -> List[str]:
    cmd: List[str] = [
        ffmpeg_bin,
        "-y",
        "-i", input_path,
        "-filter_complex", build_filter_complex(variants),
    ]

    for i, v in enumerate(variants):
        cmd += ["-map", f"[v{i}out]"]
        if with_audio:
            cmd += ["-map", "0:a?"]

        cmd += [
            f"-c:v:{i}", "libx264",
            "-profile:v", "main",
            "-pix_fmt", "yuv420p",
            f"-b:v:{i}", v["video_bitrate"],
            "-g", "48",
            "-keyint_min", "48",
            "-sc_threshold", "0",
        ]

        if with_audio:
            cmd += [
                f"-c:a:{i}", "aac",
                "-ac", "2",
                f"-b:a:{i}", v["audio_bitrate"],
            ]

    if with_audio:
        var_map = " ".join(f"v:{i},a:{i},name:{v['name']}" for i, v in enumerate(variants))
    else:
        var_map = " ".join(f"v:{i},name:{v['name']}" for i, v in enumerate(variants))

    cmd += [
        "-f", "hls",
        "-hls_time", str(hls_time),
        "-hls_playlist_type", "vod",
        "-hls_flags", "independent_segments",
        "-hls_segment_filename", "v%v/index%d.ts",
        "-master_pl_name", "master.m3u8",
        "-var_stream_map", var_map,
        "v%v/index.m3u8",
    ]
    return cmd


def transcode_to_hls(
    *,
    video_id: int,
    input_path: str,
    output_root: Path,
    ffmpeg_bin: str,
    ffprobe_bin: str,
    hls_time: int,
    timeout: Optional[int],
) -> Path:
    # 입력 해상도 기반 variant 선택
    w, h = _probe_resolution(input_path, ffprobe_bin, min(60, int(timeout or 60)))
    variants = _select_variants(w, h)

    prepare_output_dirs(output_root, variants)

    with_audio = has_audio_stream(
        input_path=input_path,
        ffprobe_bin=ffprobe_bin,
        timeout=min(60, int(timeout or 60)),
    )

    cmd = build_ffmpeg_command(
        input_path=input_path,
        variants=variants,
        with_audio=with_audio,
        ffmpeg_bin=ffmpeg_bin,
        hls_time=hls_time,
    )

    try:
        p = subprocess.run(
            cmd,
            cwd=str(output_root.resolve()),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=timeout,
            check=False,
        )
    except subprocess.TimeoutExpired as e:
        raise TranscodeError(f"ffmpeg timeout video_id={video_id} seconds={timeout}") from e

    if p.returncode != 0:
        raise TranscodeError(
            f"ffmpeg failed video_id={video_id} with_audio={with_audio} stderr={trim_tail(p.stderr)}"
        )

    master = output_root / "master.m3u8"
    if not master.exists():
        raise TranscodeError("master.m3u8 not created")

    return master


==========================================================================================
# FILE: video_worker/video/validate.py
==========================================================================================
# PATH: apps/worker/video_worker/video/validate.py

from __future__ import annotations

from pathlib import Path


def validate_hls_output(root: Path, min_segments: int) -> None:
    """
    인코딩 결과 깨짐 자동 fail 처리용 검증:
    - master.m3u8 존재
    - 각 variant playlist 존재
    - 각 variant에 최소 세그먼트 수 확보

    상품 레벨 보정:
    - 짧은 영상(총 길이 < min_segments * HLS_TIME)의 경우
      ffmpeg 정상 동작에서도 세그먼트 수가 min보다 작을 수 있음
    - 따라서 "0개"만 실패로 간주하고, 1개 이상이면 정상 처리
    """
    master = root / "master.m3u8"
    if not master.exists():
        raise RuntimeError("master.m3u8 missing")

    variants = list(root.glob("v*/index.m3u8"))
    if not variants:
        raise RuntimeError("no variant playlists (v*/index.m3u8)")

    for v in variants:
        segs = list(v.parent.glob("*.ts"))

        # 기존: 고정 min_segments 강제
        # if len(segs) < int(min_segments):
        #     raise RuntimeError(f"HLS validation failed: {v} segments={len(segs)} min={min_segments}")

        # MODIFIED: 짧은 영상 허용 (세그먼트 1개 이상이면 정상)
        if len(segs) <= 0:  # MODIFIED
            raise RuntimeError(  # MODIFIED
                f"HLS validation failed: {v} segments={len(segs)} min=1"  # MODIFIED
            )
