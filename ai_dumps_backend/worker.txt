====================================================================================================
# BACKEND APP: worker
# ROOT PATH: C:\academy\apps\worker
====================================================================================================


==========================================================================================
# FILE: __init__.py
==========================================================================================
#apps/worker/__init__.py

#ÏïÑÎ¨¥ÎÇ¥Ïö©ÏóÜÏùå.


==========================================================================================
# FILE: ai_worker/__init__.py
==========================================================================================



==========================================================================================
# FILE: ai_worker/celery.py
==========================================================================================
# apps/worker/ai_worker/celery.py

from celery import Celery

app = Celery("academy_ai")

app.conf.broker_url = "redis://172.31.32.109:6379/0"
app.conf.result_backend = "redis://172.31.32.109:6379/0"

# ü§ñ AI Ï†ÑÏö©
app.autodiscover_tasks([
    "apps.worker.ai_worker.ai",
])

print("ü§ñ AI Celery READY ü§ñ")


==========================================================================================
# FILE: ai_worker/run.py
==========================================================================================
# apps/worker/run.py
from __future__ import annotations

import os
import sys
import time
import signal
import logging
import requests

from apps.shared.contracts.ai_job import AIJob
from apps.shared.contracts.ai_result import AIResult
from apps.worker.ai_worker.ai.pipelines.dispatcher import handle_ai_job

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] [AI-WORKER] %(message)s",
)
logger = logging.getLogger(__name__)

API_BASE_URL = os.getenv("API_BASE_URL", "http://localhost:8000")
INTERNAL_WORKER_TOKEN = os.getenv("INTERNAL_WORKER_TOKEN", "long-random-secret")
POLL_INTERVAL_SEC = float(os.getenv("AI_WORKER_POLL_INTERVAL", "1.0"))

_running = True


def _shutdown(signum, frame):
    global _running
    logger.warning("Shutdown signal received (%s)", signum)
    _running = False


signal.signal(signal.SIGINT, _shutdown)
signal.signal(signal.SIGTERM, _shutdown)


def fetch_job() -> AIJob | None:
    """
    API ‚Üí Worker
    GET /api/v1/internal/ai/job/next/
    response: { "job": {...} | null }
    """
    url = f"{API_BASE_URL}/api/v1/internal/ai/job/next/"
    headers = {"X-Worker-Token": INTERNAL_WORKER_TOKEN}

    resp = requests.get(url, headers=headers, timeout=10)
    resp.raise_for_status()

    data = resp.json()
    job_data = data.get("job")
    if not job_data:
        return None

    return AIJob.from_dict(job_data)


def submit_result(result: AIResult, submission_id: int) -> None:
    """
    Worker ‚Üí API
    POST /api/v1/internal/ai/job/result/
    """
    url = f"{API_BASE_URL}/api/v1/internal/ai/job/result/"
    headers = {
        "X-Worker-Token": INTERNAL_WORKER_TOKEN,
        "Content-Type": "application/json",
    }

    # ‚úÖ Íµ¨Ï°∞ Í≥†Ï†ï (Ï§ëÏöî)
    payload = {
        "submission_id": submission_id,
        "status": result.status,
        "result": result.result,
        "error": result.error,
    }

    resp = requests.post(
        url,
        json=payload,
        headers=headers,
        timeout=20,
    )
    resp.raise_for_status()


def main():
    logger.info("AI Worker started")

    while _running:
        try:
            job = fetch_job()
            if job is None:
                time.sleep(POLL_INTERVAL_SEC)
                continue

            logger.info("Job received: id=%s type=%s", job.id, job.type)

            # üî• AI Ï≤òÎ¶¨
            result = handle_ai_job(job)

            # üî• Í≤∞Í≥º Ï†ÑÏÜ° (submission_id = job.source_id)
            submit_result(result, int(job.source_id))

            logger.info(
                "Job finished: id=%s status=%s",
                job.id,
                result.status,
            )

        except Exception:
            logger.exception("Worker loop error")
            time.sleep(2.0)

    logger.info("AI Worker shutdown complete")


if __name__ == "__main__":
    sys.exit(main())


==========================================================================================
# FILE: ai_worker/ai/__init__.py
==========================================================================================
# apps/worker/ai/__init__.py
# Celery Ï†úÍ±∞Îê®. run.py Îã®Ïùº ÏßÑÏûÖ


==========================================================================================
# FILE: ai_worker/ai/config.py
==========================================================================================
# apps/worker/ai/config.py
from __future__ import annotations

import os
from dataclasses import dataclass
from typing import List, Optional


def _env(name: str, default: Optional[str] = None) -> Optional[str]:
    v = os.getenv(name)
    return v if v not in (None, "") else default


@dataclass(frozen=True)
class AIConfig:
    # OCR
    OCR_ENGINE: str = "google"  # google | tesseract | auto
    GOOGLE_APPLICATION_CREDENTIALS: Optional[str] = None  # optional (google sdk default)

    # Segmentation
    QUESTION_SEGMENTATION_ENGINE: str = "auto"  # yolo|opencv|template|auto

    # YOLO (optional)
    YOLO_QUESTION_MODEL_PATH: Optional[str] = None
    YOLO_QUESTION_INPUT_SIZE: int = 640
    YOLO_QUESTION_CONF_THRESHOLD: float = 0.4
    YOLO_QUESTION_IOU_THRESHOLD: float = 0.5

    # Embedding
    EMBEDDING_BACKEND: str = "auto"  # local|openai|auto
    EMBEDDING_LOCAL_MODEL: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    EMBEDDING_OPENAI_MODEL: str = "text-embedding-3-small"
    OPENAI_API_KEY: Optional[str] = None

    # Problem generation
    PROBLEM_GEN_MODEL: str = "gpt-4.1-mini"  # default

    @staticmethod
    def load() -> "AIConfig":
        return AIConfig(
            OCR_ENGINE=_env("OCR_ENGINE", "google") or "google",
            GOOGLE_APPLICATION_CREDENTIALS=_env("GOOGLE_APPLICATION_CREDENTIALS"),

            QUESTION_SEGMENTATION_ENGINE=_env("QUESTION_SEGMENTATION_ENGINE", "auto") or "auto",

            YOLO_QUESTION_MODEL_PATH=_env("YOLO_QUESTION_MODEL_PATH"),
            YOLO_QUESTION_INPUT_SIZE=int(_env("YOLO_QUESTION_INPUT_SIZE", "640") or "640"),
            YOLO_QUESTION_CONF_THRESHOLD=float(_env("YOLO_QUESTION_CONF_THRESHOLD", "0.4") or "0.4"),
            YOLO_QUESTION_IOU_THRESHOLD=float(_env("YOLO_QUESTION_IOU_THRESHOLD", "0.5") or "0.5"),

            EMBEDDING_BACKEND=_env("EMBEDDING_BACKEND", "auto") or "auto",
            EMBEDDING_LOCAL_MODEL=_env(
                "EMBEDDING_LOCAL_MODEL",
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            ) or "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            EMBEDDING_OPENAI_MODEL=_env("EMBEDDING_OPENAI_MODEL", "text-embedding-3-small") or "text-embedding-3-small",
            OPENAI_API_KEY=_env("OPENAI_API_KEY") or _env("EMBEDDING_OPENAI_API_KEY"),

            PROBLEM_GEN_MODEL=_env("PROBLEM_GEN_MODEL", "gpt-4.1-mini") or "gpt-4.1-mini",
        )


==========================================================================================
# FILE: ai_worker/ai/detection/__init__.py
==========================================================================================
# apps/worker/ai/detection/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/detection/segment_dispatcher.py
==========================================================================================
# apps/worker/ai/detection/segment_dispatcher.py
from __future__ import annotations

from typing import List, Tuple

from apps.worker.ai_worker.ai.config import AIConfig
from apps.worker.ai_worker.ai.detection.segment_opencv import segment_questions_opencv
from apps.worker.ai_worker.ai.detection.segment_yolo import segment_questions_yolo

BBox = Tuple[int, int, int, int]


def segment_questions(image_path: str) -> List[BBox]:
    """
    worker-side segmentation single entrypoint
    """
    cfg = AIConfig.load()
    engine = (cfg.QUESTION_SEGMENTATION_ENGINE or "auto").lower()

    if engine == "opencv":
        return segment_questions_opencv(image_path)
    if engine == "yolo":
        return segment_questions_yolo(image_path)

    # auto: yolo -> opencv
    try:
        boxes = segment_questions_yolo(image_path)
        if boxes:
            return boxes
    except Exception:
        pass
    return segment_questions_opencv(image_path)


==========================================================================================
# FILE: ai_worker/ai/detection/segment_opencv.py
==========================================================================================
# apps/worker/ai/detection/segment_opencv.py
from __future__ import annotations

from typing import List, Tuple
import cv2  # type: ignore

BBox = Tuple[int, int, int, int]


def segment_questions_opencv(image_path: str) -> List[BBox]:
    """
    legacy ÏÑûÏó¨ÏûàÎçò opencv segmentation Ï†ïÎ¶¨Î≥∏
    ÏûÖÎ†•: image_path
    Ï∂úÎ†•: [(x,y,w,h), ...]
    """
    image_bgr = cv2.imread(image_path)
    if image_bgr is None:
        return []

    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5, 5), 0)

    _, thresh = cv2.threshold(
        blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU
    )

    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))
    dilated = cv2.dilate(thresh, kernel, iterations=1)

    contours, _ = cv2.findContours(
        dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
    )

    h_img, w_img = gray.shape[:2]
    min_area = w_img * h_img * 0.005
    max_area = w_img * h_img * 0.9

    boxes: List[BBox] = []
    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)
        area = w * h
        if area < min_area or area > max_area:
            continue

        aspect = h / (w + 1e-6)
        if aspect < 0.3:
            continue

        boxes.append((x, y, w, h))

    boxes.sort(key=lambda b: (b[1], b[0]))
    return boxes


==========================================================================================
# FILE: ai_worker/ai/detection/segment_yolo.py
==========================================================================================
# apps/worker/ai/detection/segment_yolo.py
from __future__ import annotations

from functools import lru_cache
from typing import List, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore

from apps.worker.ai_worker.ai.config import AIConfig

BBox = Tuple[int, int, int, int]


class YoloNotConfiguredError(RuntimeError):
    pass


try:
    import onnxruntime as ort  # type: ignore
    _HAS_ORT = True
except Exception:
    _HAS_ORT = False


@lru_cache()
def _get_session():
    cfg = AIConfig.load()

    if not _HAS_ORT:
        raise YoloNotConfiguredError("onnxruntime not installed")

    if not cfg.YOLO_QUESTION_MODEL_PATH:
        raise YoloNotConfiguredError("YOLO_QUESTION_MODEL_PATH not set")

    providers = ["CPUExecutionProvider"]
    return ort.InferenceSession(str(cfg.YOLO_QUESTION_MODEL_PATH), providers=providers)


def _preprocess(image_bgr, input_size: int):
    h0, w0 = image_bgr.shape[:2]
    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
    resized = cv2.resize(image_rgb, (input_size, input_size))
    resized = resized.astype(np.float32) / 255.0

    tensor = np.transpose(resized, (2, 0, 1))
    tensor = np.expand_dims(tensor, axis=0)

    scale_x = w0 / float(input_size)
    scale_y = h0 / float(input_size)
    return tensor, scale_x, scale_y


def _nms(boxes, scores, iou_threshold: float):
    if len(boxes) == 0:
        return []

    boxes = boxes.astype(np.float32)
    scores = scores.astype(np.float32)

    x1, y1, x2, y2 = boxes.T
    areas = (x2 - x1) * (y2 - y1)
    order = scores.argsort()[::-1]

    keep = []
    while order.size > 0:
        i = int(order[0])
        keep.append(i)

        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])

        w = np.maximum(0.0, xx2 - xx1)
        h = np.maximum(0.0, yy2 - yy1)
        inter = w * h

        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-6)
        idxs = np.where(iou <= iou_threshold)[0]
        order = order[idxs + 1]

    return keep


def segment_questions_yolo(image_path: str) -> List[BBox]:
    cfg = AIConfig.load()
    sess = _get_session()

    image_bgr = cv2.imread(image_path)
    if image_bgr is None:
        return []

    input_tensor, scale_x, scale_y = _preprocess(image_bgr, cfg.YOLO_QUESTION_INPUT_SIZE)

    input_name = sess.get_inputs()[0].name
    outputs = sess.run(None, {input_name: input_tensor})
    preds = outputs[0]
    if preds.ndim == 3:
        preds = preds[0]

    boxes = []
    scores = []

    for det in preds:
        cx, cy, w, h, obj_conf = det[:5]
        cls_scores = det[5:]
        cls_conf = float(cls_scores.max()) if cls_scores.size > 0 else 1.0

        score = float(obj_conf * cls_conf)
        if score < cfg.YOLO_QUESTION_CONF_THRESHOLD:
            continue

        x1 = (cx - w / 2.0) * scale_x
        y1 = (cy - h / 2.0) * scale_y
        x2 = (cx + w / 2.0) * scale_x
        y2 = (cy + h / 2.0) * scale_y

        boxes.append([x1, y1, x2, y2])
        scores.append(score)

    if not boxes:
        return []

    boxes_np = np.array(boxes)
    scores_np = np.array(scores)

    keep_idx = _nms(boxes_np, scores_np, cfg.YOLO_QUESTION_IOU_THRESHOLD)

    final: List[BBox] = []
    for i in keep_idx:
        x1, y1, x2, y2 = boxes_np[i]
        final.append((int(x1), int(y1), int(x2 - x1), int(y2 - y1)))

    final.sort(key=lambda b: (b[1], b[0]))
    return final


==========================================================================================
# FILE: ai_worker/ai/embedding/__init__.py
==========================================================================================
# apps/worker/ai/embedding/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/embedding/service.py
==========================================================================================
# apps/worker/ai/embedding/service.py
from __future__ import annotations

from dataclasses import dataclass
from typing import List, Optional, Sequence, Literal
import math

from apps.worker.ai_worker.ai.config import AIConfig

EmbeddingBackendName = Literal["local", "openai"]


@dataclass
class EmbeddingBatch:
    vectors: List[List[float]]
    backend: EmbeddingBackendName


def cosine_similarity(a: Sequence[float], b: Sequence[float]) -> float:
    """
    cosine similarity normalized to 0~1
    """
    if not a or not b:
        return 0.0
    if len(a) != len(b):
        return 0.0

    dot = 0.0
    na = 0.0
    nb = 0.0
    for x, y in zip(a, b):
        dot += x * y
        na += x * x
        nb += y * y

    if na <= 0.0 or nb <= 0.0:
        return 0.0

    sim = dot / (math.sqrt(na) * math.sqrt(nb))
    sim = max(-1.0, min(1.0, sim))
    return (sim + 1.0) / 2.0


# -------- local (sentence-transformers) --------
try:
    from sentence_transformers import SentenceTransformer  # type: ignore
except Exception:
    SentenceTransformer = None  # type: ignore

_local_model: Optional["SentenceTransformer"] = None


def _get_local_model() -> "SentenceTransformer":
    global _local_model
    if _local_model is not None:
        return _local_model

    if SentenceTransformer is None:
        raise RuntimeError("sentence-transformers not installed")

    cfg = AIConfig.load()
    _local_model = SentenceTransformer(cfg.EMBEDDING_LOCAL_MODEL)
    return _local_model


def _embed_local(texts: List[str]) -> EmbeddingBatch:
    model = _get_local_model()
    vectors = model.encode(texts, convert_to_numpy=False)
    vectors_list = [list(map(float, v)) for v in vectors]
    return EmbeddingBatch(vectors=vectors_list, backend="local")


# -------- openai --------
try:
    from openai import OpenAI  # type: ignore
except Exception:
    OpenAI = None  # type: ignore

_openai_client: Optional["OpenAI"] = None


def _get_openai_client() -> "OpenAI":
    global _openai_client
    if _openai_client is not None:
        return _openai_client

    if OpenAI is None:
        raise RuntimeError("openai not installed")

    cfg = AIConfig.load()
    if not cfg.OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY not set")

    _openai_client = OpenAI(api_key=cfg.OPENAI_API_KEY)
    return _openai_client


def _embed_openai(texts: List[str]) -> EmbeddingBatch:
    cfg = AIConfig.load()
    client = _get_openai_client()
    response = client.embeddings.create(model=cfg.EMBEDDING_OPENAI_MODEL, input=texts)
    vectors = [list(map(float, d.embedding)) for d in response.data]
    return EmbeddingBatch(vectors=vectors, backend="openai")


def _choose_backend() -> EmbeddingBackendName:
    cfg = AIConfig.load()
    mode = (cfg.EMBEDDING_BACKEND or "auto").lower()

    if mode == "local":
        return "local"
    if mode == "openai":
        return "openai"

    # auto: local Í∞ÄÎä•ÌïòÎ©¥ local
    if SentenceTransformer is not None:
        try:
            _get_local_model()
            return "local"
        except Exception:
            pass
    return "openai"


def get_embeddings(texts: List[str]) -> EmbeddingBatch:
    if not texts:
        return EmbeddingBatch(vectors=[], backend=_choose_backend())

    backend = _choose_backend()
    norm = [(t or "").strip() for t in texts]

    if backend == "local":
        return _embed_local(norm)
    return _embed_openai(norm)


==========================================================================================
# FILE: ai_worker/ai/handwriting/__init__.py
==========================================================================================
# apps/worker/ai/handwriting/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/handwriting/detector.py
==========================================================================================
# apps/worker/ai/handwriting/detector.py
from __future__ import annotations

from typing import Dict
import cv2  # type: ignore
import numpy as np  # type: ignore


def analyze_handwriting(image_path: str) -> Dict[str, float]:
    """
    legacy(doc_ai/handwriting/handwriting_detector.py) Ïù¥ÏãùÎ≥∏
    - Ìïú Ïù¥ÎØ∏ÏßÄÏóêÏÑú ÌïÑÍ∏∞ ÌùîÏ†Å/Í≥ÑÏÇ∞Ïãù ÌòïÌÉú ÌùîÏ†Å Ïó¨Î∂Ä Ï†êÏàò Î∞òÌôò

    return:
      {
        "writing_score": 0~1,
        "calculation_score": 0~1
      }
    """
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        return {"writing_score": 0.0, "calculation_score": 0.0}

    blur = cv2.GaussianBlur(img, (5, 5), 0)
    edges = cv2.Canny(blur, 50, 150)

    writing_density = float(np.sum(edges > 0) / edges.size)

    sobel_x = cv2.Sobel(blur, cv2.CV_64F, 1, 0, ksize=5)
    sobel_y = cv2.Sobel(blur, cv2.CV_64F, 0, 1, ksize=5)
    grad_mag = (np.mean(np.abs(sobel_x)) + np.mean(np.abs(sobel_y))) / 255.0

    writing_score = min(max(writing_density * 12.0, 0.0), 1.0)
    calculation_score = min(max(grad_mag * 3.0, 0.0), 1.0)

    return {
        "writing_score": float(writing_score),
        "calculation_score": float(calculation_score),
    }


==========================================================================================
# FILE: ai_worker/ai/homework/__init__.py
==========================================================================================



==========================================================================================
# FILE: ai_worker/ai/ocr/__init__.py
==========================================================================================
# apps/worker/ai/ocr/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/ocr/engine.py
==========================================================================================
from __future__ import annotations
from typing import List
from .schemas import OCRResultPayload, OMRDetectedAnswer


def run_ocr_engine(
    *,
    image_path: str,
) -> OCRResultPayload:
    """
    Ïã§Ï†ú OCR/OMR ÏóîÏßÑ ÏûêÎ¶¨
    - ÏßÄÍ∏àÏùÄ ÎçîÎØ∏
    - ÎÇòÏ§ëÏóê OpenCV / Tesseract / Ïô∏Î∂Ä API ÍµêÏ≤¥
    """

    answers: List[OMRDetectedAnswer] = [
        {
            "question_number": 1,
            "detected": ["B"],
            "confidence": 0.92,
            "marking": "single",
            "status": "ok",
        },
        {
            "question_number": 2,
            "detected": ["D"],
            "confidence": 0.88,
            "marking": "single",
            "status": "ok",
        },
    ]

    return {
        "version": "v1",
        "answers": answers,
        "raw_text": None,
    }


==========================================================================================
# FILE: ai_worker/ai/ocr/google.py
==========================================================================================
# apps/worker/ai/ocr/google.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Optional

# google cloud vision
from google.cloud import vision  # type: ignore


@dataclass
class OCRResult:
    text: str
    confidence: Optional[float] = None
    raw: Optional[Any] = None


def google_ocr(image_path: str) -> OCRResult:
    """
    WorkerÏóêÏÑú Ïã§ÌñâÎêòÎäî Google OCR
    - service accountÎäî GOOGLE_APPLICATION_CREDENTIALS ÎòêÎäî Í∏∞Î≥∏ ÌôòÍ≤ΩÏóê Îî∞Î¶Ñ
    """
    client = vision.ImageAnnotatorClient()

    with open(image_path, "rb") as f:
        content = f.read()

    image = vision.Image(content=content)
    response = client.text_detection(image=image)

    if getattr(response, "error", None) and response.error.message:
        return OCRResult(text="", confidence=None, raw={"error": response.error.message})

    annotations = getattr(response, "text_annotations", None) or []
    if not annotations:
        return OCRResult(text="", confidence=None, raw=None)

    return OCRResult(
        text=annotations[0].description or "",
        confidence=None,
        raw=None,  # rawÎ•º ÌÜµÏß∏Î°ú ÎÑòÍ∏∞Î©¥ ÏßÅÎ†¨Ìôî Ïù¥ÏäàÍ∞Ä ÏÉùÍ∏∏ Ïàò ÏûàÏñ¥ Í∏∞Î≥∏ None
    )


==========================================================================================
# FILE: ai_worker/ai/ocr/schemas.py
==========================================================================================
from __future__ import annotations
from typing import TypedDict, List, Optional


class OMRDetectedAnswer(TypedDict):
    question_number: int
    detected: List[str]
    confidence: float
    marking: str     # single / multi / blank
    status: str      # ok / error


class OCRResultPayload(TypedDict):
    version: str
    answers: List[OMRDetectedAnswer]
    raw_text: Optional[str]


==========================================================================================
# FILE: ai_worker/ai/ocr/tesseract.py
==========================================================================================
# apps/worker/ai/ocr/tesseract.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Optional

from PIL import Image  # type: ignore
import pytesseract  # type: ignore


@dataclass
class OCRResult:
    text: str
    confidence: Optional[float] = None
    raw: Optional[Any] = None


def tesseract_ocr(image_path: str) -> OCRResult:
    img = Image.open(image_path)

    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)
    text = "\n".join(data.get("text", [])).strip()

    confs = [c for c in data.get("conf", []) if c != -1]
    confidence = (sum(confs) / len(confs)) if confs else None

    # raw=data Îäî ÎÑàÎ¨¥ ÌÅ¥ Ïàò ÏûàÏñ¥ ÌïÑÏöî ÏãúÎßå ÏºúÍ∏∞
    return OCRResult(text=text, confidence=confidence, raw=None)


==========================================================================================
# FILE: ai_worker/ai/omr/__init__.py
==========================================================================================



==========================================================================================
# FILE: ai_worker/ai/omr/engine.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/identifier.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore

from apps.worker.ai_worker.ai.omr.meta_px import build_page_scale_from_meta, PageScale


BBox = Tuple[int, int, int, int]


@dataclass(frozen=True)
class IdentifierConfigV1:
    """
    Identifier OMR v1 (8 digits, each digit 0~9 single mark).

    Principles:
    - ROI based fill score (same philosophy as detect_omr_answers_v1)
    - Robust to scan/photo noise by sampling a square ROI around each bubble
    - No DB, no external calls, worker-only judgement/extraction
    """
    # Ï£ºÎ≥Ä ROI(Î≤ÑÎ∏î Ï§ëÏã¨ Í∏∞Ï§Ä) ÌôïÏû• Í≥ÑÏàò: r * k
    roi_expand_k: float = 1.55

    # blank ÌåêÎã®: Ìï¥Îãπ digitÏóêÏÑú ÏµúÍ≥† fillÏù¥ Ïù¥ Í∞íÎ≥¥Îã§ ÏûëÏúºÎ©¥ blank
    blank_threshold: float = 0.070

    # ambiguous ÌåêÎã®: top-2 gapÏù¥ Ïù¥ Í∞íÎ≥¥Îã§ ÏûëÏúºÎ©¥ ambiguous
    conf_gap_threshold: float = 0.060

    # (Ïö¥ÏòÅ Ìé∏Ïùò) digit-level confidence clamp
    min_confidence: float = 0.0
    max_confidence: float = 1.0


def _clamp(v: int, lo: int, hi: int) -> int:
    return max(lo, min(hi, v))


def _crop(gray: np.ndarray, bbox: BBox) -> np.ndarray:
    x, y, w, h = bbox
    x = _clamp(int(x), 0, gray.shape[1] - 1)
    y = _clamp(int(y), 0, gray.shape[0] - 1)
    w = max(1, int(w))
    h = max(1, int(h))
    w = min(w, gray.shape[1] - x)
    h = min(h, gray.shape[0] - y)
    return gray[y:y + h, x:x + w]


def _fill_score(roi_gray: np.ndarray) -> float:
    """
    Same core idea as OMR v1:
    - blur
    - OTSU + INV
    - filled pixel ratio
    """
    if roi_gray.size == 0:
        return 0.0

    blur = cv2.GaussianBlur(roi_gray, (5, 5), 0)
    _, th = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    filled = float(np.sum(th > 0))
    total = float(th.size) if th.size > 0 else 1.0
    score = filled / total
    return float(max(0.0, min(1.0, score)))


def _bubble_roi_bbox_px(
    *,
    center_px: Tuple[int, int],
    r_px: int,
    cfg: IdentifierConfigV1,
    img_w: int,
    img_h: int,
) -> BBox:
    cx, cy = center_px
    side = int(round(max(2, r_px) * cfg.roi_expand_k)) * 2
    x = int(cx - side // 2)
    y = int(cy - side // 2)
    x = _clamp(x, 0, img_w - 1)
    y = _clamp(y, 0, img_h - 1)
    w = _clamp(side, 1, img_w - x)
    h = _clamp(side, 1, img_h - y)
    return (x, y, w, h)


def detect_identifier_v1(
    *,
    image_bgr: np.ndarray,
    meta: Dict[str, Any],
    cfg: Optional[IdentifierConfigV1] = None,
) -> Dict[str, Any]:
    """
    Extract identifier(8 digits) from aligned full-page image.

    meta requirements:
      meta["identifier"]["bubbles"] list with:
        - digit_index (1..8)
        - number (0..9)
        - center: {x(mm), y(mm)}
        - r(mm)

    return contract:
      {
        "identifier": "12345678" | None,
        "digits": [{"digit_index":1,"value":1,"status":"ok|blank|ambiguous","confidence":0.91,"marks":[...]}...],
        "confidence": 0.0~1.0,
        "status": "ok|ambiguous|blank|error"
      }
    """
    cfg = cfg or IdentifierConfigV1()

    if image_bgr is None or image_bgr.size == 0:
        return {"identifier": None, "digits": [], "confidence": 0.0, "status": "error"}

    h, w = image_bgr.shape[:2]
    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)

    ident = meta.get("identifier") or {}
    bubbles = list(ident.get("bubbles") or [])
    if not bubbles:
        return {"identifier": None, "digits": [], "confidence": 0.0, "status": "error"}

    scale: PageScale = build_page_scale_from_meta(meta=meta, image_size_px=(w, h))

    # group bubbles by digit_index
    by_digit: Dict[int, List[Dict[str, Any]]] = {}
    for b in bubbles:
        try:
            di = int(b.get("digit_index") or 0)
        except Exception:
            continue
        if di <= 0:
            continue
        by_digit.setdefault(di, []).append(b)

    digits_out: List[Dict[str, Any]] = []
    identifier_chars: List[str] = []
    status_rollup = "ok"
    confidences: List[float] = []

    for digit_index in sorted(by_digit.keys()):
        bs = by_digit[digit_index]

        marks: List[Dict[str, Any]] = []
        for b in bs:
            num = int(b.get("number") or 0)
            c = b.get("center") or {}
            r_mm = float(b.get("r") or 0.0)

            cx_mm = float(c.get("x") or 0.0)
            cy_mm = float(c.get("y") or 0.0)

            cx_px, cy_px = scale.mm_to_px_point(cx_mm, cy_mm)

            # radius: use average scale for robustness (mm->px)
            r_px_x = max(1, scale.mm_to_px_len_x(r_mm))
            r_px_y = max(1, scale.mm_to_px_len_y(r_mm))
            r_px = max(1, int(round((r_px_x + r_px_y) / 2.0)))

            bbox = _bubble_roi_bbox_px(
                center_px=(cx_px, cy_px),
                r_px=r_px,
                cfg=cfg,
                img_w=w,
                img_h=h,
            )
            roi = _crop(gray, bbox)
            fill = _fill_score(roi)

            marks.append(
                {
                    "number": int(num),
                    "fill": float(fill),
                    "center_px": {"x": int(cx_px), "y": int(cy_px)},
                    "roi_px": {"x": int(bbox[0]), "y": int(bbox[1]), "w": int(bbox[2]), "h": int(bbox[3])},
                }
            )

        marks_sorted = sorted(marks, key=lambda m: float(m.get("fill") or 0.0), reverse=True)
        top = marks_sorted[0] if marks_sorted else {"number": 0, "fill": 0.0}
        second = marks_sorted[1] if len(marks_sorted) > 1 else {"number": 0, "fill": 0.0}

        top_fill = float(top.get("fill") or 0.0)
        second_fill = float(second.get("fill") or 0.0)
        gap = float(top_fill - second_fill)

        if top_fill < cfg.blank_threshold:
            digits_out.append(
                {
                    "digit_index": int(digit_index),
                    "value": None,
                    "status": "blank",
                    "confidence": 0.0,
                    "marks": marks_sorted,
                }
            )
            identifier_chars.append("?")
            status_rollup = "blank" if status_rollup == "ok" else status_rollup
            continue

        if gap < cfg.conf_gap_threshold:
            digits_out.append(
                {
                    "digit_index": int(digit_index),
                    "value": int(top.get("number") or 0),
                    "status": "ambiguous",
                    "confidence": float(max(cfg.min_confidence, min(cfg.max_confidence, top_fill))),
                    "gap": float(gap),
                    "marks": marks_sorted,
                }
            )
            identifier_chars.append(str(int(top.get("number") or 0)))
            status_rollup = "ambiguous" if status_rollup in ("ok",) else status_rollup
            confidences.append(float(top_fill))
            continue

        # ok
        conf = float(max(cfg.min_confidence, min(cfg.max_confidence, top_fill)))
        digits_out.append(
            {
                "digit_index": int(digit_index),
                "value": int(top.get("number") or 0),
                "status": "ok",
                "confidence": conf,
                "gap": float(gap),
                "marks": marks_sorted,
            }
        )
        identifier_chars.append(str(int(top.get("number") or 0)))
        confidences.append(conf)

    # identifier validity: must be 8 digits all ok/ambiguous (blankÏù¥Î©¥ ? Ìè¨Ìï®)
    identifier = "".join(identifier_chars)
    if "?" in identifier:
        identifier_final: Optional[str] = None
    else:
        identifier_final = identifier

    # overall confidence: conservative (mean of digit conf where available)
    overall_conf = float(sum(confidences) / len(confidences)) if confidences else 0.0

    return {
        "identifier": identifier_final,
        "raw_identifier": identifier,  # '?' Ìè¨Ìï® Í∞ÄÎä• (Ïö¥ÏòÅ ÎîîÎ≤ÑÍ∑∏/Î¶¨Ìä∏ÎùºÏù¥Ïö©)
        "digits": digits_out,
        "confidence": float(max(0.0, min(1.0, overall_conf))),
        "status": status_rollup,
    }


==========================================================================================
# FILE: ai_worker/ai/omr/identifier.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/identifier.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore

from apps.worker.ai_worker.ai.omr.meta_px import build_page_scale_from_meta, PageScale


BBox = Tuple[int, int, int, int]


@dataclass(frozen=True)
class IdentifierConfigV1:
    """
    Identifier OMR v1 (8 digits, each digit 0~9 single mark).

    Principles:
    - ROI based fill score (same philosophy as detect_omr_answers_v1)
    - Robust to scan/photo noise by sampling a square ROI around each bubble
    - No DB, no external calls, worker-only judgement/extraction
    """
    # (OPS DEFAULT) Ï¥¨ÏòÅ/ÏõåÌîÑÏóêÏÑú Ï§ëÏã¨ Ïò§Ï∞®Î•º Ìù°ÏàòÌïòÍ∏∞ ÏúÑÌï¥ ÏÜåÌè≠ ÌôïÏû•
    # Ï£ºÎ≥Ä ROI(Î≤ÑÎ∏î Ï§ëÏã¨ Í∏∞Ï§Ä) ÌôïÏû• Í≥ÑÏàò: r * k
    roi_expand_k: float = 1.60

    # (OPS DEFAULT) blank Í≥ºÎã§ Î∞©ÏßÄ: Ïã§Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Ïó∞ÌïÑ ÎÜçÎèÑ ÎÇÆÏùÄ ÏºÄÏù¥Ïä§ ÎåÄÏùë
    # blank ÌåêÎã®: Ìï¥Îãπ digitÏóêÏÑú ÏµúÍ≥† fillÏù¥ Ïù¥ Í∞íÎ≥¥Îã§ ÏûëÏúºÎ©¥ blank
    blank_threshold: float = 0.055

    # (OPS DEFAULT) ambiguous Í≥ºÎã§ Î∞©ÏßÄ: top-2 gap Í∏∞Ï§Ä ÏÜåÌè≠ ÏôÑÌôî
    # ambiguous ÌåêÎã®: top-2 gapÏù¥ Ïù¥ Í∞íÎ≥¥Îã§ ÏûëÏúºÎ©¥ ambiguous
    conf_gap_threshold: float = 0.050

    # (Ïö¥ÏòÅ Ìé∏Ïùò) digit-level confidence clamp
    min_confidence: float = 0.0
    max_confidence: float = 1.0


def _clamp(v: int, lo: int, hi: int) -> int:
    return max(lo, min(hi, v))


def _crop(gray: np.ndarray, bbox: BBox) -> np.ndarray:
    x, y, w, h = bbox
    x = _clamp(int(x), 0, gray.shape[1] - 1)
    y = _clamp(int(y), 0, gray.shape[0] - 1)
    w = max(1, int(w))
    h = max(1, int(h))
    w = min(w, gray.shape[1] - x)
    h = min(h, gray.shape[0] - y)
    return gray[y:y + h, x:x + w]


def _fill_score(roi_gray: np.ndarray) -> float:
    """
    Same core idea as OMR v1:
    - blur
    - OTSU + INV
    - filled pixel ratio
    """
    if roi_gray.size == 0:
        return 0.0

    blur = cv2.GaussianBlur(roi_gray, (5, 5), 0)
    _, th = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    filled = float(np.sum(th > 0))
    total = float(th.size) if th.size > 0 else 1.0
    score = filled / total
    return float(max(0.0, min(1.0, score)))


def _bubble_roi_bbox_px(
    *,
    center_px: Tuple[int, int],
    r_px: int,
    cfg: IdentifierConfigV1,
    img_w: int,
    img_h: int,
) -> BBox:
    cx, cy = center_px
    side = int(round(max(2, r_px) * cfg.roi_expand_k)) * 2
    x = int(cx - side // 2)
    y = int(cy - side // 2)
    x = _clamp(x, 0, img_w - 1)
    y = _clamp(y, 0, img_h - 1)
    w = _clamp(side, 1, img_w - x)
    h = _clamp(side, 1, img_h - y)
    return (x, y, w, h)


def detect_identifier_v1(
    *,
    image_bgr: np.ndarray,
    meta: Dict[str, Any],
    cfg: Optional[IdentifierConfigV1] = None,
) -> Dict[str, Any]:
    """
    Extract identifier(8 digits) from aligned full-page image.

    meta requirements:
      meta["identifier"]["bubbles"] list with:
        - digit_index (1..8)
        - number (0..9)
        - center: {x(mm), y(mm)}
        - r(mm)

    return contract:
      {
        "identifier": "12345678" | None,
        "digits": [{"digit_index":1,"value":1,"status":"ok|blank|ambiguous","confidence":0.91,"marks":[...]}...],
        "confidence": 0.0~1.0,
        "status": "ok|ambiguous|blank|error"
      }
    """
    cfg = cfg or IdentifierConfigV1()

    if image_bgr is None or image_bgr.size == 0:
        return {"identifier": None, "digits": [], "confidence": 0.0, "status": "error"}

    h, w = image_bgr.shape[:2]
    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)

    ident = meta.get("identifier") or {}
    bubbles = list(ident.get("bubbles") or [])
    if not bubbles:
        return {"identifier": None, "digits": [], "confidence": 0.0, "status": "error"}

    scale: PageScale = build_page_scale_from_meta(meta=meta, image_size_px=(w, h))

    # group bubbles by digit_index
    by_digit: Dict[int, List[Dict[str, Any]]] = {}
    for b in bubbles:
        try:
            di = int(b.get("digit_index") or 0)
        except Exception:
            continue
        if di <= 0:
            continue
        by_digit.setdefault(di, []).append(b)

    digits_out: List[Dict[str, Any]] = []
    identifier_chars: List[str] = []
    status_rollup = "ok"
    confidences: List[float] = []

    for digit_index in sorted(by_digit.keys()):
        bs = by_digit[digit_index]

        marks: List[Dict[str, Any]] = []
        for b in bs:
            num = int(b.get("number") or 0)
            c = b.get("center") or {}
            r_mm = float(b.get("r") or 0.0)

            cx_mm = float(c.get("x") or 0.0)
            cy_mm = float(c.get("y") or 0.0)

            cx_px, cy_px = scale.mm_to_px_point(cx_mm, cy_mm)

            # radius: use average scale for robustness (mm->px)
            r_px_x = max(1, scale.mm_to_px_len_x(r_mm))
            r_px_y = max(1, scale.mm_to_px_len_y(r_mm))
            r_px = max(1, int(round((r_px_x + r_px_y) / 2.0)))

            bbox = _bubble_roi_bbox_px(
                center_px=(cx_px, cy_px),
                r_px=r_px,
                cfg=cfg,
                img_w=w,
                img_h=h,
            )
            roi = _crop(gray, bbox)
            fill = _fill_score(roi)

            marks.append(
                {
                    "number": int(num),
                    "fill": float(fill),
                    "center_px": {"x": int(cx_px), "y": int(cy_px)},
                    "roi_px": {"x": int(bbox[0]), "y": int(bbox[1]), "w": int(bbox[2]), "h": int(bbox[3])},
                }
            )

        marks_sorted = sorted(marks, key=lambda m: float(m.get("fill") or 0.0), reverse=True)
        top = marks_sorted[0] if marks_sorted else {"number": 0, "fill": 0.0}
        second = marks_sorted[1] if len(marks_sorted) > 1 else {"number": 0, "fill": 0.0}

        top_fill = float(top.get("fill") or 0.0)
        second_fill = float(second.get("fill") or 0.0)
        gap = float(top_fill - second_fill)

        if top_fill < cfg.blank_threshold:
            digits_out.append(
                {
                    "digit_index": int(digit_index),
                    "value": None,
                    "status": "blank",
                    "confidence": 0.0,
                    "marks": marks_sorted,
                }
            )
            identifier_chars.append("?")
            status_rollup = "blank" if status_rollup == "ok" else status_rollup
            continue

        if gap < cfg.conf_gap_threshold:
            digits_out.append(
                {
                    "digit_index": int(digit_index),
                    "value": int(top.get("number") or 0),
                    "status": "ambiguous",
                    "confidence": float(max(cfg.min_confidence, min(cfg.max_confidence, top_fill))),
                    "gap": float(gap),
                    "marks": marks_sorted,
                }
            )
            identifier_chars.append(str(int(top.get("number") or 0)))
            status_rollup = "ambiguous" if status_rollup in ("ok",) else status_rollup
            confidences.append(float(top_fill))
            continue

        # ok
        conf = float(max(cfg.min_confidence, min(cfg.max_confidence, top_fill)))
        digits_out.append(
            {
                "digit_index": int(digit_index),
                "value": int(top.get("number") or 0),
                "status": "ok",
                "confidence": conf,
                "gap": float(gap),
                "marks": marks_sorted,
            }
        )
        identifier_chars.append(str(int(top.get("number") or 0)))
        confidences.append(conf)

    # identifier validity: must be 8 digits all ok/ambiguous (blankÏù¥Î©¥ ? Ìè¨Ìï®)
    identifier = "".join(identifier_chars)
    if "?" in identifier:
        identifier_final: Optional[str] = None
    else:
        identifier_final = identifier

    # overall confidence: conservative (mean of digit conf where available)
    overall_conf = float(sum(confidences) / len(confidences)) if confidences else 0.0

    return {
        "identifier": identifier_final,
        "raw_identifier": identifier,  # '?' Ìè¨Ìï® Í∞ÄÎä• (Ïö¥ÏòÅ ÎîîÎ≤ÑÍ∑∏/Î¶¨Ìä∏ÎùºÏù¥Ïö©)
        "digits": digits_out,
        "confidence": float(max(0.0, min(1.0, overall_conf))),
        "status": status_rollup,
    }


==========================================================================================
# FILE: ai_worker/ai/omr/meta_px.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/meta_px.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, Tuple


def _clamp(v: int, lo: int, hi: int) -> int:
    return max(lo, min(hi, v))


@dataclass(frozen=True)
class PageScale:
    """
    Convert meta(mm) -> px.
    Boundary rule (fixed):
      - meta(mm) is assets single truth
      - px conversion is worker responsibility
      - aligned/warped image must represent the full page
    """
    sx: float
    sy: float
    img_w: int
    img_h: int

    def mm_to_px_point(self, x_mm: float, y_mm: float) -> Tuple[int, int]:
        x = int(round(float(x_mm) * self.sx))
        y = int(round(float(y_mm) * self.sy))
        x = _clamp(x, 0, self.img_w - 1)
        y = _clamp(y, 0, self.img_h - 1)
        return x, y

    def mm_to_px_len_x(self, v_mm: float) -> int:
        return max(1, int(round(float(v_mm) * self.sx)))

    def mm_to_px_len_y(self, v_mm: float) -> int:
        return max(1, int(round(float(v_mm) * self.sy)))


def build_page_scale_from_meta(
    *,
    meta: Dict[str, Any],
    image_size_px: Tuple[int, int],
) -> PageScale:
    """
    Build scaler from template meta.
    meta page size is mm. image_size_px is (width, height).
    """
    img_w, img_h = int(image_size_px[0]), int(image_size_px[1])

    page = meta.get("page") or {}
    size = page.get("size") or {}
    page_w_mm = float(size.get("width") or 0.0)
    page_h_mm = float(size.get("height") or 0.0)

    if img_w <= 0 or img_h <= 0:
        raise ValueError("invalid image_size_px")
    if page_w_mm <= 0.0 or page_h_mm <= 0.0:
        raise ValueError("invalid meta page size")

    sx = img_w / page_w_mm
    sy = img_h / page_h_mm
    return PageScale(sx=float(sx), sy=float(sy), img_w=img_w, img_h=img_h)


==========================================================================================
# FILE: ai_worker/ai/omr/template_meta.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/template_meta.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Tuple, Optional

import requests


@dataclass(frozen=True)
class TemplateMeta:
    raw: Dict[str, Any]

    @property
    def units(self) -> str:
        return str(self.raw.get("units") or "mm")

    @property
    def page_size_mm(self) -> Tuple[float, float]:
        page = self.raw.get("page") or {}
        size = page.get("size") or {}
        return float(size.get("width") or 0.0), float(size.get("height") or 0.0)

    @property
    def questions(self) -> List[Dict[str, Any]]:
        return list(self.raw.get("questions") or [])


class TemplateMetaFetchError(RuntimeError):
    pass


def fetch_objective_meta(
    *,
    base_url: str,
    question_count: int,
    # auth options (choose what your deployment uses)
    auth_cookie_header: Optional[str] = None,
    bearer_token: Optional[str] = None,
    worker_token_header: Optional[str] = None,  # e.g. X-Worker-Token (if API allows)
    extra_headers: Optional[Dict[str, str]] = None,
    timeout: int = 10,
) -> TemplateMeta:
    """
    worker -> API (assets meta)

    Stability goals:
    - clear timeouts
    - explicit error messages
    - flexible auth (cookie/bearer/custom header)
    - returns structured exception for caller to gracefully fallback

    NOTE:
    - assets endpoint currently uses IsAuthenticated.
      Production typically uses cookie session OR bearer token.
      worker_token_header is optional if you later add internal auth for workers.
    """
    url = f"{base_url.rstrip('/')}/api/v1/assets/omr/objective/meta/"
    params = {"question_count": str(int(question_count))}

    headers: Dict[str, str] = {"Accept": "application/json"}
    if auth_cookie_header:
        headers["Cookie"] = auth_cookie_header
    if bearer_token:
        headers["Authorization"] = f"Bearer {bearer_token}"
    if worker_token_header:
        headers["X-Worker-Token"] = str(worker_token_header)
    if extra_headers:
        for k, v in extra_headers.items():
            if k and v:
                headers[str(k)] = str(v)

    try:
        r = requests.get(url, params=params, headers=headers, timeout=timeout)
    except Exception as e:
        raise TemplateMetaFetchError(f"meta_fetch_network_error: {e!r}") from e

    if r.status_code >= 400:
        body = (r.text or "")[:2000]
        raise TemplateMetaFetchError(
            f"meta_fetch_http_error: status={r.status_code} body={body}"
        )

    try:
        data = r.json()
    except Exception as e:
        raise TemplateMetaFetchError(f"meta_fetch_invalid_json: {e!r}") from e

    return TemplateMeta(raw=data)


==========================================================================================
# FILE: ai_worker/ai/omr/types.py
==========================================================================================
# apps/worker/ai/omr/types.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Literal


OMRStatus = Literal["ok", "blank", "ambiguous", "low_confidence", "error"]
OMRMarking = Literal["blank", "single", "multi"]


@dataclass(frozen=True)
class OMRAnswerV1:
    """
    Worker-side OMR payload v1 (question-level)
    This should be embedded into API-side SubmissionAnswer.meta["omr"] later.

    - version: "v1" fixed
    - detected: list[str] ex) ["B"] or ["B","D"]
    - marking: blank/single/multi
    - confidence: 0~1 (top mark confidence)
    - status: ok/blank/ambiguous/low_confidence/error
    - raw: debug info (optional)
    """
    version: str
    question_id: int

    detected: List[str]
    marking: OMRMarking
    confidence: float
    status: OMRStatus

    raw: Optional[Dict[str, Any]] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "version": self.version,
            "question_id": int(self.question_id),
            "detected": list(self.detected or []),
            "marking": self.marking,
            "confidence": float(self.confidence or 0.0),
            "status": self.status,
            "raw": self.raw,
        }


==========================================================================================
# FILE: ai_worker/ai/pipelines/__init__.py
==========================================================================================
# apps/worker/ai/pipelines/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/pipelines/dispatcher.py
==========================================================================================
# apps/worker/ai_worker/ai/pipelines/dispatcher.py
from __future__ import annotations

from typing import Any, Dict

from apps.shared.contracts.ai_job import AIJob
from apps.shared.contracts.ai_result import AIResult

from apps.worker.ai_worker.ai.config import AIConfig
from apps.worker.ai_worker.ai.ocr.google import google_ocr
from apps.worker.ai_worker.ai.ocr.tesseract import tesseract_ocr
from apps.worker.ai_worker.ai.detection.segment_dispatcher import segment_questions
from apps.worker.ai_worker.ai.handwriting.detector import analyze_handwriting
from apps.worker.ai_worker.ai.embedding.service import get_embeddings
from apps.worker.ai_worker.ai.problem.generator import generate_problem_from_ocr
from apps.worker.ai_worker.ai.pipelines.homework_video_analyzer import analyze_homework_video
from apps.worker.ai_worker.storage.downloader import download_to_tmp


def handle_ai_job(job: AIJob) -> AIResult:
    try:
        cfg = AIConfig.load()
        payload: Dict[str, Any] = job.payload or {}

        download_url = payload.get("download_url")
        if not download_url:
            return AIResult.failed(job.id, "download_url missing")

        local_path = download_to_tmp(
            download_url=download_url,
            job_id=str(job.id),
        )

        # --------------------------------------------------
        # OCR
        # --------------------------------------------------
        if job.type == "ocr":
            engine = (payload.get("engine") or cfg.OCR_ENGINE or "auto").lower()

            if engine == "tesseract":
                r = tesseract_ocr(local_path)
            elif engine == "google":
                r = google_ocr(local_path)
            else:
                try:
                    r = google_ocr(local_path)
                    if not r.text.strip():
                        r = tesseract_ocr(local_path)
                except Exception:
                    r = tesseract_ocr(local_path)

            return AIResult.done(
                job.id,
                {"text": r.text, "confidence": r.confidence},
            )

        # --------------------------------------------------
        # Question segmentation
        # --------------------------------------------------
        if job.type == "question_segmentation":
            boxes = segment_questions(local_path)
            return AIResult.done(job.id, {"boxes": boxes})

        # --------------------------------------------------
        # Handwriting analysis
        # --------------------------------------------------
        if job.type == "handwriting_analysis":
            scores = analyze_handwriting(local_path)
            return AIResult.done(job.id, scores)

        # --------------------------------------------------
        # Embedding
        # --------------------------------------------------
        if job.type == "embedding":
            texts = payload.get("texts") or []
            batch = get_embeddings(list(texts))
            return AIResult.done(
                job.id,
                {"backend": batch.backend, "vectors": batch.vectors},
            )

        # --------------------------------------------------
        # Problem generation
        # --------------------------------------------------
        if job.type == "problem_generation":
            ocr_text = payload.get("ocr_text") or ""
            parsed = generate_problem_from_ocr(ocr_text)
            return AIResult.done(
                job.id,
                {
                    "body": parsed.body,
                    "choices": parsed.choices,
                    "answer": parsed.answer,
                    "difficulty": parsed.difficulty,
                    "tag": parsed.tag,
                    "summary": parsed.summary,
                    "explanation": parsed.explanation,
                },
            )

        # --------------------------------------------------
        # Homework video analysis
        # --------------------------------------------------
        if job.type == "homework_video_analysis":
            frame_stride = int(payload.get("frame_stride") or 10)
            min_frame_count = int(payload.get("min_frame_count") or 30)
            analysis = analyze_homework_video(
                video_path=local_path,
                frame_stride=frame_stride,
                min_frame_count=min_frame_count,
            )
            return AIResult.done(job.id, analysis)

        # --------------------------------------------------
        # OMR grading (meta-aware) - production final
        # --------------------------------------------------
        if job.type == "omr_grading":
            """
            payload options (recommended):
              - mode: "scan" | "photo" | "auto" (default auto)
              - question_count: 10|20|30 (required if template_fetch is used)
              - template_meta: dict (inject meta directly; no API call)
              - template_fetch:
                    {
                      "base_url": "...",
                      "cookie": "...",
                      "bearer_token": "...",
                      "worker_token": "...",
                      "timeout": 10
                    }

            Output contract:
              {
                "version": "v1",
                "mode": "...",
                "aligned": true|false,
                "identifier": {...},
                "answers": [...],
                "meta_used": true|false
              }
            """
            import cv2  # type: ignore

            from apps.worker.ai_worker.ai.omr.engine import detect_omr_answers_v1, OMRConfigV1
            from apps.worker.ai_worker.ai.omr.roi_builder import build_questions_payload_from_meta
            from apps.worker.ai_worker.ai.omr.warp import warp_to_a4_landscape
            from apps.worker.ai_worker.ai.omr.template_meta import fetch_objective_meta, TemplateMetaFetchError
            from apps.worker.ai_worker.ai.omr.identifier import detect_identifier_v1, IdentifierConfigV1

            mode = str(payload.get("mode") or "auto").lower()
            if mode not in ("scan", "photo", "auto"):
                mode = "auto"

            # 1) meta ÌôïÎ≥¥
            meta = payload.get("template_meta")
            meta_used = False
            meta_fetch_error = None

            if not meta:
                tf = payload.get("template_fetch") or {}
                base_url = tf.get("base_url")
                if base_url:
                    qc = int(payload.get("question_count") or 0)
                    if qc not in (10, 20, 30):
                        return AIResult.failed(job.id, "question_count required (10|20|30) for template_fetch")

                    try:
                        meta_obj = fetch_objective_meta(
                            base_url=str(base_url),
                            question_count=qc,
                            auth_cookie_header=tf.get("cookie"),
                            bearer_token=tf.get("bearer_token"),
                            worker_token_header=tf.get("worker_token"),
                            timeout=int(tf.get("timeout") or 10),
                        )
                        meta = meta_obj.raw
                        meta_used = True
                    except TemplateMetaFetchError as e:
                        meta = None
                        meta_fetch_error = str(e)[:500]

            # 2) Ïù¥ÎØ∏ÏßÄ Î°úÎìú
            img_bgr = cv2.imread(local_path)
            if img_bgr is None:
                return AIResult.failed(job.id, "cannot read image")

            aligned = img_bgr

            # 3) mode Ï†ïÏ±Ö
            if mode == "photo":
                warped = warp_to_a4_landscape(img_bgr)
                if warped is None:
                    return AIResult.failed(job.id, "warp_failed_for_photo_mode")
                aligned = warped

            elif mode == "auto":
                warped = warp_to_a4_landscape(img_bgr)
                if warped is not None:
                    aligned = warped

            # 4) meta ÏóÜÏúºÎ©¥ legacy
            if not meta:
                questions = payload.get("questions") or []
                if not questions:
                    return AIResult.failed(job.id, "template_meta/template_fetch failed and legacy questions missing")

                answers = detect_omr_answers_v1(
                    image_path=local_path,
                    questions=list(questions),
                    cfg=None,
                )
                return AIResult.done(
                    job.id,
                    {
                        "version": "v1",
                        "mode": "legacy_questions",
                        "aligned": False,
                        "identifier": None,
                        "answers": answers,
                        "meta_used": False,
                        "debug": {
                            "meta_fetch_error": meta_fetch_error,
                        },
                    },
                )

            # 5) ROI + identifier
            h, w = aligned.shape[:2]
            questions_payload = build_questions_payload_from_meta(meta, (w, h))

            ident = detect_identifier_v1(
                image_bgr=aligned,
                meta=meta,
                cfg=IdentifierConfigV1(),
            )

            # 6) aligned Ï†ÄÏû• ÌõÑ OMR
            import tempfile, os

            tmp_path = os.path.join(tempfile.gettempdir(), f"omr_aligned_{job.id}.jpg")
            cv2.imwrite(tmp_path, aligned)

            cfg = OMRConfigV1()
            answers = detect_omr_answers_v1(
                image_path=tmp_path,
                questions=list(questions_payload),
                cfg=cfg,
            )

            return AIResult.done(
                job.id,
                {
                    "version": "v1",
                    "mode": mode,
                    "aligned": bool(aligned is not img_bgr),
                    "identifier": ident,
                    "answers": answers,
                    "meta_used": meta_used,
                    "debug": {
                        "meta_fetch_error": meta_fetch_error,
                    },
                },
            )

        return AIResult.failed(job.id, f"Unsupported job type: {job.type}")

    except Exception as e:
        return AIResult.failed(job.id, str(e))


==========================================================================================
# FILE: ai_worker/ai/pipelines/homework_video_analyzer.py
==========================================================================================
# apps/worker/ai/pipelines/homework_video_analyzer.py
from __future__ import annotations

from typing import Dict, Any, List
import cv2  # type: ignore
import numpy as np  # type: ignore


def _estimate_writing_score(gray_roi: np.ndarray) -> float:
    if gray_roi.size == 0:
        return 0.0
    dark = (gray_roi < 220).sum()
    total = gray_roi.size
    return float(dark) / float(total)


def analyze_homework_video(
    video_path: str,
    frame_stride: int = 10,
    min_frame_count: int = 30,
) -> Dict[str, Any]:
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"cannot open video: {video_path}")

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_idx = 0
    frame_results: List[Dict[str, Any]] = []

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        if frame_idx % frame_stride != 0:
            frame_idx += 1
            continue

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        score = _estimate_writing_score(gray)

        frame_results.append(
            {
                "index": frame_idx,
                "writing_score": round(float(score), 4),
                "has_writing": bool(score >= 0.05),
            }
        )
        frame_idx += 1

    cap.release()

    sampled = len(frame_results)
    if sampled == 0:
        return {
            "total_frames": total_frames,
            "sampled_frames": 0,
            "avg_writing_score": 0.0,
            "filled_ratio": 0.0,
            "frames": [],
            "too_short": total_frames < min_frame_count,
        }

    avg = sum(fr["writing_score"] for fr in frame_results) / sampled
    filled = sum(1 for fr in frame_results if fr["has_writing"])
    ratio = filled / sampled

    return {
        "total_frames": total_frames,
        "sampled_frames": sampled,
        "avg_writing_score": round(float(avg), 4),
        "filled_ratio": round(float(ratio), 4),
        "frames": frame_results,
        "too_short": total_frames < min_frame_count,
    }


==========================================================================================
# FILE: ai_worker/ai/problem/__init__.py
==========================================================================================
# apps/worker/ai/problem/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/problem/generator.py
==========================================================================================
# apps/worker/ai/problem/generator.py
from __future__ import annotations

import json
from dataclasses import dataclass
from typing import Optional

from apps.worker.ai_worker.ai.config import AIConfig
from apps.worker.ai_worker.ai.problem.prompt import BASE_PROMPT

try:
    from openai import OpenAI  # type: ignore
except Exception:
    OpenAI = None  # type: ignore


@dataclass
class ParsedProblem:
    body: str
    choices: list
    answer: Optional[str]
    difficulty: int
    tag: str
    summary: str
    explanation: str


_client: Optional["OpenAI"] = None


def _get_client() -> "OpenAI":
    global _client
    if _client is not None:
        return _client

    if OpenAI is None:
        raise RuntimeError("openai not installed")

    cfg = AIConfig.load()
    if not cfg.OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY not set")

    _client = OpenAI(api_key=cfg.OPENAI_API_KEY)
    return _client


def generate_problem_from_ocr(ocr_text: str) -> ParsedProblem:
    cfg = AIConfig.load()
    prompt = BASE_PROMPT.format(ocr_text=ocr_text)

    client = _get_client()
    response = client.chat.completions.create(
        model=cfg.PROBLEM_GEN_MODEL,
        messages=[
            {"role": "system", "content": "ÎãπÏã†ÏùÄ ÍµêÏú°Ïö© ÏãúÌóò Î¨∏Ï†úÎ•º ÏûêÎèô ÏÉùÏÑ±ÌïòÎäî ÏóîÏßÑÏûÖÎãàÎã§."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.2,
    )

    # SDK ÌòïÌÉú Ï∞®Ïù¥ Î∞©Ïñ¥
    msg = response.choices[0].message
    content = getattr(msg, "content", None) or msg.get("content")  # type: ignore

    data = json.loads(content)

    return ParsedProblem(
        body=data.get("body", ""),
        choices=data.get("choices", []),
        answer=data.get("answer"),
        difficulty=int(data.get("difficulty", 3)),
        tag=data.get("tag", ""),
        summary=data.get("summary", ""),
        explanation=data.get("explanation", ""),
    )


==========================================================================================
# FILE: ai_worker/ai/problem/prompt.py
==========================================================================================
# apps/worker/ai/problem/prompt.py
BASE_PROMPT = """
Îã§ÏùåÏùÄ ÏãúÌóò Î¨∏Ï†úÏùò OCR Í≤∞Í≥ºÏûÖÎãàÎã§.
ÏïÑÎûò ÌÖçÏä§Ìä∏Î•º Í∏∞Î∞òÏúºÎ°ú Î¨∏Ï†ú Ï†ïÎ≥¥Î•º JSON ÌòïÏãùÏúºÎ°ú Ï∂îÏ∂úÌïòÏÑ∏Ïöî.

ÏöîÍµ¨ÏÇ¨Ìï≠:
1) Î¨∏Ï†ú Î≥∏Î¨∏ (body)
2) ÏÑ†ÌÉùÏßÄ (choices): ÏóÜÏúºÎ©¥ Îπà Î∞∞Ïó¥
3) Ï†ïÎãµ (answer): Î™ÖÏãúÎêú Ï†ïÎãµÏù¥ ÏóÜÏúºÎ©¥ AIÍ∞Ä Ï∂îÎ°†, Ï∂îÎ°† Î∂àÍ∞Ä Ïãú null
4) ÎÇúÏù¥ÎèÑ (difficulty): 1~5 Ï†ïÏàòÎ°ú Ï∂îÏ†ï
5) ÌÉúÍ∑∏ (tag): ÏàòÌïô/Í≥ºÌïô/Íµ≠Ïñ¥ Îì± Í∞ÑÎã®Ìïú Î∂ÑÎ•ò
6) Î¨∏Ï†ú ÏöîÏïΩ (summary)
7) Ìï¥ÏÑ§ (explanation): Í∞ÑÎã®Î™ÖÎ£åÌïòÍ≤å

Ï∂úÎ†•ÏùÄ Î∞òÎìúÏãú JSON ÌòïÏãùÎßå ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî. Îã§Î•∏ ÌÖçÏä§Ìä∏Îäî Ìè¨Ìï®ÌïòÏßÄ ÎßàÏÑ∏Ïöî.

Ï∂úÎ†• ÌòïÏãù ÏòàÏãú:
{
  "body": "...",
  "choices": ["A...", "B...", "C...", "D..."],
  "answer": "C",
  "difficulty": 3,
  "tag": "ÏàòÌïô",
  "summary": "...",
  "explanation": "..."
}

OCR ÌÖçÏä§Ìä∏:
\"\"\"
{ocr_text}
\"\"\"
"""


==========================================================================================
# FILE: ai_worker/apps/worker/ai_worker/celery.py
==========================================================================================



==========================================================================================
# FILE: ai_worker/queue/redis_queue.py
==========================================================================================
import redis
import json
import time
import logging

logger = logging.getLogger(__name__)

class RedisJobQueue:
    def __init__(self, redis_url: str):
        self.r = redis.from_url(redis_url, decode_responses=True)
        self.jobs = "ai:jobs"
        self.processing = "ai:processing"
        self.dead = "ai:jobs:dead"

    def claim(self, timeout=5):
        return self.r.brpoplpush(self.jobs, self.processing, timeout)

    def ack(self, raw):
        self.r.lrem(self.processing, 1, raw)

    def retry_or_dead(self, job_dict, raw, reason):
        attempt = int(job_dict.get("attempt", 0)) + 1
        max_attempts = int(job_dict.get("max_attempts", 5))

        self.ack(raw)

        if attempt >= max_attempts:
            job_dict["error"] = reason
            self.r.lpush(self.dead, json.dumps(job_dict))
            return

        job_dict["attempt"] = attempt
        self.r.lpush(self.jobs, json.dumps(job_dict))


==========================================================================================
# FILE: ai_worker/storage/__init__.py
==========================================================================================



==========================================================================================
# FILE: ai_worker/storage/downloader.py
==========================================================================================
# ==============================================================================
# PATH: apps/worker/storage/downloader.py
#
# PURPOSE:
# - AI worker Ï†ÑÏö© ÌååÏùº Îã§Ïö¥Î°úÎìú Ïú†Ìã∏
# - presigned GET URL ‚Üí /tmp local file
# - R2 / S3 credential ÏÇ¨Ïö© ‚ùå
# - video_worker ÏΩîÎìúÏôÄ Ï†àÎåÄ Í≥µÏú†ÌïòÏßÄ ÏïäÏùå
# ==============================================================================

from __future__ import annotations

import os
import tempfile
import requests
from pathlib import Path


def download_to_tmp(
    *,
    download_url: str,
    job_id: str,
    suffix: str | None = None,
    timeout: int = 60,
    chunk_size: int = 1024 * 1024,  # 1MB
) -> str:
    """
    presigned GET URLÎ°ú ÌååÏùºÏùÑ Îã§Ïö¥Î°úÎìúÌïòÏó¨ local temp file Í≤ΩÎ°ú Î∞òÌôò

    Í∑úÏπô:
    - AI workerÎäî URLÎßå Ïã†Î¢∞
    - ÌååÏùºÏùÄ /tmp ÎòêÎäî OS temp dirÏóê ÏÉùÏÑ±
    - Ìò∏Ï∂úÏûêÎäî Î∞òÌôòÎêú pathÎßå ÏÇ¨Ïö©
    """

    tmp_dir = Path(tempfile.gettempdir())
    ext = suffix or ""

    filename = f"ai_job_{job_id}{ext}"
    tmp_path = tmp_dir / filename
    part_path = tmp_dir / f"{filename}.part"

    try:
        with requests.get(download_url, stream=True, timeout=timeout) as r:
            r.raise_for_status()

            expected_len = r.headers.get("Content-Length")
            expected_len = int(expected_len) if expected_len and expected_len.isdigit() else None

            written = 0
            with open(part_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=chunk_size):
                    if not chunk:
                        continue
                    f.write(chunk)
                    written += len(chunk)

            if expected_len is not None and written != expected_len:
                raise RuntimeError(
                    f"download size mismatch (expected={expected_len}, got={written})"
                )

        part_path.replace(tmp_path)
        return str(tmp_path)

    except Exception:
        try:
            if part_path.exists():
                part_path.unlink()
        except Exception:
            pass
        raise


==========================================================================================
# FILE: ai_worker/wrong_notes/__init__.py
==========================================================================================
# apps/worker/wrong_notes/__init__.py


==========================================================================================
# FILE: ai_worker/wrong_notes/templates/wrong_note.html
==========================================================================================
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="utf-8" />
    <style>
        body {
            font-family: Arial, Helvetica, sans-serif;
            font-size: 12px;
            line-height: 1.6;
        }
        h1 {
            border-bottom: 2px solid #333;
            padding-bottom: 4px;
        }
        .exam {
            margin-top: 20px;
        }
        .question {
            margin-left: 12px;
        }
        .wrong {
            color: #c0392b;
        }
    </style>
</head>
<body>
    <h1>Ïò§Îãµ ÎÖ∏Ìä∏</h1>

    <p>
        ÌïôÏÉù ID: {{ enrollment_id }}<br/>
        ÏÉùÏÑ±Ïùº: {{ created_at }}
    </p>

    {% for exam_id, items in grouped.items %}
        <div class="exam">
            <h2>ÏãúÌóò {{ exam_id }}</h2>
            {% for item in items %}
                <div class="question">
                    <span class="wrong">
                        Q{{ item.question_id }}
                    </span>
                    / Ï†úÏ∂ú ÎãµÏïà: {{ item.answer }}
                </div>
            {% endfor %}
        </div>
    {% endfor %}
</body>
</html>


==========================================================================================
# FILE: omr/roi_builder.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/roi_builder.py
from __future__ import annotations

from typing import Any, Dict, List, Tuple

import math


def _clamp(v: int, lo: int, hi: int) -> int:
    return max(lo, min(hi, v))


def build_questions_payload_from_meta(
    *,
    meta: Dict[str, Any],
    image_size_px: Tuple[int, int],
) -> List[Dict[str, Any]]:
    """
    meta(mm) -> questions payload (px) for detect_omr_answers_v1()

    detect_omr_answers_v1 expects:
      questions: [{question_id, roi:{x,y,w,h}, choices:[...], axis:"x"}, ...]

    image_size_px: (width, height)
    """
    img_w, img_h = image_size_px

    page = meta.get("page") or {}
    size = page.get("size") or {}
    page_w_mm = float(size.get("width") or 0.0)
    page_h_mm = float(size.get("height") or 0.0)
    if page_w_mm <= 0.0 or page_h_mm <= 0.0:
        raise ValueError("invalid meta page size")

    # Ï†ïÎ†¨Îêú Ïä§Ï∫î/ÏõåÌîÑ Í≤∞Í≥ºÎäî "ÌéòÏù¥ÏßÄ Ï†ÑÏ≤¥Í∞Ä Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤¥"ÎùºÍ≥† Í∞ÄÏ†ï
    sx = img_w / page_w_mm
    sy = img_h / page_h_mm

    out: List[Dict[str, Any]] = []
    for q in (meta.get("questions") or []):
        qnum = int(q.get("question_number") or 0)
        roi = q.get("roi") or {}

        x_mm = float(roi.get("x") or 0.0)
        y_mm = float(roi.get("y") or 0.0)
        w_mm = float(roi.get("w") or 0.0)
        h_mm = float(roi.get("h") or 0.0)

        x = int(round(x_mm * sx))
        y = int(round(y_mm * sy))
        w = int(round(w_mm * sx))
        h = int(round(h_mm * sy))

        # ÏïàÏ†Ñ ÌÅ¥Îû®ÌîÑ
        x = _clamp(x, 0, img_w - 1)
        y = _clamp(y, 0, img_h - 1)
        w = _clamp(w, 1, img_w - x)
        h = _clamp(h, 1, img_h - y)

        out.append(
            {
                "question_id": qnum,  # worker ÏóîÏßÑÏùÄ question_idÎßå ÏÇ¨Ïö©
                "roi": {"x": x, "y": y, "w": w, "h": h},
                "choices": ["A", "B", "C", "D", "E"],
                "axis": "x",
            }
        )

    # question_id ÏàúÏÑú Î≥¥Ïû•
    out.sort(key=lambda d: int(d.get("question_id") or 0))
    return out


==========================================================================================
# FILE: omr/template_meta.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/template_meta.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Tuple, Optional
import requests


@dataclass(frozen=True)
class TemplateMeta:
    raw: Dict[str, Any]

    @property
    def units(self) -> str:
        return str(self.raw.get("units") or "mm")

    @property
    def page_size_mm(self) -> Tuple[float, float]:
        page = self.raw.get("page") or {}
        size = page.get("size") or {}
        return float(size.get("width") or 0.0), float(size.get("height") or 0.0)

    @property
    def questions(self) -> List[Dict[str, Any]]:
        return list(self.raw.get("questions") or [])


def fetch_objective_meta(
    *,
    base_url: str,
    question_count: int,
    auth_cookie_header: Optional[str] = None,
    timeout: int = 10,
) -> TemplateMeta:
    """
    worker -> API (assets meta)
    - Ïô∏Î∂Ä SaaS Ìò∏Ï∂ú Í∏àÏßÄ: ÎÇ¥Î∂Ä APIÎßå Ìò∏Ï∂ú
    - auth Î∞©ÏãùÏùÄ Ïö¥ÏòÅ ÌôòÍ≤ΩÏóê ÎßûÍ≤å header/cookieÎ•º Ï†ÑÎã¨
    """
    url = f"{base_url.rstrip('/')}/api/v1/assets/omr/objective/meta/"
    params = {"question_count": str(int(question_count))}

    headers: Dict[str, str] = {}
    if auth_cookie_header:
        headers["Cookie"] = auth_cookie_header

    r = requests.get(url, params=params, headers=headers, timeout=timeout)
    r.raise_for_status()
    data = r.json()
    return TemplateMeta(raw=data)


==========================================================================================
# FILE: omr/warp.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/warp.py
from __future__ import annotations

from typing import Optional, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore


def _order_points(pts: np.ndarray) -> np.ndarray:
    # pts: (4,2)
    rect = np.zeros((4, 2), dtype=np.float32)
    s = pts.sum(axis=1)
    rect[0] = pts[np.argmin(s)]  # top-left
    rect[2] = pts[np.argmax(s)]  # bottom-right

    diff = np.diff(pts, axis=1)
    rect[1] = pts[np.argmin(diff)]  # top-right
    rect[3] = pts[np.argmax(diff)]  # bottom-left
    return rect


def warp_to_a4_landscape(
    *,
    image_bgr: np.ndarray,
    out_size_px: Tuple[int, int] = (3508, 2480),  # 300dpi A4 landscape (Í∑ºÏÇ¨)
) -> Optional[np.ndarray]:
    """
    Ï¥¨ÏòÅ/ÌîÑÎ†àÏûÑ Ïù¥ÎØ∏ÏßÄÏóêÏÑú Î¨∏ÏÑú(ÎãµÏïàÏßÄ) Ïô∏Í≥ΩÏùÑ Ï∞æÏïÑ A4 landscapeÎ°ú ÏõåÌîÑ.
    ÏÑ±Í≥µÌïòÎ©¥ "ÌéòÏù¥ÏßÄ Ï†ÑÏ≤¥ = Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤¥"Í∞Ä ÎêòÎØÄÎ°ú meta ROIÎ•º Í∑∏ÎåÄÎ°ú Ï†ÅÏö© Í∞ÄÎä•.

    Ïã§Ìå®ÌïòÎ©¥ None Î∞òÌôò -> callerÍ∞Ä fallback(yolo/opencv segmentation Îì±) Ï≤òÎ¶¨
    """
    if image_bgr is None or image_bgr.size == 0:
        return None

    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5, 5), 0)
    edges = cv2.Canny(blur, 50, 150)

    # Ïú§Í≥Ω Í∞ïÌôî
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))
    edges = cv2.dilate(edges, kernel, iterations=1)

    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        return None

    contours = sorted(contours, key=cv2.contourArea, reverse=True)

    page_cnt = None
    for cnt in contours[:8]:
        peri = cv2.arcLength(cnt, True)
        approx = cv2.approxPolyDP(cnt, 0.02 * peri, True)
        if len(approx) == 4:
            page_cnt = approx
            break

    if page_cnt is None:
        return None

    pts = page_cnt.reshape(4, 2).astype(np.float32)
    rect = _order_points(pts)

    out_w, out_h = out_size_px
    dst = np.array(
        [
            [0, 0],
            [out_w - 1, 0],
            [out_w - 1, out_h - 1],
            [0, out_h - 1],
        ],
        dtype=np.float32,
    )

    M = cv2.getPerspectiveTransform(rect, dst)
    warped = cv2.warpPerspective(image_bgr, M, (out_w, out_h))
    return warped


==========================================================================================
# FILE: storage/__init__.py
==========================================================================================



==========================================================================================
# FILE: storage/downloader.py
==========================================================================================
# PATH: apps/worker/storage/downloader.py
# Ïó≠Ìï†: Î™®Îì† AI worker ÏûÖÎ†• ÌååÏùº Îã§Ïö¥Î°úÎìú (Í≥µÌÜµ)

import os
import requests
from urllib.parse import urlparse


def download_to_tmp(download_url: str, job_id: str) -> str:
    """
    presigned GET URLÏóêÏÑú ÌååÏùº Îã§Ïö¥Î°úÎìú ‚Üí /tmp
    """
    parsed = urlparse(download_url)
    filename = os.path.basename(parsed.path)

    job_dir = f"/tmp/ai_job_{job_id}"
    os.makedirs(job_dir, exist_ok=True)

    local_path = os.path.join(job_dir, filename)

    with requests.get(download_url, stream=True, timeout=30) as r:
        r.raise_for_status()
        with open(local_path, "wb") as f:
            for chunk in r.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)

    return local_path


==========================================================================================
# FILE: storage/r2_client.py
==========================================================================================



==========================================================================================
# FILE: video_worker/config.py
==========================================================================================
from __future__ import annotations

import os
import sys
from dataclasses import dataclass


def _require(name: str) -> str:
    v = os.environ.get(name)
    if not v:
        raise RuntimeError(f"Missing required env: {name}")
    return v


def _require_any(*names: str) -> str:
    """
    Ïó¨Îü¨ env Ï§ë ÌïòÎÇòÎùºÎèÑ ÏûàÏúºÎ©¥ ÏÇ¨Ïö©.
    (SSOT Ïú†ÏßÄ + Ïö¥ÏòÅ ÌôòÍ≤Ω Ï∞®Ïù¥ Ìù°ÏàòÏö©)
    """
    for name in names:
        v = os.environ.get(name)
        if v:
            return v
    raise RuntimeError(f"Missing required env (any of): {', '.join(names)}")


def _float(name: str, default: str) -> float:
    try:
        return float(os.environ.get(name, default))
    except Exception:
        return float(default)


def _int(name: str, default: str) -> int:
    try:
        return int(os.environ.get(name, default))
    except Exception:
        return int(default)


@dataclass(frozen=True)
class Config:
    # API
    API_BASE_URL: str
    WORKER_TOKEN: str
    WORKER_ID: str

    # Polling / retry
    POLL_INTERVAL_SECONDS: float
    HTTP_TIMEOUT_SECONDS: float
    RETRY_MAX_ATTEMPTS: int
    BACKOFF_BASE_SECONDS: float
    BACKOFF_CAP_SECONDS: float

    # Temp
    TEMP_DIR: str

    # Locking (Idempotency)
    LOCK_DIR: str
    LOCK_STALE_SECONDS: int

    # Heartbeat
    HEARTBEAT_INTERVAL_SECONDS: int

    # ffmpeg / ffprobe
    FFMPEG_BIN: str
    FFPROBE_BIN: str
    FFPROBE_TIMEOUT_SECONDS: int
    FFMPEG_TIMEOUT_SECONDS: int

    # HLS / thumb
    HLS_TIME_SECONDS: int
    THUMBNAIL_AT_SECONDS: float

    # Validation
    MIN_SEGMENTS_PER_VARIANT: int

    # R2 (S3 compatible)
    R2_BUCKET: str
    R2_PREFIX: str
    R2_ENDPOINT: str
    R2_ACCESS_KEY: str
    R2_SECRET_KEY: str
    R2_REGION: str
    UPLOAD_MAX_CONCURRENCY: int

    # download tuning
    DOWNLOAD_TIMEOUT_SECONDS: float
    DOWNLOAD_CHUNK_BYTES: int


def load_config() -> Config:
    try:
        return Config(
            API_BASE_URL=_require("API_BASE_URL").rstrip("/"),
            WORKER_TOKEN=_require("INTERNAL_WORKER_TOKEN"),
            WORKER_ID=os.environ.get("WORKER_ID", "video-worker-1"),

            POLL_INTERVAL_SECONDS=_float("VIDEO_WORKER_POLL_INTERVAL", "1.0"),
            HTTP_TIMEOUT_SECONDS=_float("VIDEO_WORKER_HTTP_TIMEOUT", "10.0"),
            RETRY_MAX_ATTEMPTS=_int("VIDEO_WORKER_RETRY_MAX", "6"),
            BACKOFF_BASE_SECONDS=_float("VIDEO_WORKER_BACKOFF_BASE", "0.5"),
            BACKOFF_CAP_SECONDS=_float("VIDEO_WORKER_BACKOFF_CAP", "10.0"),

            TEMP_DIR=os.environ.get("VIDEO_WORKER_TEMP_DIR", "/tmp/video-worker"),

            # Idempotency lock
            LOCK_DIR=os.environ.get("VIDEO_WORKER_LOCK_DIR", "/tmp/video-worker-locks"),
            LOCK_STALE_SECONDS=_int("VIDEO_WORKER_LOCK_STALE_SECONDS", "3600"),

            # Heartbeat
            HEARTBEAT_INTERVAL_SECONDS=_int("VIDEO_WORKER_HEARTBEAT_INTERVAL", "20"),

            FFMPEG_BIN=os.environ.get("FFMPEG_BIN", "ffmpeg"),
            FFPROBE_BIN=os.environ.get("FFPROBE_BIN", "ffprobe"),
            FFPROBE_TIMEOUT_SECONDS=_int("FFPROBE_TIMEOUT_SECONDS", "60"),
            FFMPEG_TIMEOUT_SECONDS=_int("FFMPEG_TIMEOUT_SECONDS", "3600"),

            HLS_TIME_SECONDS=_int("HLS_TIME_SECONDS", "4"),
            THUMBNAIL_AT_SECONDS=_float("THUMBNAIL_AT_SECONDS", "1.0"),

            MIN_SEGMENTS_PER_VARIANT=_int("MIN_SEGMENTS_PER_VARIANT", "3"),

            # ‚úÖ ÌïµÏã¨ ÏàòÏ†ï Ìè¨Ïù∏Ìä∏ (ÏõêÎ≥∏ Î°úÏßÅ Ïú†ÏßÄ + env Î∂àÏùºÏπò Ìù°Ïàò)
            R2_BUCKET=_require_any("R2_BUCKET", "R2_VIDEO_BUCKET"),
            R2_PREFIX=os.environ.get("R2_PREFIX", "media/hls/videos"),
            R2_ENDPOINT=_require("R2_ENDPOINT"),
            R2_ACCESS_KEY=_require("R2_ACCESS_KEY"),
            R2_SECRET_KEY=_require("R2_SECRET_KEY"),
            R2_REGION=os.environ.get("R2_REGION", "auto"),
            UPLOAD_MAX_CONCURRENCY=_int("UPLOAD_MAX_CONCURRENCY", "8"),

            DOWNLOAD_TIMEOUT_SECONDS=_float("DOWNLOAD_TIMEOUT_SECONDS", "30.0"),
            DOWNLOAD_CHUNK_BYTES=_int("DOWNLOAD_CHUNK_BYTES", str(1024 * 1024)),
        )
    except Exception as e:
        print(f"[fatal] config error: {e}", file=sys.stderr)
        sys.exit(1)


==========================================================================================
# FILE: video_worker/download.py
==========================================================================================
from __future__ import annotations

import logging
from pathlib import Path

import requests

from apps.worker.video_worker.config import Config
from apps.worker.video_worker.utils import backoff_sleep, ensure_dir, trim_tail

logger = logging.getLogger("video_worker")


class DownloadError(RuntimeError):
    pass


def download_to_file(*, url: str, dst: Path, cfg: Config) -> None:
    """
    ÏïàÏ†ïÏ†Å Îã§Ïö¥Î°úÎìú:
    - stream chunk
    - retry with backoff
    - tmp(.part) -> atomic rename
    """
    ensure_dir(dst.parent)

    attempt = 0
    while True:
        try:
            with requests.get(url, stream=True, timeout=cfg.DOWNLOAD_TIMEOUT_SECONDS) as r:
                r.raise_for_status()

                tmp = dst.with_suffix(dst.suffix + ".part")
                bytes_written = 0

                with open(tmp, "wb") as f:
                    for chunk in r.iter_content(chunk_size=cfg.DOWNLOAD_CHUNK_BYTES):
                        if chunk:
                            f.write(chunk)
                            bytes_written += len(chunk)

                if bytes_written <= 0:
                    raise DownloadError("downloaded file is empty")

                tmp.replace(dst)
                return

        except Exception as e:
            attempt += 1
            if attempt >= cfg.RETRY_MAX_ATTEMPTS:
                raise DownloadError(f"download failed: {trim_tail(str(e))}") from e
            logger.warning("download retry attempt=%s err=%s", attempt, e)
            backoff_sleep(attempt, cfg.BACKOFF_BASE_SECONDS, cfg.BACKOFF_CAP_SECONDS)


==========================================================================================
# FILE: video_worker/heartbeat.py
==========================================================================================
# PATH: apps/worker/video_worker/heartbeat.py
#
# PURPOSE:
# - long-running ÏûëÏóÖ ÎèôÏïà backendÏóê Ï£ºÍ∏∞Ï†ÅÏúºÎ°ú heartbeat Ï†ÑÏÜ°
# - backendÏùò reclaim Ïò§Ìåê Î∞©ÏßÄ
#
# CONTRACT:
# - POST /api/v1/internal/video-worker/{video_id}/heartbeat/
# - Ïã§Ìå®Ìï¥ÎèÑ worker Î©îÏù∏ ÏûëÏóÖÏùÄ Ï§ëÎã®ÌïòÏßÄ ÏïäÏùå
#
# DESIGN:
# - exponential backoff
# - stop() Ìò∏Ï∂ú Ïãú Ï¶âÏãú Ï¢ÖÎ£å
# - thread-safe

from __future__ import annotations

import logging
import threading
import time
from typing import Optional

from apps.worker.video_worker.http_client import VideoAPIClient

logger = logging.getLogger("video_worker.heartbeat")


class HeartbeatThread:
    def __init__(
        self,
        *,
        client: VideoAPIClient,
        video_id: int,
        interval: int,
        backoff_base: int,
        backoff_cap: int,
    ):
        self._client = client
        self._video_id = video_id
        self._interval = max(1, int(interval))
        self._backoff_base = max(1, int(backoff_base))
        self._backoff_cap = max(self._backoff_base, int(backoff_cap))

        self._stop_event = threading.Event()
        self._thread: Optional[threading.Thread] = None

    def start(self) -> None:
        if self._thread is not None:
            return

        self._thread = threading.Thread(
            target=self._run,
            name=f"heartbeat-video-{self._video_id}",
            daemon=True,
        )
        self._thread.start()

    def stop(self) -> None:
        self._stop_event.set()
        if self._thread is not None:
            self._thread.join(timeout=5)

    def _run(self) -> None:
        backoff = self._backoff_base

        # ÏµúÏ¥à 1ÌöåÎäî Î∞îÎ°ú Î≥¥ÎÇ¥ÏßÄ ÏïäÍ≥† interval Ïù¥ÌõÑ Ï†ÑÏÜ°
        next_sleep = self._interval

        while not self._stop_event.wait(next_sleep):
            try:
                self._client.send_heartbeat(self._video_id)
                # ÏÑ±Í≥µ Ïãú backoff Î¶¨ÏÖã
                backoff = self._backoff_base
                next_sleep = self._interval
            except Exception as e:
                # heartbeat Ïã§Ìå®Îäî ÏπòÎ™ÖÏ†ÅÏù¥ÏßÄ ÏïäÎã§
                logger.warning(
                    "heartbeat failed video_id=%s err=%s",
                    self._video_id,
                    e,
                )
                # backoff Ï¶ùÍ∞Ä
                next_sleep = min(backoff, self._backoff_cap)
                backoff = min(backoff * 2, self._backoff_cap)


==========================================================================================
# FILE: video_worker/http_client.py
==========================================================================================
from __future__ import annotations

import logging
from typing import Any, Dict, Optional

import requests

logger = logging.getLogger("video_worker.http")


class VideoAPIClient:
    def __init__(
        self,
        *,
        base_url: str,
        worker_token: str,
        worker_id: str,
        timeout_seconds: int,
    ):
        self._base_url = base_url.rstrip("/")
        self._timeout = timeout_seconds
        self._headers = {
            "X-Worker-Token": worker_token,
            "X-Worker-Id": worker_id,
            "Content-Type": "application/json",
        }

    # --------------------------------------------------
    # Job control
    # --------------------------------------------------

    def fetch_next_job(self) -> Optional[Dict[str, Any]]:
        url = f"{self._base_url}/internal/video-worker/next/"
        resp = requests.get(
            url,
            headers=self._headers,
            timeout=self._timeout,
        )
        if resp.status_code == 204:
            return None
        resp.raise_for_status()
        return resp.json()

    def notify_complete(self, video_id: int, payload: Dict[str, Any]) -> None:
        url = f"{self._base_url}/internal/video-worker/{video_id}/complete/"
        resp = requests.post(
            url,
            json=payload,
            headers=self._headers,
            timeout=self._timeout,
        )
        resp.raise_for_status()

    def notify_fail(self, video_id: int, reason: str) -> None:
        url = f"{self._base_url}/internal/video-worker/{video_id}/fail/"
        resp = requests.post(
            url,
            json={"reason": reason},
            headers=self._headers,
            timeout=self._timeout,
        )
        resp.raise_for_status()

    # --------------------------------------------------
    # Heartbeat
    # --------------------------------------------------

    def send_heartbeat(self, video_id: int) -> None:
        url = f"{self._base_url}/internal/video-worker/{video_id}/heartbeat/"
        resp = requests.post(
            url,
            headers=self._headers,
            timeout=self._timeout,
        )
        resp.raise_for_status()

    # --------------------------------------------------
    # Lifecycle
    # --------------------------------------------------

    def close(self) -> None:
        """
        requests.SessionÏùÑ Ïì∞ÏßÄ ÏïäÏúºÎØÄÎ°ú noop.
        main.pyÏùò client.close() Í≥ÑÏïΩÏùÑ ÎßûÏ∂îÍ∏∞ ÏúÑÌïú Î©îÏÑúÎìú.
        """
        return


==========================================================================================
# FILE: video_worker/locking.py
==========================================================================================
from __future__ import annotations

import os
import time
from pathlib import Path
from dataclasses import dataclass


@dataclass(frozen=True)
class LockHandle:
    path: Path
    fd: int


class LockBusyError(RuntimeError):
    pass


def acquire_video_lock(lock_dir: str, video_id: int, stale_seconds: int) -> LockHandle:
    """
    Î°úÏª¨ idempotency lock:
    - single hostÏóêÏÑú Ï§ëÎ≥µÏ≤òÎ¶¨ Î∞©ÏßÄ
    - stale lockÏùÄ mtime Í∏∞Î∞òÏúºÎ°ú ÌöåÏàò

    NOTE:
    - multi-hostÎäî backend leaseÍπåÏßÄ ÏûàÏñ¥Ïïº ÏôÑÎ≤Ω (ÏöîÍµ¨ÏÇ¨Ìï≠ÏÉÅ ÏµúÏÜå ÌïòÎÇò Íµ¨ÌòÑÏù¥Î©¥ OK)
    """
    Path(lock_dir).mkdir(parents=True, exist_ok=True)
    path = Path(lock_dir) / f"video_{video_id}.lock"

    now = int(time.time())
    pid = os.getpid()
    payload = f"pid={pid}\ncreated_at={now}\n"

    try:
        fd = os.open(str(path), os.O_CREAT | os.O_EXCL | os.O_WRONLY, 0o644)
        os.write(fd, payload.encode())
        os.fsync(fd)
        return LockHandle(path=path, fd=fd)

    except FileExistsError:
        try:
            stat = path.stat()
            age = now - int(stat.st_mtime)
            if age > int(stale_seconds):
                try:
                    path.unlink()
                except Exception:
                    pass
                return acquire_video_lock(lock_dir, video_id, stale_seconds)
        except Exception:
            # stat Ïã§Ìå® ÏãúÏóêÎèÑ lockÏùÄ Ï°¥Ï§ë
            pass

        raise LockBusyError(f"video {video_id} already processing")


def release_video_lock(handle: LockHandle) -> None:
    try:
        os.close(handle.fd)
    except Exception:
        pass
    try:
        handle.path.unlink()
    except Exception:
        pass


==========================================================================================
# FILE: video_worker/main.py
==========================================================================================
from __future__ import annotations

import logging
import signal
import time

from apps.worker.video_worker.config import load_config
from apps.worker.video_worker.http_client import VideoAPIClient
from apps.worker.video_worker.video.processor import process_video_job
from apps.worker.video_worker.utils import backoff_sleep

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] [VIDEO-WORKER] %(message)s",
)
logger = logging.getLogger("video_worker")

_shutdown = False


def _handle_signal(sig, frame):
    global _shutdown
    logger.warning("shutdown signal received sig=%s", sig)
    _shutdown = True


def main() -> None:
    signal.signal(signal.SIGINT, _handle_signal)
    signal.signal(signal.SIGTERM, _handle_signal)

    cfg = load_config()

    # ‚úÖ FIX: ÏÉùÏÑ±Ïûê ÏãúÍ∑∏ÎãàÏ≤ò Î™ÖÌôïÌûà ÎßûÏ∂§ (ÏõêÎ≥∏ Íµ¨Ï°∞ Ïú†ÏßÄ)
    client = VideoAPIClient(
        base_url=cfg.API_BASE_URL,
        worker_token=cfg.WORKER_TOKEN,
        worker_id=cfg.WORKER_ID,
        timeout_seconds=int(cfg.HTTP_TIMEOUT_SECONDS),
    )

    logger.info(
        "Video Worker started worker_id=%s api=%s poll=%ss",
        cfg.WORKER_ID,
        cfg.API_BASE_URL,
        cfg.POLL_INTERVAL_SECONDS,
    )

    error_attempt = 0

    try:
        while not _shutdown:
            try:
                job = client.fetch_next_job()
                if not job:
                    time.sleep(cfg.POLL_INTERVAL_SECONDS)
                    continue

                error_attempt = 0

                logger.info("job received video_id=%s", job.get("video_id"))

                process_video_job(job=job, cfg=cfg, client=client)

            except Exception:
                logger.exception("worker loop error")
                error_attempt = min(error_attempt + 1, 10)
                backoff_sleep(
                    error_attempt,
                    cfg.BACKOFF_BASE_SECONDS,
                    cfg.BACKOFF_CAP_SECONDS,
                )

    finally:
        try:
            client.close()
        except Exception:
            pass
        logger.info("Video Worker shutdown complete")


if __name__ == "__main__":
    main()


==========================================================================================
# FILE: video_worker/utils.py
==========================================================================================
from __future__ import annotations

import logging
import random
import shutil
import tempfile
import time
from contextlib import contextmanager
from pathlib import Path

logger = logging.getLogger("video_worker")


@contextmanager
def temp_workdir(base_dir: str, prefix: str):
    Path(base_dir).mkdir(parents=True, exist_ok=True)
    path = Path(tempfile.mkdtemp(prefix=prefix, dir=base_dir))
    try:
        yield path
    finally:
        try:
            shutil.rmtree(path, ignore_errors=True)
        except Exception:
            logger.warning("Failed to cleanup temp dir: %s", path)


def ensure_dir(path: Path) -> None:
    path.mkdir(parents=True, exist_ok=True)


def backoff_sleep(attempt: int, base: float, cap: float) -> None:
    raw = min(cap, base * (2 ** attempt))
    jitter = random.uniform(0.5, 1.5)
    time.sleep(raw * jitter)


def trim_tail(s: str, limit: int = 2000) -> str:
    if not s:
        return ""
    return s[-limit:] if len(s) > limit else s


def guess_content_type(name: str) -> str:
    n = name.lower()
    if n.endswith(".m3u8"):
        return "application/vnd.apple.mpegurl"
    if n.endswith(".ts"):
        return "video/MP2T"
    if n.endswith(".mp4"):
        return "video/mp4"
    if n.endswith(".jpg") or n.endswith(".jpeg"):
        return "image/jpeg"
    if n.endswith(".png"):
        return "image/png"
    if n.endswith(".json"):
        return "application/json"
    return "application/octet-stream"


def cache_control_for_object(name: str) -> str:
    """
    R2 Cache-Control Ï†ÑÎûµ (ÏöîÍµ¨ÏÇ¨Ìï≠ Î∞òÏòÅ)

    - HLS playlist (.m3u8): ÏÑúÎ™Ö Ï†ïÏ±Ö/Ïø†ÌÇ§ Í∏∞Î∞ò Ï†ëÍ∑ºÏùÑ Ï†ÑÏ†úÎ°ú "no-cache"
      (ÌîåÎ†àÏù¥Î¶¨Ïä§Ìä∏Îäî Ïû¨ÏÉù Ï†ïÏ±Ö/ÌÜ†ÌÅ∞ Í∞±Ïã† ÏòÅÌñ• Î∞õÏùå)
    - Segment (.ts): immutable (ÏΩòÌÖêÏ∏† Ï£ºÏÜåÍ∞Ä prefix/video_id Í≥†Ï†ïÏù¥ÎùºÎèÑ,
      ÏÑ∏Í∑∏Î®ºÌä∏Îäî VOD ÏÉùÏÑ± ÌõÑ Î≥ÄÍ≤ΩÎêòÏßÄ ÏïäÎäî Í≤ÉÏù¥ Ï†ïÏÉÅ)
    - Thumbnail: 7d Ï∫êÏãú
    """
    n = name.lower()
    if n.endswith(".m3u8"):
        return "no-cache"
    if n.endswith(".ts"):
        return "public, max-age=31536000, immutable"
    if n.endswith(".jpg") or n.endswith(".jpeg") or n.endswith(".png"):
        return "public, max-age=604800"
    return "public, max-age=3600"


==========================================================================================
# FILE: video_worker/validate.py
==========================================================================================
from __future__ import annotations

from pathlib import Path
from apps.worker.video_worker.video.validate import validate_hls_output

__all__ = ["validate_hls_output"]


==========================================================================================
# FILE: video_worker/video/duration.py
==========================================================================================
# PATH: apps/worker/video_worker/video/duration.py
#
# PURPOSE:
# - Î°úÏª¨ ÏòÅÏÉÅ ÌååÏùºÏóêÏÑú ffprobeÎ°ú duration(Ï¥à) Ï∂îÏ∂ú
# - Ïã§Ìå®Ìï¥ÎèÑ worker Ï†ÑÏ≤¥ ÏûëÏóÖÏùÑ fail ÏãúÌÇ§ÏßÄ ÏïäÏùå (best-effort)

from __future__ import annotations

import subprocess
from typing import Optional


class DurationProbeError(RuntimeError):
    pass


def probe_duration_seconds(
    *,
    input_path: str,
    ffprobe_bin: str,
    timeout: int,
) -> Optional[int]:
    if not input_path:
        return None

    cmd = [
        ffprobe_bin,
        "-v", "error",
        "-show_entries", "format=duration",
        "-of", "default=noprint_wrappers=1:nokey=1",
        input_path,
    ]

    try:
        p = subprocess.run(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=timeout,
            check=False,
        )
    except Exception:
        return None

    if p.returncode != 0:
        return None

    raw = (p.stdout or "").strip()
    if not raw:
        return None

    try:
        sec = float(raw)
        if sec < 0:
            return None
        return int(sec)
    except Exception:
        return None


==========================================================================================
# FILE: video_worker/video/processor.py
==========================================================================================
# PATH: apps/worker/video_worker/video/processor.py

from __future__ import annotations

import logging
from typing import Dict, Any

import requests

from apps.worker.video_worker.config import settings
from apps.worker.video_worker.video.storage import (
    upload_hls_directory,
    upload_thumbnail_bytes,
)
from apps.support.video.utils import (
    extract_duration_seconds_from_url,
    generate_thumbnail_from_url,
)

logger = logging.getLogger("video.worker.processor")


class VideoProcessor:
    """
    Video processing pipeline (Worker)

    Ï±ÖÏûÑ:
    - source video URL Ï≤òÎ¶¨
    - HLS Î≥ÄÌôò
    - Ïç∏ÎÑ§Ïùº ÏÉùÏÑ±
    - API ÏÑúÎ≤ÑÏóê ÏôÑÎ£å/Ïã§Ìå® Î≥¥Í≥†
    """

    def __init__(self, *, api_base: str, worker_id: str, worker_token: str):
        self.api_base = api_base.rstrip("/")
        self.worker_id = worker_id
        self.worker_token = worker_token

    # --------------------------------------------------
    # Internal HTTP helpers
    # --------------------------------------------------

    def _headers(self) -> Dict[str, str]:
        return {
            "X-Worker-Token": self.worker_token,
            "X-Worker-Id": self.worker_id,
        }

    def _post(self, path: str, json: Dict[str, Any]) -> requests.Response:
        url = f"{self.api_base}{path}"
        return requests.post(url, json=json, headers=self._headers(), timeout=30)

    # --------------------------------------------------
    # Main entry
    # --------------------------------------------------

    def process(self, *, video_id: int, source_url: str) -> None:
        """
        Îã®Ïùº ÏòÅÏÉÅ Ï≤òÎ¶¨
        """
        try:
            logger.info(
                "video processing started video_id=%s worker=%s",
                video_id,
                self.worker_id,
            )

            # ==================================================
            # 1. duration Ï∂îÏ∂ú (ffprobe, URL Í∏∞Î∞ò)
            # ==================================================
            duration = extract_duration_seconds_from_url(source_url)

            if not duration or duration <= 0:
                raise RuntimeError("duration_probe_failed")

            # ==================================================
            # 2. HLS Î≥ÄÌôò
            # ==================================================
            # (‚ö†Ô∏è Í∏∞Ï°¥ Íµ¨ÌòÑ Ïú†ÏßÄ: ÎÇ¥Î∂ÄÏ†ÅÏúºÎ°ú ffmpeg ÏÇ¨Ïö©)
            hls_output_dir = upload_hls_directory(
                video_id=video_id,
                source_url=source_url,
            )

            # ==================================================
            # 3. Ïç∏ÎÑ§Ïùº ÏÉùÏÑ± (Î∞©Î≤ï 2)
            # --------------------------------------------------
            # ‚úÖ ÏòÅÏÉÅ Í∏∏Ïù¥ "Ï§ëÍ∞Ñ ÏßÄÏ†ê" Í∏∞Ï§Ä
            # - ÎÑàÎ¨¥ Ïïû/Îí§ ÌîÑÎ†àÏûÑ Î∞©ÏßÄ
            # ==================================================
            if duration >= 10:
                ss = int(duration * 0.5)
            elif duration >= 3:
                ss = max(1, duration // 2)
            else:
                ss = 0

            thumbnail_bytes = generate_thumbnail_from_url(
                source_url,
                ss_seconds=ss,
            )

            if thumbnail_bytes:
                upload_thumbnail_bytes(
                    video_id=video_id,
                    data=thumbnail_bytes,
                )
            else:
                logger.warning(
                    "thumbnail generation failed video_id=%s",
                    video_id,
                )

            # ==================================================
            # 4. API ÏÑúÎ≤ÑÏóê ÏôÑÎ£å Î≥¥Í≥†
            # ==================================================
            resp = self._post(
                f"/internal/video-worker/{video_id}/complete/",
                {
                    "hls_path": hls_output_dir,
                    "duration": duration,
                },
            )

            resp.raise_for_status()

            logger.info(
                "video processing completed video_id=%s duration=%s",
                video_id,
                duration,
            )

        except Exception as e:
            logger.exception(
                "video processing failed video_id=%s error=%s",
                video_id,
                e,
            )

            try:
                self._post(
                    f"/internal/video-worker/{video_id}/fail/",
                    {"reason": str(e)},
                )
            except Exception:
                logger.exception(
                    "failed to report processing failure video_id=%s",
                    video_id,
                )


==========================================================================================
# FILE: video_worker/video/r2_uploader.py
==========================================================================================
from __future__ import annotations

import os
from pathlib import Path

import boto3
from boto3.s3.transfer import TransferConfig

from apps.worker.video_worker.utils import guess_content_type, cache_control_for_object, trim_tail, backoff_sleep


class UploadError(RuntimeError):
    pass


def upload_directory(
    *,
    local_dir: Path,
    bucket: str,
    prefix: str,
    endpoint_url: str,
    access_key: str,
    secret_key: str,
    region: str,
    max_concurrency: int,
    retry_max: int = 5,
    backoff_base: float = 0.5,
    backoff_cap: float = 10.0,
) -> None:
    """
    ÏóÖÎ°úÎìú Ï†ïÏ±Ö (ÏöîÍµ¨ÏÇ¨Ìï≠ Î∞òÏòÅ):
    - Content-Type Ï†ïÌôïÌûà
    - Cache-Control Ï†ÑÎûµ Ìè¨Ìï®
      - .m3u8 : no-cache
      - .ts   : public, max-age=31536000, immutable
      - thumb : 7d
    - Î∂ÄÎ∂Ñ ÏóÖÎ°úÎìú Î∞©ÏßÄ:
      - boto3 multipart Ïã§Ìå® Ïãú ÏòàÏô∏ / retry
      - ÎèôÏùº KeyÏóê overwriteÎäî ÌóàÏö© (idempotent)
    """
    s3 = boto3.client(
        "s3",
        endpoint_url=endpoint_url,
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key,
        region_name=region,
    )

    transfer_cfg = TransferConfig(
        max_concurrency=max_concurrency,
        multipart_threshold=8 * 1024 * 1024,
        multipart_chunksize=8 * 1024 * 1024,
        use_threads=True,
    )

    local_dir = local_dir.resolve()

    for root, _, files in os.walk(local_dir):
        for name in files:
            full_path = Path(root) / name
            rel = full_path.relative_to(local_dir)
            key = f"{prefix.rstrip('/')}/{rel.as_posix()}"

            extra = {
                "ContentType": guess_content_type(name),
                "CacheControl": cache_control_for_object(name),
            }

            attempt = 0
            while True:
                try:
                    s3.upload_file(
                        Filename=str(full_path),
                        Bucket=bucket,
                        Key=key,
                        ExtraArgs=extra,
                        Config=transfer_cfg,
                    )
                    break
                except Exception as e:
                    attempt += 1
                    if attempt >= retry_max:
                        raise UploadError(f"upload failed key={key} err={trim_tail(str(e))}") from e
                    backoff_sleep(attempt, backoff_base, backoff_cap)


==========================================================================================
# FILE: video_worker/video/thumbnail.py
==========================================================================================
from __future__ import annotations

import subprocess
from pathlib import Path

from apps.worker.video_worker.utils import ensure_dir, trim_tail


class ThumbnailError(RuntimeError):
    pass


def generate_thumbnail(
    *,
    input_path: str,
    output_path: Path,
    ffmpeg_bin: str,
    at_seconds: float,
    timeout: int,
) -> None:
    ensure_dir(output_path.parent)

    cmd = [
        ffmpeg_bin,
        "-y",
        "-ss", f"{at_seconds:.3f}",
        "-i", input_path,
        "-frames:v", "1",
        "-q:v", "2",
        str(output_path),
    ]

    try:
        p = subprocess.run(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=timeout,
            check=False,
        )
    except subprocess.TimeoutExpired as e:
        raise ThumbnailError(f"thumbnail timeout ({timeout}s)") from e

    if p.returncode != 0:
        raise ThumbnailError(f"thumbnail ffmpeg failed: {trim_tail(p.stderr)}")


==========================================================================================
# FILE: video_worker/video/transcoder.py
==========================================================================================
# PATH: apps/worker/video_worker/video/transcoder.py

from __future__ import annotations

import json
import subprocess
from pathlib import Path
from typing import List, Optional

from apps.worker.video_worker.utils import ensure_dir, trim_tail

# preset Ïú†ÏßÄ (ÏàúÏÑú Ï§ëÏöî)
HLS_VARIANTS = [
    {"name": "1", "width": 426, "height": 240, "video_bitrate": "400k", "audio_bitrate": "64k"},
    {"name": "2", "width": 640, "height": 360, "video_bitrate": "800k", "audio_bitrate": "96k"},
    {"name": "3", "width": 1280, "height": 720, "video_bitrate": "2500k", "audio_bitrate": "128k"},
]


class TranscodeError(RuntimeError):
    pass


def _probe_resolution(input_path: str, ffprobe_bin: str, timeout: int) -> tuple[int, int]:
    cmd = [
        ffprobe_bin,
        "-v", "error",
        "-select_streams", "v:0",
        "-show_entries", "stream=width,height",
        "-of", "json",
        input_path,
    ]
    try:
        p = subprocess.run(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=timeout,
            check=False,
        )
    except subprocess.TimeoutExpired:
        return 0, 0

    if p.returncode != 0:
        return 0, 0

    try:
        data = json.loads(p.stdout)
        s = (data.get("streams") or [{}])[0]
        return int(s.get("width") or 0), int(s.get("height") or 0)
    except Exception:
        return 0, 0


def _select_variants(input_w: int, input_h: int) -> List[dict]:
    """
    ÏûÖÎ†• Ìï¥ÏÉÅÎèÑ ÏÉÅÌïú Î∞òÏòÅ:
    - ÏõêÎ≥∏Î≥¥Îã§ ÌÅ∞ variantÎäî Ï†úÏô∏
    """
    selected = []
    for v in HLS_VARIANTS:
        if v["width"] <= input_w and v["height"] <= input_h:
            selected.append(v)
    # ÏïàÏ†ÑÏû•Ïπò: ÏµúÏÜå 1Í∞ú
    if not selected:
        selected.append(HLS_VARIANTS[0])
    return selected


def prepare_output_dirs(output_root: Path, variants: List[dict]) -> None:
    ensure_dir(output_root)
    for v in variants:
        ensure_dir(output_root / f"v{v['name']}")


def has_audio_stream(*, input_path: str, ffprobe_bin: str, timeout: int) -> bool:
    cmd = [
        ffprobe_bin,
        "-v", "error",
        "-print_format", "json",
        "-show_streams",
        input_path,
    ]
    try:
        p = subprocess.run(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=timeout,
            check=False,
        )
    except subprocess.TimeoutExpired:
        return False

    if p.returncode != 0:
        return False

    try:
        data = json.loads(p.stdout)
        streams = data.get("streams") or []
        return any(s.get("codec_type") == "audio" for s in streams)
    except Exception:
        return False


def build_filter_complex(variants: List[dict]) -> str:
    parts: List[str] = []
    split_count = len(variants)
    parts.append("[0:v]split={}".format(split_count) + "".join(f"[v{i}]" for i in range(split_count)))
    for i, v in enumerate(variants):
        parts.append(f"[v{i}]scale={v['width']}:{v['height']}[v{i}out]")
    return ";".join(parts)


def build_ffmpeg_command(
    *,
    input_path: str,
    variants: List[dict],
    with_audio: bool,
    ffmpeg_bin: str,
    hls_time: int,
) -> List[str]:
    cmd: List[str] = [
        ffmpeg_bin,
        "-y",
        "-i", input_path,
        "-filter_complex", build_filter_complex(variants),
    ]

    for i, v in enumerate(variants):
        cmd += ["-map", f"[v{i}out]"]
        if with_audio:
            cmd += ["-map", "0:a?"]

        cmd += [
            f"-c:v:{i}", "libx264",
            "-profile:v", "main",
            "-pix_fmt", "yuv420p",
            f"-b:v:{i}", v["video_bitrate"],
            "-g", "48",
            "-keyint_min", "48",
            "-sc_threshold", "0",
        ]

        if with_audio:
            cmd += [
                f"-c:a:{i}", "aac",
                "-ac", "2",
                f"-b:a:{i}", v["audio_bitrate"],
            ]

    if with_audio:
        var_map = " ".join(f"v:{i},a:{i},name:{v['name']}" for i, v in enumerate(variants))
    else:
        var_map = " ".join(f"v:{i},name:{v['name']}" for i, v in enumerate(variants))

    cmd += [
        "-f", "hls",
        "-hls_time", str(hls_time),
        "-hls_playlist_type", "vod",
        "-hls_flags", "independent_segments",
        "-hls_segment_filename", "v%v/index%d.ts",
        "-master_pl_name", "master.m3u8",
        "-var_stream_map", var_map,
        "v%v/index.m3u8",
    ]
    return cmd


def transcode_to_hls(
    *,
    video_id: int,
    input_path: str,
    output_root: Path,
    ffmpeg_bin: str,
    ffprobe_bin: str,
    hls_time: int,
    timeout: Optional[int],
) -> Path:
    # ÏûÖÎ†• Ìï¥ÏÉÅÎèÑ Í∏∞Î∞ò variant ÏÑ†ÌÉù
    w, h = _probe_resolution(input_path, ffprobe_bin, min(60, int(timeout or 60)))
    variants = _select_variants(w, h)

    prepare_output_dirs(output_root, variants)

    with_audio = has_audio_stream(
        input_path=input_path,
        ffprobe_bin=ffprobe_bin,
        timeout=min(60, int(timeout or 60)),
    )

    cmd = build_ffmpeg_command(
        input_path=input_path,
        variants=variants,
        with_audio=with_audio,
        ffmpeg_bin=ffmpeg_bin,
        hls_time=hls_time,
    )

    try:
        p = subprocess.run(
            cmd,
            cwd=str(output_root.resolve()),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            timeout=timeout,
            check=False,
        )
    except subprocess.TimeoutExpired as e:
        raise TranscodeError(f"ffmpeg timeout video_id={video_id} seconds={timeout}") from e

    if p.returncode != 0:
        raise TranscodeError(
            f"ffmpeg failed video_id={video_id} with_audio={with_audio} stderr={trim_tail(p.stderr)}"
        )

    master = output_root / "master.m3u8"
    if not master.exists():
        raise TranscodeError("master.m3u8 not created")

    return master


==========================================================================================
# FILE: video_worker/video/validate.py
==========================================================================================
from __future__ import annotations

from pathlib import Path


def validate_hls_output(root: Path, min_segments: int) -> None:
    """
    Ïù∏ÏΩîÎî© Í≤∞Í≥º Íπ®Ïßê ÏûêÎèô fail Ï≤òÎ¶¨Ïö© Í≤ÄÏ¶ù:
    - master.m3u8 Ï°¥Ïû¨
    - Í∞Å variant playlist Ï°¥Ïû¨
    - Í∞Å variantÏóê ÏµúÏÜå ÏÑ∏Í∑∏Î®ºÌä∏ Ïàò ÌôïÎ≥¥
    """
    master = root / "master.m3u8"
    if not master.exists():
        raise RuntimeError("master.m3u8 missing")

    variants = list(root.glob("v*/index.m3u8"))
    if not variants:
        raise RuntimeError("no variant playlists (v*/index.m3u8)")

    for v in variants:
        segs = list(v.parent.glob("*.ts"))
        if len(segs) < int(min_segments):
            raise RuntimeError(f"HLS validation failed: {v} segments={len(segs)} min={min_segments}")
