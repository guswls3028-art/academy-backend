====================================================================================================
# BACKEND APP: worker
# ROOT PATH: C:\academy\apps\worker
====================================================================================================


==========================================================================================
# FILE: __init__.py
==========================================================================================
#apps/worker/__init__.py

#ì•„ë¬´ë‚´ìš©ì—†ìŒ.


==========================================================================================
# FILE: ai_worker/__init__.py
==========================================================================================



==========================================================================================
# FILE: ai_worker/run.py
==========================================================================================
# apps/worker/run.py
from __future__ import annotations

import os
import sys
import time
import signal
import logging
import requests

from apps.shared.contracts.ai_job import AIJob
from apps.shared.contracts.ai_result import AIResult
from apps.worker.ai_worker.ai.pipelines.dispatcher import handle_ai_job

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] [AI-WORKER] %(message)s",
)
logger = logging.getLogger(__name__)

API_BASE_URL = os.getenv("API_BASE_URL", "http://localhost:8000")
INTERNAL_WORKER_TOKEN = os.getenv("INTERNAL_WORKER_TOKEN", "long-random-secret")
POLL_INTERVAL_SEC = float(os.getenv("AI_WORKER_POLL_INTERVAL", "1.0"))

_running = True


def _shutdown(signum, frame):
    global _running
    logger.warning("Shutdown signal received (%s)", signum)
    _running = False


signal.signal(signal.SIGINT, _shutdown)
signal.signal(signal.SIGTERM, _shutdown)


def fetch_job() -> AIJob | None:
    """
    API â†’ Worker
    GET /api/v1/internal/ai/job/next/
    response: { "job": {...} | null }
    """
    url = f"{API_BASE_URL}/api/v1/internal/ai/job/next/"
    headers = {"X-Worker-Token": INTERNAL_WORKER_TOKEN}

    resp = requests.get(url, headers=headers, timeout=10)
    resp.raise_for_status()

    data = resp.json()
    job_data = data.get("job")
    if not job_data:
        return None

    return AIJob.from_dict(job_data)


def submit_result(result: AIResult, submission_id: int) -> None:
    """
    Worker â†’ API
    POST /api/v1/internal/ai/job/result/
    """
    url = f"{API_BASE_URL}/api/v1/internal/ai/job/result/"
    headers = {
        "X-Worker-Token": INTERNAL_WORKER_TOKEN,
        "Content-Type": "application/json",
    }

    # âœ… êµ¬ì¡° ê³ ì • (ì¤‘ìš”)
    payload = {
        "submission_id": submission_id,
        "status": result.status,
        "result": result.result,
        "error": result.error,
    }

    resp = requests.post(
        url,
        json=payload,
        headers=headers,
        timeout=20,
    )
    resp.raise_for_status()


def main():
    logger.info("AI Worker started")

    while _running:
        try:
            job = fetch_job()
            if job is None:
                time.sleep(POLL_INTERVAL_SEC)
                continue

            logger.info("Job received: id=%s type=%s", job.id, job.type)

            # ğŸ”¥ AI ì²˜ë¦¬
            result = handle_ai_job(job)

            # ğŸ”¥ ê²°ê³¼ ì „ì†¡ (submission_id = job.source_id)
            submit_result(result, int(job.source_id))

            logger.info(
                "Job finished: id=%s status=%s",
                job.id,
                result.status,
            )

        except Exception:
            logger.exception("Worker loop error")
            time.sleep(2.0)

    logger.info("AI Worker shutdown complete")


if __name__ == "__main__":
    sys.exit(main())


==========================================================================================
# FILE: ai_worker/ai/__init__.py
==========================================================================================
# apps/worker/ai/__init__.py
# Celery ì œê±°ë¨. run.py ë‹¨ì¼ ì§„ì…


==========================================================================================
# FILE: ai_worker/ai/config.py
==========================================================================================
# apps/worker/ai/config.py
from __future__ import annotations

import os
from dataclasses import dataclass
from typing import List, Optional


def _env(name: str, default: Optional[str] = None) -> Optional[str]:
    v = os.getenv(name)
    return v if v not in (None, "") else default


@dataclass(frozen=True)
class AIConfig:
    # OCR
    OCR_ENGINE: str = "google"  # google | tesseract | auto
    GOOGLE_APPLICATION_CREDENTIALS: Optional[str] = None  # optional (google sdk default)

    # Segmentation
    QUESTION_SEGMENTATION_ENGINE: str = "auto"  # yolo|opencv|template|auto

    # YOLO (optional)
    YOLO_QUESTION_MODEL_PATH: Optional[str] = None
    YOLO_QUESTION_INPUT_SIZE: int = 640
    YOLO_QUESTION_CONF_THRESHOLD: float = 0.4
    YOLO_QUESTION_IOU_THRESHOLD: float = 0.5

    # Embedding
    EMBEDDING_BACKEND: str = "auto"  # local|openai|auto
    EMBEDDING_LOCAL_MODEL: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    EMBEDDING_OPENAI_MODEL: str = "text-embedding-3-small"
    OPENAI_API_KEY: Optional[str] = None

    # Problem generation
    PROBLEM_GEN_MODEL: str = "gpt-4.1-mini"  # default

    @staticmethod
    def load() -> "AIConfig":
        return AIConfig(
            OCR_ENGINE=_env("OCR_ENGINE", "google") or "google",
            GOOGLE_APPLICATION_CREDENTIALS=_env("GOOGLE_APPLICATION_CREDENTIALS"),

            QUESTION_SEGMENTATION_ENGINE=_env("QUESTION_SEGMENTATION_ENGINE", "auto") or "auto",

            YOLO_QUESTION_MODEL_PATH=_env("YOLO_QUESTION_MODEL_PATH"),
            YOLO_QUESTION_INPUT_SIZE=int(_env("YOLO_QUESTION_INPUT_SIZE", "640") or "640"),
            YOLO_QUESTION_CONF_THRESHOLD=float(_env("YOLO_QUESTION_CONF_THRESHOLD", "0.4") or "0.4"),
            YOLO_QUESTION_IOU_THRESHOLD=float(_env("YOLO_QUESTION_IOU_THRESHOLD", "0.5") or "0.5"),

            EMBEDDING_BACKEND=_env("EMBEDDING_BACKEND", "auto") or "auto",
            EMBEDDING_LOCAL_MODEL=_env(
                "EMBEDDING_LOCAL_MODEL",
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            ) or "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            EMBEDDING_OPENAI_MODEL=_env("EMBEDDING_OPENAI_MODEL", "text-embedding-3-small") or "text-embedding-3-small",
            OPENAI_API_KEY=_env("OPENAI_API_KEY") or _env("EMBEDDING_OPENAI_API_KEY"),

            PROBLEM_GEN_MODEL=_env("PROBLEM_GEN_MODEL", "gpt-4.1-mini") or "gpt-4.1-mini",
        )


==========================================================================================
# FILE: ai_worker/ai/detection/__init__.py
==========================================================================================
# apps/worker/ai/detection/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/detection/segment_dispatcher.py
==========================================================================================
# apps/worker/ai/detection/segment_dispatcher.py
from __future__ import annotations

from typing import List, Tuple

from apps.worker.ai_worker.ai.config import AIConfig
from apps.worker.ai_worker.ai.detection.segment_opencv import segment_questions_opencv
from apps.worker.ai_worker.ai.detection.segment_yolo import segment_questions_yolo

BBox = Tuple[int, int, int, int]


def segment_questions(image_path: str) -> List[BBox]:
    """
    worker-side segmentation single entrypoint
    """
    cfg = AIConfig.load()
    engine = (cfg.QUESTION_SEGMENTATION_ENGINE or "auto").lower()

    if engine == "opencv":
        return segment_questions_opencv(image_path)
    if engine == "yolo":
        return segment_questions_yolo(image_path)

    # auto: yolo -> opencv
    try:
        boxes = segment_questions_yolo(image_path)
        if boxes:
            return boxes
    except Exception:
        pass
    return segment_questions_opencv(image_path)


==========================================================================================
# FILE: ai_worker/ai/detection/segment_opencv.py
==========================================================================================
# apps/worker/ai/detection/segment_opencv.py
from __future__ import annotations

from typing import List, Tuple
import cv2  # type: ignore

BBox = Tuple[int, int, int, int]


def segment_questions_opencv(image_path: str) -> List[BBox]:
    """
    legacy ì„ì—¬ìˆë˜ opencv segmentation ì •ë¦¬ë³¸
    ì…ë ¥: image_path
    ì¶œë ¥: [(x,y,w,h), ...]
    """
    image_bgr = cv2.imread(image_path)
    if image_bgr is None:
        return []

    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5, 5), 0)

    _, thresh = cv2.threshold(
        blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU
    )

    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))
    dilated = cv2.dilate(thresh, kernel, iterations=1)

    contours, _ = cv2.findContours(
        dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
    )

    h_img, w_img = gray.shape[:2]
    min_area = w_img * h_img * 0.005
    max_area = w_img * h_img * 0.9

    boxes: List[BBox] = []
    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)
        area = w * h
        if area < min_area or area > max_area:
            continue

        aspect = h / (w + 1e-6)
        if aspect < 0.3:
            continue

        boxes.append((x, y, w, h))

    boxes.sort(key=lambda b: (b[1], b[0]))
    return boxes


==========================================================================================
# FILE: ai_worker/ai/detection/segment_yolo.py
==========================================================================================
# apps/worker/ai/detection/segment_yolo.py
from __future__ import annotations

from functools import lru_cache
from typing import List, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore

from apps.worker.ai_worker.ai.config import AIConfig

BBox = Tuple[int, int, int, int]


class YoloNotConfiguredError(RuntimeError):
    pass


try:
    import onnxruntime as ort  # type: ignore
    _HAS_ORT = True
except Exception:
    _HAS_ORT = False


@lru_cache()
def _get_session():
    cfg = AIConfig.load()

    if not _HAS_ORT:
        raise YoloNotConfiguredError("onnxruntime not installed")

    if not cfg.YOLO_QUESTION_MODEL_PATH:
        raise YoloNotConfiguredError("YOLO_QUESTION_MODEL_PATH not set")

    providers = ["CPUExecutionProvider"]
    return ort.InferenceSession(str(cfg.YOLO_QUESTION_MODEL_PATH), providers=providers)


def _preprocess(image_bgr, input_size: int):
    h0, w0 = image_bgr.shape[:2]
    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
    resized = cv2.resize(image_rgb, (input_size, input_size))
    resized = resized.astype(np.float32) / 255.0

    tensor = np.transpose(resized, (2, 0, 1))
    tensor = np.expand_dims(tensor, axis=0)

    scale_x = w0 / float(input_size)
    scale_y = h0 / float(input_size)
    return tensor, scale_x, scale_y


def _nms(boxes, scores, iou_threshold: float):
    if len(boxes) == 0:
        return []

    boxes = boxes.astype(np.float32)
    scores = scores.astype(np.float32)

    x1, y1, x2, y2 = boxes.T
    areas = (x2 - x1) * (y2 - y1)
    order = scores.argsort()[::-1]

    keep = []
    while order.size > 0:
        i = int(order[0])
        keep.append(i)

        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])

        w = np.maximum(0.0, xx2 - xx1)
        h = np.maximum(0.0, yy2 - yy1)
        inter = w * h

        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-6)
        idxs = np.where(iou <= iou_threshold)[0]
        order = order[idxs + 1]

    return keep


def segment_questions_yolo(image_path: str) -> List[BBox]:
    cfg = AIConfig.load()
    sess = _get_session()

    image_bgr = cv2.imread(image_path)
    if image_bgr is None:
        return []

    input_tensor, scale_x, scale_y = _preprocess(image_bgr, cfg.YOLO_QUESTION_INPUT_SIZE)

    input_name = sess.get_inputs()[0].name
    outputs = sess.run(None, {input_name: input_tensor})
    preds = outputs[0]
    if preds.ndim == 3:
        preds = preds[0]

    boxes = []
    scores = []

    for det in preds:
        cx, cy, w, h, obj_conf = det[:5]
        cls_scores = det[5:]
        cls_conf = float(cls_scores.max()) if cls_scores.size > 0 else 1.0

        score = float(obj_conf * cls_conf)
        if score < cfg.YOLO_QUESTION_CONF_THRESHOLD:
            continue

        x1 = (cx - w / 2.0) * scale_x
        y1 = (cy - h / 2.0) * scale_y
        x2 = (cx + w / 2.0) * scale_x
        y2 = (cy + h / 2.0) * scale_y

        boxes.append([x1, y1, x2, y2])
        scores.append(score)

    if not boxes:
        return []

    boxes_np = np.array(boxes)
    scores_np = np.array(scores)

    keep_idx = _nms(boxes_np, scores_np, cfg.YOLO_QUESTION_IOU_THRESHOLD)

    final: List[BBox] = []
    for i in keep_idx:
        x1, y1, x2, y2 = boxes_np[i]
        final.append((int(x1), int(y1), int(x2 - x1), int(y2 - y1)))

    final.sort(key=lambda b: (b[1], b[0]))
    return final


==========================================================================================
# FILE: ai_worker/ai/embedding/__init__.py
==========================================================================================
# apps/worker/ai/embedding/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/embedding/service.py
==========================================================================================
# apps/worker/ai/embedding/service.py
from __future__ import annotations

from dataclasses import dataclass
from typing import List, Optional, Sequence, Literal
import math

from apps.worker.ai_worker.ai.config import AIConfig

EmbeddingBackendName = Literal["local", "openai"]


@dataclass
class EmbeddingBatch:
    vectors: List[List[float]]
    backend: EmbeddingBackendName


def cosine_similarity(a: Sequence[float], b: Sequence[float]) -> float:
    """
    cosine similarity normalized to 0~1
    """
    if not a or not b:
        return 0.0
    if len(a) != len(b):
        return 0.0

    dot = 0.0
    na = 0.0
    nb = 0.0
    for x, y in zip(a, b):
        dot += x * y
        na += x * x
        nb += y * y

    if na <= 0.0 or nb <= 0.0:
        return 0.0

    sim = dot / (math.sqrt(na) * math.sqrt(nb))
    sim = max(-1.0, min(1.0, sim))
    return (sim + 1.0) / 2.0


# -------- local (sentence-transformers) --------
try:
    from sentence_transformers import SentenceTransformer  # type: ignore
except Exception:
    SentenceTransformer = None  # type: ignore

_local_model: Optional["SentenceTransformer"] = None


def _get_local_model() -> "SentenceTransformer":
    global _local_model
    if _local_model is not None:
        return _local_model

    if SentenceTransformer is None:
        raise RuntimeError("sentence-transformers not installed")

    cfg = AIConfig.load()
    _local_model = SentenceTransformer(cfg.EMBEDDING_LOCAL_MODEL)
    return _local_model


def _embed_local(texts: List[str]) -> EmbeddingBatch:
    model = _get_local_model()
    vectors = model.encode(texts, convert_to_numpy=False)
    vectors_list = [list(map(float, v)) for v in vectors]
    return EmbeddingBatch(vectors=vectors_list, backend="local")


# -------- openai --------
try:
    from openai import OpenAI  # type: ignore
except Exception:
    OpenAI = None  # type: ignore

_openai_client: Optional["OpenAI"] = None


def _get_openai_client() -> "OpenAI":
    global _openai_client
    if _openai_client is not None:
        return _openai_client

    if OpenAI is None:
        raise RuntimeError("openai not installed")

    cfg = AIConfig.load()
    if not cfg.OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY not set")

    _openai_client = OpenAI(api_key=cfg.OPENAI_API_KEY)
    return _openai_client


def _embed_openai(texts: List[str]) -> EmbeddingBatch:
    cfg = AIConfig.load()
    client = _get_openai_client()
    response = client.embeddings.create(model=cfg.EMBEDDING_OPENAI_MODEL, input=texts)
    vectors = [list(map(float, d.embedding)) for d in response.data]
    return EmbeddingBatch(vectors=vectors, backend="openai")


def _choose_backend() -> EmbeddingBackendName:
    cfg = AIConfig.load()
    mode = (cfg.EMBEDDING_BACKEND or "auto").lower()

    if mode == "local":
        return "local"
    if mode == "openai":
        return "openai"

    # auto: local ê°€ëŠ¥í•˜ë©´ local
    if SentenceTransformer is not None:
        try:
            _get_local_model()
            return "local"
        except Exception:
            pass
    return "openai"


def get_embeddings(texts: List[str]) -> EmbeddingBatch:
    if not texts:
        return EmbeddingBatch(vectors=[], backend=_choose_backend())

    backend = _choose_backend()
    norm = [(t or "").strip() for t in texts]

    if backend == "local":
        return _embed_local(norm)
    return _embed_openai(norm)


==========================================================================================
# FILE: ai_worker/ai/handwriting/__init__.py
==========================================================================================
# apps/worker/ai/handwriting/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/handwriting/detector.py
==========================================================================================
# apps/worker/ai/handwriting/detector.py
from __future__ import annotations

from typing import Dict
import cv2  # type: ignore
import numpy as np  # type: ignore


def analyze_handwriting(image_path: str) -> Dict[str, float]:
    """
    legacy(doc_ai/handwriting/handwriting_detector.py) ì´ì‹ë³¸
    - í•œ ì´ë¯¸ì§€ì—ì„œ í•„ê¸° í”ì /ê³„ì‚°ì‹ í˜•íƒœ í”ì  ì—¬ë¶€ ì ìˆ˜ ë°˜í™˜

    return:
      {
        "writing_score": 0~1,
        "calculation_score": 0~1
      }
    """
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        return {"writing_score": 0.0, "calculation_score": 0.0}

    blur = cv2.GaussianBlur(img, (5, 5), 0)
    edges = cv2.Canny(blur, 50, 150)

    writing_density = float(np.sum(edges > 0) / edges.size)

    sobel_x = cv2.Sobel(blur, cv2.CV_64F, 1, 0, ksize=5)
    sobel_y = cv2.Sobel(blur, cv2.CV_64F, 0, 1, ksize=5)
    grad_mag = (np.mean(np.abs(sobel_x)) + np.mean(np.abs(sobel_y))) / 255.0

    writing_score = min(max(writing_density * 12.0, 0.0), 1.0)
    calculation_score = min(max(grad_mag * 3.0, 0.0), 1.0)

    return {
        "writing_score": float(writing_score),
        "calculation_score": float(calculation_score),
    }


==========================================================================================
# FILE: ai_worker/ai/homework/__init__.py
==========================================================================================



==========================================================================================
# FILE: ai_worker/ai/ocr/__init__.py
==========================================================================================
# apps/worker/ai/ocr/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/ocr/engine.py
==========================================================================================
from __future__ import annotations
from typing import List
from .schemas import OCRResultPayload, OMRDetectedAnswer


def run_ocr_engine(
    *,
    image_path: str,
) -> OCRResultPayload:
    """
    ì‹¤ì œ OCR/OMR ì—”ì§„ ìë¦¬
    - ì§€ê¸ˆì€ ë”ë¯¸
    - ë‚˜ì¤‘ì— OpenCV / Tesseract / ì™¸ë¶€ API êµì²´
    """

    answers: List[OMRDetectedAnswer] = [
        {
            "question_number": 1,
            "detected": ["B"],
            "confidence": 0.92,
            "marking": "single",
            "status": "ok",
        },
        {
            "question_number": 2,
            "detected": ["D"],
            "confidence": 0.88,
            "marking": "single",
            "status": "ok",
        },
    ]

    return {
        "version": "v1",
        "answers": answers,
        "raw_text": None,
    }


==========================================================================================
# FILE: ai_worker/ai/ocr/google.py
==========================================================================================
# apps/worker/ai/ocr/google.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Optional

# google cloud vision
from google.cloud import vision  # type: ignore


@dataclass
class OCRResult:
    text: str
    confidence: Optional[float] = None
    raw: Optional[Any] = None


def google_ocr(image_path: str) -> OCRResult:
    """
    Workerì—ì„œ ì‹¤í–‰ë˜ëŠ” Google OCR
    - service accountëŠ” GOOGLE_APPLICATION_CREDENTIALS ë˜ëŠ” ê¸°ë³¸ í™˜ê²½ì— ë”°ë¦„
    """
    client = vision.ImageAnnotatorClient()

    with open(image_path, "rb") as f:
        content = f.read()

    image = vision.Image(content=content)
    response = client.text_detection(image=image)

    if getattr(response, "error", None) and response.error.message:
        return OCRResult(text="", confidence=None, raw={"error": response.error.message})

    annotations = getattr(response, "text_annotations", None) or []
    if not annotations:
        return OCRResult(text="", confidence=None, raw=None)

    return OCRResult(
        text=annotations[0].description or "",
        confidence=None,
        raw=None,  # rawë¥¼ í†µì§¸ë¡œ ë„˜ê¸°ë©´ ì§ë ¬í™” ì´ìŠˆê°€ ìƒê¸¸ ìˆ˜ ìˆì–´ ê¸°ë³¸ None
    )


==========================================================================================
# FILE: ai_worker/ai/ocr/schemas.py
==========================================================================================
from __future__ import annotations
from typing import TypedDict, List, Optional


class OMRDetectedAnswer(TypedDict):
    question_number: int
    detected: List[str]
    confidence: float
    marking: str     # single / multi / blank
    status: str      # ok / error


class OCRResultPayload(TypedDict):
    version: str
    answers: List[OMRDetectedAnswer]
    raw_text: Optional[str]


==========================================================================================
# FILE: ai_worker/ai/ocr/tesseract.py
==========================================================================================
# apps/worker/ai/ocr/tesseract.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Optional

from PIL import Image  # type: ignore
import pytesseract  # type: ignore


@dataclass
class OCRResult:
    text: str
    confidence: Optional[float] = None
    raw: Optional[Any] = None


def tesseract_ocr(image_path: str) -> OCRResult:
    img = Image.open(image_path)

    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)
    text = "\n".join(data.get("text", [])).strip()

    confs = [c for c in data.get("conf", []) if c != -1]
    confidence = (sum(confs) / len(confs)) if confs else None

    # raw=data ëŠ” ë„ˆë¬´ í´ ìˆ˜ ìˆì–´ í•„ìš” ì‹œë§Œ ì¼œê¸°
    return OCRResult(text=text, confidence=confidence, raw=None)


==========================================================================================
# FILE: ai_worker/ai/omr/__init__.py
==========================================================================================



==========================================================================================
# FILE: ai_worker/ai/omr/engine.py
==========================================================================================
# apps/worker/ai/omr/engine.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore

from apps.worker.ai_worker.ai.omr.types import OMRAnswerV1


# ------------------------------------------------------------
# OMR v1 Simple Engine (CPU)
# - ROI(ë§ˆí‚¹ ì˜ì—­) ì´ë¯¸ì§€ì—ì„œ choiceë³„ fill ì ìˆ˜ë¥¼ ê³„ì‚°
# - WorkerëŠ” "íŒë‹¨/ì¶”ì¶œ"ë§Œ í•˜ê³ , ì •ë‹µ ë¹„êµ/ì ìˆ˜ ê³„ì‚°ì€ API(results)ì—ì„œ í•¨
#
# ì…ë ¥:
#   - image_path: ì‹œí—˜ì§€ ì „ì²´ ì´ë¯¸ì§€
#   - questions: [{question_id, roi: {x,y,w,h}, choices: ["A","B","C","D","E"]}, ...]
#   - threshold params
#
# ì¶œë ¥:
#   - answers: [OMRAnswerV1, ...]
# ------------------------------------------------------------

BBox = Tuple[int, int, int, int]


@dataclass(frozen=True)
class OMRConfigV1:
    # ì´ ê°’ë“¤ì€ v1 baseline. ìš´ì˜í•˜ë©´ì„œ ì¡°ì • ê°€ëŠ¥.
    # 0~1 fill ì ìˆ˜ (ì–´ë‘ìš´ í”½ì…€ ë¹„ìœ¨ ê¸°ë°˜)
    blank_threshold: float = 0.08      # ëª¨ë‘ ì•½í•˜ë©´ blank
    multi_threshold: float = 0.62      # 2ê°œ ì´ìƒì´ ì´ ì´ìƒì´ë©´ multi
    conf_gap_threshold: float = 0.08   # 1ë“±-2ë“± ì°¨ì´ê°€ ì´ë³´ë‹¤ ì‘ìœ¼ë©´ ambiguous
    low_confidence_threshold: float = 0.70  # API ì±„ì  ì •ì±…ê³¼ ë§ì¶”ê¸° ìœ„í•´ ë™ì¼ ê¸°ë³¸ê°’


def _safe_int(v: Any, default: int = 0) -> int:
    try:
        return int(v)
    except Exception:
        return default


def _crop(gray: np.ndarray, bbox: BBox) -> np.ndarray:
    x, y, w, h = bbox
    x = max(0, int(x))
    y = max(0, int(y))
    w = max(1, int(w))
    h = max(1, int(h))
    return gray[y:y + h, x:x + w]


def _fill_score(roi_gray: np.ndarray) -> float:
    """
    ROIì—ì„œ 'ì±„ì›Œì§(fill)' ì ìˆ˜ ê³„ì‚° (0~1)
    - ë§¤ìš° ë‹¨ìˆœí•œ v1: ë°ê¸° ì„ê³„ê°’ìœ¼ë¡œ ì–´ë‘ìš´ í”½ì…€ ë¹„ìœ¨
    """
    if roi_gray.size == 0:
        return 0.0

    # normalize
    blur = cv2.GaussianBlur(roi_gray, (5, 5), 0)

    # adaptive-ish: OTSU
    _, th = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    # th: ì±„ì›Œì§„ ë¶€ë¶„ì´ 255, ë°°ê²½ì´ 0 (INV)
    filled = float(np.sum(th > 0))
    total = float(th.size)
    if total <= 0:
        return 0.0

    score = filled / total
    # 0~1 clamp
    return float(max(0.0, min(1.0, score)))


def _split_choices_bbox(roi_bbox: BBox, n: int, axis: str = "x") -> List[BBox]:
    """
    ROI bboxë¥¼ në“±ë¶„í•´ì„œ choiceë³„ bboxë¥¼ ë§Œë“ ë‹¤.
    - axis="x": ê°€ë¡œë¡œ në¶„í•  (A B C D Eê°€ ê°€ë¡œë°°ì¹˜ì¸ ê²½ìš°)
    - axis="y": ì„¸ë¡œë¡œ në¶„í• 
    """
    x, y, w, h = roi_bbox
    boxes: List[BBox] = []
    if n <= 0:
        return boxes

    if axis == "y":
        step = h / float(n)
        for i in range(n):
            yy = y + int(round(i * step))
            hh = int(round(step))
            boxes.append((x, yy, w, max(1, hh)))
        return boxes

    # default x
    step = w / float(n)
    for i in range(n):
        xx = x + int(round(i * step))
        ww = int(round(step))
        boxes.append((xx, y, max(1, ww), h))
    return boxes


def detect_omr_answers_v1(
    *,
    image_path: str,
    questions: List[Dict[str, Any]],
    cfg: Optional[OMRConfigV1] = None,
) -> List[Dict[str, Any]]:
    """
    Worker entry: return list of dict payloads (each is OMRAnswerV1.to_dict()).
    """
    cfg = cfg or OMRConfigV1()

    img_bgr = cv2.imread(image_path)
    if img_bgr is None:
        # image load fail -> return error answers (best effort)
        out: List[Dict[str, Any]] = []
        for q in questions or []:
            qid = _safe_int(q.get("question_id"))
            out.append(
                OMRAnswerV1(
                    version="v1",
                    question_id=qid,
                    detected=[],
                    marking="blank",
                    confidence=0.0,
                    status="error",
                    raw={"error": "cannot read image"},
                ).to_dict()
            )
        return out

    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)

    results: List[Dict[str, Any]] = []

    for q in questions or []:
        qid = _safe_int(q.get("question_id"))
        roi = q.get("roi") or {}
        roi_bbox: BBox = (
            _safe_int(roi.get("x")),
            _safe_int(roi.get("y")),
            _safe_int(roi.get("w"), 1),
            _safe_int(roi.get("h"), 1),
        )
        choices = q.get("choices") or ["A", "B", "C", "D", "E"]
        axis = (q.get("axis") or "x").lower()

        n = len(choices)
        choice_boxes = _split_choices_bbox(roi_bbox, n=n, axis=axis)

        marks = []
        for idx, cb in enumerate(choice_boxes):
            roi_choice = _crop(gray, cb)
            fill = _fill_score(roi_choice)
            marks.append({"choice": str(choices[idx]), "fill": float(fill)})

        # sort by fill desc
        marks_sorted = sorted(marks, key=lambda m: m["fill"], reverse=True)
        top = marks_sorted[0] if marks_sorted else {"choice": "", "fill": 0.0}
        second = marks_sorted[1] if len(marks_sorted) > 1 else {"choice": "", "fill": 0.0}

        top_fill = float(top.get("fill") or 0.0)
        second_fill = float(second.get("fill") or 0.0)

        # blank íŒë‹¨
        if top_fill < cfg.blank_threshold:
            results.append(
                OMRAnswerV1(
                    version="v1",
                    question_id=qid,
                    detected=[],
                    marking="blank",
                    confidence=0.0,
                    status="blank",
                    raw={"marks": marks_sorted},
                ).to_dict()
            )
            continue

        # multi íŒë‹¨ (v1: 2ê°œ ì´ìƒì´ multi_threshold ì´ìƒì´ë©´ multi)
        high = [m for m in marks_sorted if float(m.get("fill") or 0.0) >= cfg.multi_threshold]
        if len(high) >= 2:
            detected = [str(m["choice"]) for m in high]
            results.append(
                OMRAnswerV1(
                    version="v1",
                    question_id=qid,
                    detected=detected,
                    marking="multi",
                    confidence=float(top_fill),
                    status="ambiguous",  # multiëŠ” ambiguousë¡œ ì²˜ë¦¬ (v1)
                    raw={"marks": marks_sorted},
                ).to_dict()
            )
            continue

        # ambiguous íŒë‹¨ (top-2 gapì´ ë„ˆë¬´ ì‘ìŒ)
        gap = top_fill - second_fill
        if gap < cfg.conf_gap_threshold:
            results.append(
                OMRAnswerV1(
                    version="v1",
                    question_id=qid,
                    detected=[str(top["choice"])],
                    marking="single",
                    confidence=float(top_fill),
                    status="ambiguous",
                    raw={"marks": marks_sorted, "gap": float(gap)},
                ).to_dict()
            )
            continue

        # ok / low_confidence
        status = "ok" if top_fill >= cfg.low_confidence_threshold else "low_confidence"

        results.append(
            OMRAnswerV1(
                version="v1",
                question_id=qid,
                detected=[str(top["choice"])],
                marking="single",
                confidence=float(top_fill),
                status=status,  # ok ë˜ëŠ” low_confidence
                raw={"marks": marks_sorted, "gap": float(gap)},
            ).to_dict()
        )

    return results


==========================================================================================
# FILE: ai_worker/ai/omr/types.py
==========================================================================================
# apps/worker/ai/omr/types.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Literal


OMRStatus = Literal["ok", "blank", "ambiguous", "low_confidence", "error"]
OMRMarking = Literal["blank", "single", "multi"]


@dataclass(frozen=True)
class OMRAnswerV1:
    """
    Worker-side OMR payload v1 (question-level)
    This should be embedded into API-side SubmissionAnswer.meta["omr"] later.

    - version: "v1" fixed
    - detected: list[str] ex) ["B"] or ["B","D"]
    - marking: blank/single/multi
    - confidence: 0~1 (top mark confidence)
    - status: ok/blank/ambiguous/low_confidence/error
    - raw: debug info (optional)
    """
    version: str
    question_id: int

    detected: List[str]
    marking: OMRMarking
    confidence: float
    status: OMRStatus

    raw: Optional[Dict[str, Any]] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "version": self.version,
            "question_id": int(self.question_id),
            "detected": list(self.detected or []),
            "marking": self.marking,
            "confidence": float(self.confidence or 0.0),
            "status": self.status,
            "raw": self.raw,
        }


==========================================================================================
# FILE: ai_worker/ai/pipelines/__init__.py
==========================================================================================
# apps/worker/ai/pipelines/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/pipelines/dispatcher.py
==========================================================================================
# apps/worker/ai/pipelines/dispatcher.py
from __future__ import annotations

from typing import Any, Dict

from apps.shared.contracts.ai_job import AIJob
from apps.shared.contracts.ai_result import AIResult

from apps.worker.ai_worker.ai.config import AIConfig
from apps.worker.ai_worker.ai.ocr.google import google_ocr
from apps.worker.ai_worker.ai.ocr.tesseract import tesseract_ocr
from apps.worker.ai_worker.ai.detection.segment_dispatcher import segment_questions
from apps.worker.ai_worker.ai.handwriting.detector import analyze_handwriting
from apps.worker.ai_worker.ai.embedding.service import get_embeddings
from apps.worker.ai_worker.ai.problem.generator import generate_problem_from_ocr
from apps.worker.ai_worker.ai.pipelines.homework_video_analyzer import analyze_homework_video
from apps.worker.storage.downloader import download_to_tmp


def handle_ai_job(job: AIJob) -> AIResult:
    """
    Worker-side single entrypoint (STEP 2 í™•ì •íŒ)

    ê·œì¹™:
    - workerëŠ” R2 credential ì—†ìŒ
    - payload["download_url"]ë§Œ ì‹ ë¢°
    - presigned URL â†’ /tmp ë‹¤ìš´ë¡œë“œ â†’ local_path ì‚¬ìš©
    """
    try:
        cfg = AIConfig.load()
        payload: Dict[str, Any] = job.payload or {}

        # ğŸ”¥ STEP 2 í•µì‹¬: presigned GET URL â†’ local file
        download_url = payload.get("download_url")
        if not download_url:
            return AIResult.failed(job.id, "download_url missing")

        local_path = download_to_tmp(
            download_url=download_url,
            job_id=str(job.id),
        )

        # --------------------------------------------------
        # OCR
        # --------------------------------------------------
        if job.type == "ocr":
            engine = (payload.get("engine") or cfg.OCR_ENGINE or "auto").lower()

            if engine == "tesseract":
                r = tesseract_ocr(local_path)
            elif engine == "google":
                r = google_ocr(local_path)
            else:
                # auto: google â†’ tesseract
                try:
                    r = google_ocr(local_path)
                    if not r.text.strip():
                        r = tesseract_ocr(local_path)
                except Exception:
                    r = tesseract_ocr(local_path)

            return AIResult.done(
                job.id,
                {"text": r.text, "confidence": r.confidence},
            )

        # --------------------------------------------------
        # Question segmentation
        # --------------------------------------------------
        if job.type == "question_segmentation":
            boxes = segment_questions(local_path)
            return AIResult.done(job.id, {"boxes": boxes})

        # --------------------------------------------------
        # Handwriting analysis
        # --------------------------------------------------
        if job.type == "handwriting_analysis":
            scores = analyze_handwriting(local_path)
            return AIResult.done(job.id, scores)

        # --------------------------------------------------
        # Embedding
        # --------------------------------------------------
        if job.type == "embedding":
            texts = payload.get("texts") or []
            batch = get_embeddings(list(texts))
            return AIResult.done(
                job.id,
                {"backend": batch.backend, "vectors": batch.vectors},
            )

        # --------------------------------------------------
        # Problem generation
        # --------------------------------------------------
        if job.type == "problem_generation":
            ocr_text = payload.get("ocr_text") or ""
            parsed = generate_problem_from_ocr(ocr_text)
            return AIResult.done(
                job.id,
                {
                    "body": parsed.body,
                    "choices": parsed.choices,
                    "answer": parsed.answer,
                    "difficulty": parsed.difficulty,
                    "tag": parsed.tag,
                    "summary": parsed.summary,
                    "explanation": parsed.explanation,
                },
            )

        # --------------------------------------------------
        # Homework video analysis
        # --------------------------------------------------
        if job.type == "homework_video_analysis":
            frame_stride = int(payload.get("frame_stride") or 10)
            min_frame_count = int(payload.get("min_frame_count") or 30)
            analysis = analyze_homework_video(
                video_path=local_path,
                frame_stride=frame_stride,
                min_frame_count=min_frame_count,
            )
            return AIResult.done(job.id, analysis)

        # --------------------------------------------------
        # OMR grading
        # --------------------------------------------------
        if job.type == "omr_grading":
            questions = payload.get("questions") or []
            from apps.worker.ai_worker.ai.omr.engine import detect_omr_answers_v1

            answers = detect_omr_answers_v1(
                image_path=local_path,
                questions=list(questions),
            )
            return AIResult.done(
                job.id,
                {
                    "version": "v1",
                    "answers": answers,
                },
            )

        return AIResult.failed(job.id, f"Unsupported job type: {job.type}")

    except Exception as e:
        return AIResult.failed(job.id, str(e))


==========================================================================================
# FILE: ai_worker/ai/pipelines/homework_video_analyzer.py
==========================================================================================
# apps/worker/ai/pipelines/homework_video_analyzer.py
from __future__ import annotations

from typing import Dict, Any, List
import cv2  # type: ignore
import numpy as np  # type: ignore


def _estimate_writing_score(gray_roi: np.ndarray) -> float:
    if gray_roi.size == 0:
        return 0.0
    dark = (gray_roi < 220).sum()
    total = gray_roi.size
    return float(dark) / float(total)


def analyze_homework_video(
    video_path: str,
    frame_stride: int = 10,
    min_frame_count: int = 30,
) -> Dict[str, Any]:
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"cannot open video: {video_path}")

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_idx = 0
    frame_results: List[Dict[str, Any]] = []

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        if frame_idx % frame_stride != 0:
            frame_idx += 1
            continue

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        score = _estimate_writing_score(gray)

        frame_results.append(
            {
                "index": frame_idx,
                "writing_score": round(float(score), 4),
                "has_writing": bool(score >= 0.05),
            }
        )
        frame_idx += 1

    cap.release()

    sampled = len(frame_results)
    if sampled == 0:
        return {
            "total_frames": total_frames,
            "sampled_frames": 0,
            "avg_writing_score": 0.0,
            "filled_ratio": 0.0,
            "frames": [],
            "too_short": total_frames < min_frame_count,
        }

    avg = sum(fr["writing_score"] for fr in frame_results) / sampled
    filled = sum(1 for fr in frame_results if fr["has_writing"])
    ratio = filled / sampled

    return {
        "total_frames": total_frames,
        "sampled_frames": sampled,
        "avg_writing_score": round(float(avg), 4),
        "filled_ratio": round(float(ratio), 4),
        "frames": frame_results,
        "too_short": total_frames < min_frame_count,
    }


==========================================================================================
# FILE: ai_worker/ai/problem/__init__.py
==========================================================================================
# apps/worker/ai/problem/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai_worker/ai/problem/generator.py
==========================================================================================
# apps/worker/ai/problem/generator.py
from __future__ import annotations

import json
from dataclasses import dataclass
from typing import Optional

from apps.worker.ai_worker.ai.config import AIConfig
from apps.worker.ai_worker.ai.problem.prompt import BASE_PROMPT

try:
    from openai import OpenAI  # type: ignore
except Exception:
    OpenAI = None  # type: ignore


@dataclass
class ParsedProblem:
    body: str
    choices: list
    answer: Optional[str]
    difficulty: int
    tag: str
    summary: str
    explanation: str


_client: Optional["OpenAI"] = None


def _get_client() -> "OpenAI":
    global _client
    if _client is not None:
        return _client

    if OpenAI is None:
        raise RuntimeError("openai not installed")

    cfg = AIConfig.load()
    if not cfg.OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY not set")

    _client = OpenAI(api_key=cfg.OPENAI_API_KEY)
    return _client


def generate_problem_from_ocr(ocr_text: str) -> ParsedProblem:
    cfg = AIConfig.load()
    prompt = BASE_PROMPT.format(ocr_text=ocr_text)

    client = _get_client()
    response = client.chat.completions.create(
        model=cfg.PROBLEM_GEN_MODEL,
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ êµìœ¡ìš© ì‹œí—˜ ë¬¸ì œë¥¼ ìë™ ìƒì„±í•˜ëŠ” ì—”ì§„ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.2,
    )

    # SDK í˜•íƒœ ì°¨ì´ ë°©ì–´
    msg = response.choices[0].message
    content = getattr(msg, "content", None) or msg.get("content")  # type: ignore

    data = json.loads(content)

    return ParsedProblem(
        body=data.get("body", ""),
        choices=data.get("choices", []),
        answer=data.get("answer"),
        difficulty=int(data.get("difficulty", 3)),
        tag=data.get("tag", ""),
        summary=data.get("summary", ""),
        explanation=data.get("explanation", ""),
    )


==========================================================================================
# FILE: ai_worker/ai/problem/prompt.py
==========================================================================================
# apps/worker/ai/problem/prompt.py
BASE_PROMPT = """
ë‹¤ìŒì€ ì‹œí—˜ ë¬¸ì œì˜ OCR ê²°ê³¼ì…ë‹ˆë‹¤.
ì•„ë˜ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì œ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1) ë¬¸ì œ ë³¸ë¬¸ (body)
2) ì„ íƒì§€ (choices): ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´
3) ì •ë‹µ (answer): ëª…ì‹œëœ ì •ë‹µì´ ì—†ìœ¼ë©´ AIê°€ ì¶”ë¡ , ì¶”ë¡  ë¶ˆê°€ ì‹œ null
4) ë‚œì´ë„ (difficulty): 1~5 ì •ìˆ˜ë¡œ ì¶”ì •
5) íƒœê·¸ (tag): ìˆ˜í•™/ê³¼í•™/êµ­ì–´ ë“± ê°„ë‹¨í•œ ë¶„ë¥˜
6) ë¬¸ì œ ìš”ì•½ (summary)
7) í•´ì„¤ (explanation): ê°„ë‹¨ëª…ë£Œí•˜ê²Œ

ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON í˜•ì‹ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:
{
  "body": "...",
  "choices": ["A...", "B...", "C...", "D..."],
  "answer": "C",
  "difficulty": 3,
  "tag": "ìˆ˜í•™",
  "summary": "...",
  "explanation": "..."
}

OCR í…ìŠ¤íŠ¸:
\"\"\"
{ocr_text}
\"\"\"
"""


==========================================================================================
# FILE: ai_worker/storage/__init__.py
==========================================================================================



==========================================================================================
# FILE: ai_worker/storage/downloader.py
==========================================================================================
# ==============================================================================
# PATH: apps/worker/storage/downloader.py
#
# PURPOSE:
# - AI worker ì „ìš© íŒŒì¼ ë‹¤ìš´ë¡œë“œ ìœ í‹¸
# - presigned GET URL â†’ /tmp local file
# - R2 / S3 credential ì‚¬ìš© âŒ
# - video_worker ì½”ë“œì™€ ì ˆëŒ€ ê³µìœ í•˜ì§€ ì•ŠìŒ
# ==============================================================================

from __future__ import annotations

import os
import tempfile
import requests
from pathlib import Path


def download_to_tmp(
    *,
    download_url: str,
    job_id: str,
    suffix: str | None = None,
    timeout: int = 60,
    chunk_size: int = 1024 * 1024,  # 1MB
) -> str:
    """
    presigned GET URLë¡œ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ local temp file ê²½ë¡œ ë°˜í™˜

    ê·œì¹™:
    - AI workerëŠ” URLë§Œ ì‹ ë¢°
    - íŒŒì¼ì€ /tmp ë˜ëŠ” OS temp dirì— ìƒì„±
    - í˜¸ì¶œìëŠ” ë°˜í™˜ëœ pathë§Œ ì‚¬ìš©
    """

    tmp_dir = Path(tempfile.gettempdir())
    ext = suffix or ""

    filename = f"ai_job_{job_id}{ext}"
    tmp_path = tmp_dir / filename
    part_path = tmp_dir / f"{filename}.part"

    try:
        with requests.get(download_url, stream=True, timeout=timeout) as r:
            r.raise_for_status()

            expected_len = r.headers.get("Content-Length")
            expected_len = int(expected_len) if expected_len and expected_len.isdigit() else None

            written = 0
            with open(part_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=chunk_size):
                    if not chunk:
                        continue
                    f.write(chunk)
                    written += len(chunk)

            if expected_len is not None and written != expected_len:
                raise RuntimeError(
                    f"download size mismatch (expected={expected_len}, got={written})"
                )

        part_path.replace(tmp_path)
        return str(tmp_path)

    except Exception:
        try:
            if part_path.exists():
                part_path.unlink()
        except Exception:
            pass
        raise


==========================================================================================
# FILE: ai_worker/wrong_notes/__init__.py
==========================================================================================
# apps/worker/wrong_notes/__init__.py


==========================================================================================
# FILE: ai_worker/wrong_notes/templates/wrong_note.html
==========================================================================================
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="utf-8" />
    <style>
        body {
            font-family: Arial, Helvetica, sans-serif;
            font-size: 12px;
            line-height: 1.6;
        }
        h1 {
            border-bottom: 2px solid #333;
            padding-bottom: 4px;
        }
        .exam {
            margin-top: 20px;
        }
        .question {
            margin-left: 12px;
        }
        .wrong {
            color: #c0392b;
        }
    </style>
</head>
<body>
    <h1>ì˜¤ë‹µ ë…¸íŠ¸</h1>

    <p>
        í•™ìƒ ID: {{ enrollment_id }}<br/>
        ìƒì„±ì¼: {{ created_at }}
    </p>

    {% for exam_id, items in grouped.items %}
        <div class="exam">
            <h2>ì‹œí—˜ {{ exam_id }}</h2>
            {% for item in items %}
                <div class="question">
                    <span class="wrong">
                        Q{{ item.question_id }}
                    </span>
                    / ì œì¶œ ë‹µì•ˆ: {{ item.answer }}
                </div>
            {% endfor %}
        </div>
    {% endfor %}
</body>
</html>


==========================================================================================
# FILE: messaging/__init__.py
==========================================================================================



==========================================================================================
# FILE: messaging/kakao/__init__.py
==========================================================================================



==========================================================================================
# FILE: messaging/pipelines/__init__.py
==========================================================================================



==========================================================================================
# FILE: messaging/sms/__init__.py
==========================================================================================



==========================================================================================
# FILE: video_worker/__init__.py
==========================================================================================



==========================================================================================
# FILE: video_worker/r2_uploader.py
==========================================================================================
# apps/worker/media/r2_uploader.py

import boto3
import mimetypes
from pathlib import Path
from django.conf import settings

# ---------------------------------------------------------------------
# R2 S3 Client (ê³µìš©)
# ---------------------------------------------------------------------

s3 = boto3.client(
    "s3",
    endpoint_url=settings.R2_ENDPOINT,
    aws_access_key_id=settings.R2_ACCESS_KEY,
    aws_secret_access_key=settings.R2_SECRET_KEY,
    region_name="auto",
)

# ---------------------------------------------------------------------
# Upload helpers (ê¸°ì¡´)
# ---------------------------------------------------------------------

def upload_fileobj_to_r2(*, fileobj, key: str, content_type: str | None = None):
    """
    Django UploadedFile -> R2 ì—…ë¡œë“œ
    """
    s3.upload_fileobj(
        Fileobj=fileobj,
        Bucket=settings.R2_BUCKET,
        Key=key,
        ExtraArgs={
            "ContentType": content_type or "application/octet-stream"
        },
    )


def upload_dir(local_dir: Path, prefix: str):
    """
    local_dir ì „ì²´ë¥¼ prefix ê¸°ì¤€ìœ¼ë¡œ R2ì— ì—…ë¡œë“œ
    """
    for path in local_dir.rglob("*"):
        if not path.is_file():
            continue

        relative_path = path.relative_to(local_dir).as_posix()
        key = f"{prefix}/{relative_path}"

        content_type, _ = mimetypes.guess_type(path.name)

        s3.upload_file(
            str(path),
            settings.R2_BUCKET,
            key,
            ExtraArgs={
                "ContentType": content_type or "application/octet-stream"
            },
        )

# ---------------------------------------------------------------------
# â­ STEP 2 í•µì‹¬: presigned GET URL ìƒì„±
# ---------------------------------------------------------------------

def generate_presigned_get_url(
    *,
    key: str,
    expires_in: int = 3600,
) -> str:
    """
    R2 objectë¥¼ ì„ì‹œë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆëŠ” presigned GET URL ìƒì„±

    - API ì„œë²„ì—ì„œë§Œ í˜¸ì¶œ
    - workerëŠ” ì´ URLë§Œ ì‚¬ìš© (R2 credential ë¶ˆí•„ìš”)
    """
    return s3.generate_presigned_url(
        ClientMethod="get_object",
        Params={
            "Bucket": settings.R2_BUCKET,
            "Key": key,
        },
        ExpiresIn=expires_in,
    )


==========================================================================================
# FILE: video_worker/drm/__init__.py
==========================================================================================



==========================================================================================
# FILE: video_worker/video/__init__.py
==========================================================================================



==========================================================================================
# FILE: video_worker/video/processor.py
==========================================================================================
# apps/worker/media/video/processor.py

from __future__ import annotations

import json
import shutil
import subprocess
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Optional, Union

import requests

from .transcoder import transcode_to_hls


# ---------------------------------------------------------------------
# Errors (processor -> task boundary)
# ---------------------------------------------------------------------

class MediaProcessingError(RuntimeError):
    """
    Processor-level error with structured context.
    Task layer should catch this and mark Video as FAILED.
    """

    def __init__(
        self,
        *,
        stage: str,
        code: str,
        message: str,
        context: Optional[Dict[str, Any]] = None,
    ) -> None:
        super().__init__(message)
        self.stage = stage
        self.code = code
        self.context = context or {}

    def to_dict(self) -> Dict[str, Any]:
        return {
            "stage": self.stage,
            "code": self.code,
            "message": str(self),
            "context": self.context,
        }


# ---------------------------------------------------------------------
# Result contract (processor -> task)
# ---------------------------------------------------------------------

@dataclass(frozen=True)
class ProcessResult:
    video_id: int
    duration_seconds: float
    thumbnail_path: Path
    master_playlist_path: Path
    output_root: Path


# ---------------------------------------------------------------------
# Processor (single responsibility: make one video READY)
# ---------------------------------------------------------------------

class VideoProcessor:
    """
    The single authority that turns one uploaded video into READY artifacts:
      - download source to local
      - duration (ffprobe local)
      - thumbnail (ffmpeg local)
      - HLS (transcoder.transcode_to_hls local)
      - verification (master.m3u8 exists)
    """

    def run(
        self,
        *,
        video_id: int,
        input_url: str,
        output_root: Union[str, Path],
        timeout_download: Optional[int] = 60 * 30,
        timeout_probe: Optional[int] = 60,
        timeout_thumbnail: Optional[int] = 120,
        timeout_hls: Optional[int] = None,
        cleanup_source: bool = False,
    ) -> ProcessResult:
        out_root = Path(output_root)

        # 1) pre-clean
        self._pre_clean_output_root(
            video_id=video_id,
            output_root=out_root,
        )

        # 2) download source to local (ì •ì„)
        local_input_path = self._download_source(
            video_id=video_id,
            input_url=input_url,
            output_root=out_root,
            timeout=timeout_download,
        )

        try:
            # 3) probe duration (local)
            duration = self._probe_duration_seconds(
                video_id=video_id,
                input_path=str(local_input_path),
                timeout=timeout_probe,
            )

            # 4) thumbnail (local)
            thumb_path = out_root / "thumbnail.jpg"
            self._generate_thumbnail(
                video_id=video_id,
                input_path=str(local_input_path),
                output_path=thumb_path,
                timeout=timeout_thumbnail,
            )

            # 5) HLS transcode (local)
            master_path = self._transcode_hls(
                video_id=video_id,
                input_path=str(local_input_path),
                output_root=out_root, 
                timeout=timeout_hls,
            )

            # 6) verify
            self._verify_ready(
                video_id=video_id,
                master_path=master_path,
            )

            return ProcessResult(
                video_id=video_id,
                duration_seconds=duration,
                thumbnail_path=thumb_path,
                master_playlist_path=master_path,
                output_root=out_root,
            )

        finally:
            if cleanup_source:
                try:
                    if local_input_path.exists():
                        local_input_path.unlink()
                except Exception:
                    pass

    # -----------------------------------------------------------------
    # Internal steps
    # -----------------------------------------------------------------

    def _pre_clean_output_root(self, *, video_id: int, output_root: Path) -> None:
        """
        Delete output_root entirely if exists, then recreate.
        Prevents stale artifacts from making retries look successful.
        """
        try:
            if output_root.exists():
                shutil.rmtree(output_root)
            output_root.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            raise MediaProcessingError(
                stage="CLEAN",
                code="CLEAN_FAILED",
                message=f"Failed to pre-clean output_root (video_id={video_id})",
                context={
                    "video_id": video_id,
                    "output_root": str(output_root),
                    "error": repr(e),
                },
            ) from e

    def _download_source(
        self,
        *,
        video_id: int,
        input_url: str,
        output_root: Path,
        timeout: Optional[int] = 60 * 30,  # 30m
        chunk_size: int = 1024 * 1024,      # 1MB
    ) -> Path:
        """
        Download source MP4 from presigned GET URL to local disk.
        """
        output_root.mkdir(parents=True, exist_ok=True)

        final_path = output_root / "_source.mp4"
        tmp_path = output_root / "_source.mp4.part"

        try:
            with requests.get(input_url, stream=True, timeout=30) as r:
                r.raise_for_status()

                expected_len = r.headers.get("Content-Length")
                expected_len_int = int(expected_len) if expected_len and expected_len.isdigit() else None

                written = 0
                with open(tmp_path, "wb") as f:
                    for chunk in r.iter_content(chunk_size=chunk_size):
                        if not chunk:
                            continue
                        f.write(chunk)
                        written += len(chunk)

                if expected_len_int is not None and written != expected_len_int:
                    raise RuntimeError(
                        f"download size mismatch (expected={expected_len_int}, got={written})"
                    )

            tmp_path.replace(final_path)
            return final_path

        except Exception as e:
            try:
                if tmp_path.exists():
                    tmp_path.unlink()
            except Exception:
                pass

            raise MediaProcessingError(
                stage="DOWNLOAD",
                code="DOWNLOAD_FAILED",
                message=f"Failed to download source (video_id={video_id})",
                context={
                    "video_id": video_id,
                    "input_url": input_url[:2000],
                    "output_root": str(output_root),
                    "error": repr(e),
                },
            ) from e

    def _probe_duration_seconds(
        self,
        *,
        video_id: int,
        input_path: str,
        timeout: Optional[int],
    ) -> float:
        """
        ffprobe local file.
        """
        cmd = [
            "ffprobe",
            "-v", "error",
            "-print_format", "json",
            "-show_format",
            "-show_streams",
            input_path,
        ]

        try:
            p = subprocess.run(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                timeout=timeout,
                check=False,
            )
        except Exception as e:
            raise MediaProcessingError(
                stage="PROBE",
                code="PROBE_FAILED",
                message=f"ffprobe execution failed (video_id={video_id})",
                context={
                    "video_id": video_id,
                    "cmd": cmd,
                    "error": repr(e),
                },
            ) from e

        if p.returncode != 0:
            raise MediaProcessingError(
                stage="PROBE",
                code="PROBE_FAILED",
                message=f"ffprobe returned non-zero (video_id={video_id})",
                context={
                    "video_id": video_id,
                    "cmd": cmd,
                    "returncode": p.returncode,
                    "stderr": p.stderr,
                },
            )

        try:
            data = json.loads(p.stdout)
            fmt = data.get("format") or {}
            duration_str = fmt.get("duration")
            duration = float(duration_str) if duration_str is not None else 0.0
        except Exception as e:
            raise MediaProcessingError(
                stage="PROBE",
                code="PROBE_FAILED",
                message=f"Failed to parse ffprobe output (video_id={video_id})",
                context={
                    "video_id": video_id,
                    "cmd": cmd,
                    "stdout": p.stdout[:4000],
                    "error": repr(e),
                },
            ) from e

        if duration <= 0:
            raise MediaProcessingError(
                stage="PROBE",
                code="PROBE_FAILED",
                message=f"Invalid duration from ffprobe (video_id={video_id}, duration={duration})",
                context={
                    "video_id": video_id,
                    "cmd": cmd,
                    "duration": duration,
                },
            )

        return duration

    def _generate_thumbnail(
        self,
        *,
        video_id: int,
        input_path: str,
        output_path: Path,
        timeout: Optional[int],
    ) -> None:
        """
        Generate one thumbnail jpg from local file.
        """
        output_path.parent.mkdir(parents=True, exist_ok=True)

        cmd = [
            "ffmpeg",
            "-y",
            "-i", input_path,
            "-vf", "thumbnail,scale=1280:-2",
            "-frames:v", "1",
            str(output_path),
        ]

        try:
            p = subprocess.run(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                timeout=timeout,
                check=False,
            )
        except Exception as e:
            raise MediaProcessingError(
                stage="THUMBNAIL",
                code="THUMBNAIL_FAILED",
                message=f"ffmpeg thumbnail execution failed (video_id={video_id})",
                context={
                    "video_id": video_id,
                    "cmd": cmd,
                    "error": repr(e),
                },
            ) from e

        if p.returncode != 0:
            raise MediaProcessingError(
                stage="THUMBNAIL",
                code="THUMBNAIL_FAILED",
                message=f"ffmpeg thumbnail returned non-zero (video_id={video_id})",
                context={
                    "video_id": video_id,
                    "cmd": cmd,
                    "returncode": p.returncode,
                    "stderr": p.stderr,
                },
            )

        if not output_path.exists():
            raise MediaProcessingError(
                stage="THUMBNAIL",
                code="THUMBNAIL_FAILED",
                message=f"thumbnail file not created (video_id={video_id})",
                context={
                    "video_id": video_id,
                    "output_path": str(output_path),
                },
            )

    def _transcode_hls(
        self,
        *,
        video_id: int,
        input_path: str,
        output_root: Path, 
        timeout: Optional[int],
    ) -> Path:
        """
        Delegate to transcoder (local input).
        """
        try:
            master_path = transcode_to_hls(
                video_id=video_id,
                input_path=input_path,
                output_root=output_root, 
                timeout=timeout,
            )
            return master_path
        except Exception as e:
            raise MediaProcessingError(
                stage="HLS",
                code="HLS_FAILED",
                message=f"HLS transcode failed (video_id={video_id})",
                context={
                    "video_id": video_id,
                    "input_path": input_path,
                    "error": repr(e),
                },
            ) from e

    def _verify_ready(self, *, video_id: int, master_path: Path) -> None:
        """
        READY criterion is strictly: master.m3u8 exists.
        """
        if not master_path.exists():
            raise MediaProcessingError(
                stage="VERIFY",
                code="OUTPUT_INVALID",
                message=f"READY criterion failed: master.m3u8 missing (video_id={video_id})",
                context={
                    "video_id": video_id,
                    "master_path": str(master_path),
                },
            )


# ---------------------------------------------------------------------
# Public entry
# ---------------------------------------------------------------------

def run(
    *,
    video_id: int,
    input_url: str,
    output_root: Union[str, Path],
    timeout_download: Optional[int] = 60 * 30,
    timeout_probe: Optional[int] = 60,
    timeout_thumbnail: Optional[int] = 120,
    timeout_hls: Optional[int] = None,
) -> ProcessResult:
    return VideoProcessor().run(
        video_id=video_id,
        input_url=input_url,
        output_root=output_root,
        timeout_download=timeout_download,
        timeout_probe=timeout_probe,
        timeout_thumbnail=timeout_thumbnail,
        timeout_hls=timeout_hls,
    )


==========================================================================================
# FILE: video_worker/video/transcoder.py
==========================================================================================
from __future__ import annotations

import subprocess
from pathlib import Path
from typing import List


# ---------------------------------------------------------------------
# HLS Variant Ladder
# ---------------------------------------------------------------------
# âš ï¸ nameì—ëŠ” ì ˆëŒ€ 'v' ë¶™ì´ì§€ ë§ ê²ƒ
HLS_VARIANTS = [
    {"name": "1", "width": 426,  "height": 240,  "video_bitrate": "400k",  "audio_bitrate": "64k"},
    {"name": "2", "width": 640,  "height": 360,  "video_bitrate": "800k",  "audio_bitrate": "96k"},
    {"name": "3", "width": 1280, "height": 720,  "video_bitrate": "2500k", "audio_bitrate": "128k"},
]


# ---------------------------------------------------------------------
# Directory preparation
# ---------------------------------------------------------------------
def prepare_output_dirs(output_root: Path) -> None:
    """
    storage/media/hls/videos/{video_id}/
      â”œâ”€ master.m3u8
      â”œâ”€ v1/
      â”‚   â”œâ”€ index.m3u8
      â”‚   â””â”€ index0.ts ...
      â”œâ”€ v2/
      â””â”€ v3/
    """
    output_root.mkdir(parents=True, exist_ok=True)

    # v1, v2, v3 ë””ë ‰í† ë¦¬ ë¯¸ë¦¬ ìƒì„±
    for v in HLS_VARIANTS:
        (output_root / f"v{v['name']}").mkdir(parents=True, exist_ok=True)


# ---------------------------------------------------------------------
# ffmpeg filter_complex builder
# ---------------------------------------------------------------------
def build_filter_complex() -> str:
    parts: List[str] = []
    split_count = len(HLS_VARIANTS)

    parts.append(
        "[0:v]split={}".format(split_count)
        + "".join(f"[v{i}]" for i in range(split_count))
    )

    for i, v in enumerate(HLS_VARIANTS):
        parts.append(f"[v{i}]scale={v['width']}:{v['height']}[v{i}out]")

    return ";".join(parts)


# ---------------------------------------------------------------------
# ffmpeg command builder (â­ í‘œì¤€í˜•)
# ---------------------------------------------------------------------
def build_ffmpeg_command(input_path: str) -> List[str]:
    """
    âš ï¸ ëª¨ë“  ì¶œë ¥ ê²½ë¡œëŠ” 'ìƒëŒ€ ê²½ë¡œ'ë§Œ ì‚¬ìš©
    âš ï¸ cwd ê¸°ì¤€ìœ¼ë¡œ ffmpeg ì‹¤í–‰ë¨
    """
    cmd: List[str] = [
        "ffmpeg",
        "-y",
        "-i", input_path,  # ì…ë ¥ì€ ì ˆëŒ€ ê²½ë¡œì—¬ë„ OK
        "-filter_complex", build_filter_complex(),
    ]

    for i, v in enumerate(HLS_VARIANTS):
        cmd += [
            "-map", f"[v{i}out]",
            "-map", "0:a?",

            f"-c:v:{i}", "libx264",
            "-profile:v", "main",
            "-pix_fmt", "yuv420p",
            f"-b:v:{i}", v["video_bitrate"],

            "-g", "48",
            "-keyint_min", "48",
            "-sc_threshold", "0",

            f"-c:a:{i}", "aac",
            "-ac", "2",
            f"-b:a:{i}", v["audio_bitrate"],
        ]

    cmd += [
        "-f", "hls",
        "-hls_time", "4",
        "-hls_playlist_type", "vod",
        "-hls_flags", "independent_segments",

        # âœ… í‘œì¤€: ìƒëŒ€ ê²½ë¡œ + POSIX ìŠ¬ë˜ì‹œ
        "-hls_segment_filename", "v%v/index%d.ts",
        "-master_pl_name", "master.m3u8",

        "-var_stream_map",
        " ".join(
            f"v:{i},a:{i},name:{v['name']}"
            for i, v in enumerate(HLS_VARIANTS)
        ),

        # âœ… variant playlistë„ ìƒëŒ€ ê²½ë¡œ
        "v%v/index.m3u8",
    ]

    return cmd


# ---------------------------------------------------------------------
# Public API (â­ í‘œì¤€í˜• ì‹¤í–‰ë¶€)
# ---------------------------------------------------------------------
def transcode_to_hls(
    *,
    video_id: int,
    input_path: str,
    output_root: Path,
    timeout: int | None = None,
) -> Path:
    """
    Execute HLS transcoding from local mp4 (Best Practice)
    """

    # 1. ì¶œë ¥ ë””ë ‰í† ë¦¬ ì¤€ë¹„
    prepare_output_dirs(output_root)

    # 2. ffmpeg ëª…ë ¹ì–´ ìƒì„±
    cmd = build_ffmpeg_command(input_path)

    # 3. â­ í•µì‹¬: output_rootë¥¼ cwdë¡œ ì‹¤í–‰
    process = subprocess.run(
        cmd,
        cwd=str(output_root.resolve()),
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
        timeout=timeout,
        check=False,
    )

    if process.returncode != 0:
        raise RuntimeError({
            "video_id": video_id,
            "cmd": cmd,
            "stderr": process.stderr,
        })

    master_path = output_root / "master.m3u8"
    if not master_path.exists():
        raise RuntimeError("master.m3u8 not created")

    return master_path
