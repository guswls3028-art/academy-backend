====================================================================================================
# BACKEND APP: ai_worker
# ROOT PATH: C:\academy\apps\worker\ai_worker
====================================================================================================


==========================================================================================
# FILE: __init__.py
==========================================================================================



==========================================================================================
# FILE: celery.py
==========================================================================================
# PATH: apps/worker/ai_worker/celery.py
"""
This project is queue-less.

Kept only to avoid breaking imports in legacy scripts.
Do not add any queue/task dependencies here.
"""
from __future__ import annotations

__all__ = ["get_app"]


def get_app():
    return None


==========================================================================================
# FILE: run.py
==========================================================================================
# PATH: apps/worker/ai_worker/run.py
from __future__ import annotations

import os
import sys
import time
import signal
import logging
import random
import requests

from apps.shared.contracts.ai_job import AIJob
from apps.shared.contracts.ai_result import AIResult
from apps.worker.ai_worker.ai.pipelines.dispatcher import handle_ai_job

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] [AI-WORKER] %(message)s",
)
logger = logging.getLogger(__name__)

API_BASE_URL = os.getenv("API_BASE_URL", "http://localhost:8000")
INTERNAL_WORKER_TOKEN = os.getenv("INTERNAL_WORKER_TOKEN", "long-random-secret")
POLL_INTERVAL_SEC = float(os.getenv("AI_WORKER_POLL_INTERVAL", "1.0"))
MAX_BACKOFF = float(os.getenv("AI_WORKER_MAX_BACKOFF", "30.0"))

_running = True


def _shutdown(signum, frame):
    global _running
    logger.warning("Shutdown signal received (%s)", signum)
    _running = False


signal.signal(signal.SIGINT, _shutdown)
signal.signal(signal.SIGTERM, _shutdown)


def _headers() -> dict:
    return {
        "X-Worker-Token": INTERNAL_WORKER_TOKEN,
        "X-Worker-Id": os.getenv("HOSTNAME", "ai-worker"),
    }


def fetch_job() -> AIJob | None:
    url = f"{API_BASE_URL.rstrip('/')}/api/v1/internal/ai/job/next/"
    resp = requests.get(url, headers=_headers(), timeout=10)
    resp.raise_for_status()

    data = resp.json()
    job_data = data.get("job")
    if not job_data:
        return None

    return AIJob.from_dict(job_data)


def submit_result(*, result: AIResult, job: AIJob) -> None:
    url = f"{API_BASE_URL.rstrip('/')}/api/v1/internal/ai/job/result/"
    headers = _headers()
    headers["Content-Type"] = "application/json"

    submission_id = None
    try:
        if job.source_id is not None and str(job.source_id).isdigit():
            submission_id = int(str(job.source_id))
    except Exception:
        submission_id = None

    payload = {
        "job_id": job.id,
        "submission_id": submission_id,
        "status": result.status,
        "result": result.result,
        "error": result.error,
    }

    resp = requests.post(url, json=payload, headers=headers, timeout=20)
    resp.raise_for_status()


def main():
    logger.info("AI Worker started (API_BASE_URL=%s)", API_BASE_URL)

    backoff = 0.0

    while _running:
        try:
            job = fetch_job()
            if job is None:
                time.sleep(POLL_INTERVAL_SEC)
                backoff = 0.0
                continue

            logger.info("Job received: id=%s type=%s", job.id, job.type)

            result = handle_ai_job(job)

            submit_result(result=result, job=job)

            logger.info("Job finished: id=%s status=%s", job.id, result.status)
            backoff = 0.0

        except requests.HTTPError as e:
            code = getattr(e.response, "status_code", None)
            logger.exception("Worker loop HTTP error (status=%s)", code)
            backoff = min(MAX_BACKOFF, backoff * 2.0 + 1.0) if backoff > 0 else 1.0
            time.sleep(backoff + random.random())

        except Exception:
            logger.exception("Worker loop error")
            backoff = min(MAX_BACKOFF, backoff * 2.0 + 1.0) if backoff > 0 else 1.0
            time.sleep(backoff + random.random())

    logger.info("AI Worker shutdown complete")


if __name__ == "__main__":
    sys.exit(main())


==========================================================================================
# FILE: ai/__init__.py
==========================================================================================
# apps/worker/ai/__init__.py
# Celery 제거됨. run.py 단일 진입


==========================================================================================
# FILE: ai/config.py
==========================================================================================
# apps/worker/ai/config.py
from __future__ import annotations

import os
from dataclasses import dataclass
from typing import List, Optional


def _env(name: str, default: Optional[str] = None) -> Optional[str]:
    v = os.getenv(name)
    return v if v not in (None, "") else default


@dataclass(frozen=True)
class AIConfig:
    # OCR
    OCR_ENGINE: str = "google"  # google | tesseract | auto
    GOOGLE_APPLICATION_CREDENTIALS: Optional[str] = None  # optional (google sdk default)

    # Segmentation
    QUESTION_SEGMENTATION_ENGINE: str = "auto"  # yolo|opencv|template|auto

    # YOLO (optional)
    YOLO_QUESTION_MODEL_PATH: Optional[str] = None
    YOLO_QUESTION_INPUT_SIZE: int = 640
    YOLO_QUESTION_CONF_THRESHOLD: float = 0.4
    YOLO_QUESTION_IOU_THRESHOLD: float = 0.5

    # Embedding
    EMBEDDING_BACKEND: str = "auto"  # local|openai|auto
    EMBEDDING_LOCAL_MODEL: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    EMBEDDING_OPENAI_MODEL: str = "text-embedding-3-small"
    OPENAI_API_KEY: Optional[str] = None

    # Problem generation
    PROBLEM_GEN_MODEL: str = "gpt-4.1-mini"  # default

    @staticmethod
    def load() -> "AIConfig":
        return AIConfig(
            OCR_ENGINE=_env("OCR_ENGINE", "google") or "google",
            GOOGLE_APPLICATION_CREDENTIALS=_env("GOOGLE_APPLICATION_CREDENTIALS"),

            QUESTION_SEGMENTATION_ENGINE=_env("QUESTION_SEGMENTATION_ENGINE", "auto") or "auto",

            YOLO_QUESTION_MODEL_PATH=_env("YOLO_QUESTION_MODEL_PATH"),
            YOLO_QUESTION_INPUT_SIZE=int(_env("YOLO_QUESTION_INPUT_SIZE", "640") or "640"),
            YOLO_QUESTION_CONF_THRESHOLD=float(_env("YOLO_QUESTION_CONF_THRESHOLD", "0.4") or "0.4"),
            YOLO_QUESTION_IOU_THRESHOLD=float(_env("YOLO_QUESTION_IOU_THRESHOLD", "0.5") or "0.5"),

            EMBEDDING_BACKEND=_env("EMBEDDING_BACKEND", "auto") or "auto",
            EMBEDDING_LOCAL_MODEL=_env(
                "EMBEDDING_LOCAL_MODEL",
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            ) or "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            EMBEDDING_OPENAI_MODEL=_env("EMBEDDING_OPENAI_MODEL", "text-embedding-3-small") or "text-embedding-3-small",
            OPENAI_API_KEY=_env("OPENAI_API_KEY") or _env("EMBEDDING_OPENAI_API_KEY"),

            PROBLEM_GEN_MODEL=_env("PROBLEM_GEN_MODEL", "gpt-4.1-mini") or "gpt-4.1-mini",
        )


==========================================================================================
# FILE: ai/detection/__init__.py
==========================================================================================
# apps/worker/ai/detection/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai/detection/segment_dispatcher.py
==========================================================================================
# apps/worker/ai/detection/segment_dispatcher.py
from __future__ import annotations

from typing import List, Tuple

from apps.worker.ai_worker.ai.config import AIConfig
from apps.worker.ai_worker.ai.detection.segment_opencv import segment_questions_opencv
from apps.worker.ai_worker.ai.detection.segment_yolo import segment_questions_yolo

BBox = Tuple[int, int, int, int]


def segment_questions(image_path: str) -> List[BBox]:
    """
    worker-side segmentation single entrypoint
    """
    cfg = AIConfig.load()
    engine = (cfg.QUESTION_SEGMENTATION_ENGINE or "auto").lower()

    if engine == "opencv":
        return segment_questions_opencv(image_path)
    if engine == "yolo":
        return segment_questions_yolo(image_path)

    # auto: yolo -> opencv
    try:
        boxes = segment_questions_yolo(image_path)
        if boxes:
            return boxes
    except Exception:
        pass
    return segment_questions_opencv(image_path)


==========================================================================================
# FILE: ai/detection/segment_opencv.py
==========================================================================================
# apps/worker/ai/detection/segment_opencv.py
from __future__ import annotations

from typing import List, Tuple
import cv2  # type: ignore

BBox = Tuple[int, int, int, int]


def segment_questions_opencv(image_path: str) -> List[BBox]:
    """
    legacy 섞여있던 opencv segmentation 정리본
    입력: image_path
    출력: [(x,y,w,h), ...]
    """
    image_bgr = cv2.imread(image_path)
    if image_bgr is None:
        return []

    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5, 5), 0)

    _, thresh = cv2.threshold(
        blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU
    )

    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))
    dilated = cv2.dilate(thresh, kernel, iterations=1)

    contours, _ = cv2.findContours(
        dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
    )

    h_img, w_img = gray.shape[:2]
    min_area = w_img * h_img * 0.005
    max_area = w_img * h_img * 0.9

    boxes: List[BBox] = []
    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)
        area = w * h
        if area < min_area or area > max_area:
            continue

        aspect = h / (w + 1e-6)
        if aspect < 0.3:
            continue

        boxes.append((x, y, w, h))

    boxes.sort(key=lambda b: (b[1], b[0]))
    return boxes


==========================================================================================
# FILE: ai/detection/segment_yolo.py
==========================================================================================
# apps/worker/ai/detection/segment_yolo.py
from __future__ import annotations

from functools import lru_cache
from typing import List, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore

from apps.worker.ai_worker.ai.config import AIConfig

BBox = Tuple[int, int, int, int]


class YoloNotConfiguredError(RuntimeError):
    pass


try:
    import onnxruntime as ort  # type: ignore
    _HAS_ORT = True
except Exception:
    _HAS_ORT = False


@lru_cache()
def _get_session():
    cfg = AIConfig.load()

    if not _HAS_ORT:
        raise YoloNotConfiguredError("onnxruntime not installed")

    if not cfg.YOLO_QUESTION_MODEL_PATH:
        raise YoloNotConfiguredError("YOLO_QUESTION_MODEL_PATH not set")

    providers = ["CPUExecutionProvider"]
    return ort.InferenceSession(str(cfg.YOLO_QUESTION_MODEL_PATH), providers=providers)


def _preprocess(image_bgr, input_size: int):
    h0, w0 = image_bgr.shape[:2]
    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
    resized = cv2.resize(image_rgb, (input_size, input_size))
    resized = resized.astype(np.float32) / 255.0

    tensor = np.transpose(resized, (2, 0, 1))
    tensor = np.expand_dims(tensor, axis=0)

    scale_x = w0 / float(input_size)
    scale_y = h0 / float(input_size)
    return tensor, scale_x, scale_y


def _nms(boxes, scores, iou_threshold: float):
    if len(boxes) == 0:
        return []

    boxes = boxes.astype(np.float32)
    scores = scores.astype(np.float32)

    x1, y1, x2, y2 = boxes.T
    areas = (x2 - x1) * (y2 - y1)
    order = scores.argsort()[::-1]

    keep = []
    while order.size > 0:
        i = int(order[0])
        keep.append(i)

        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])

        w = np.maximum(0.0, xx2 - xx1)
        h = np.maximum(0.0, yy2 - yy1)
        inter = w * h

        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-6)
        idxs = np.where(iou <= iou_threshold)[0]
        order = order[idxs + 1]

    return keep


def segment_questions_yolo(image_path: str) -> List[BBox]:
    cfg = AIConfig.load()
    sess = _get_session()

    image_bgr = cv2.imread(image_path)
    if image_bgr is None:
        return []

    input_tensor, scale_x, scale_y = _preprocess(image_bgr, cfg.YOLO_QUESTION_INPUT_SIZE)

    input_name = sess.get_inputs()[0].name
    outputs = sess.run(None, {input_name: input_tensor})
    preds = outputs[0]
    if preds.ndim == 3:
        preds = preds[0]

    boxes = []
    scores = []

    for det in preds:
        cx, cy, w, h, obj_conf = det[:5]
        cls_scores = det[5:]
        cls_conf = float(cls_scores.max()) if cls_scores.size > 0 else 1.0

        score = float(obj_conf * cls_conf)
        if score < cfg.YOLO_QUESTION_CONF_THRESHOLD:
            continue

        x1 = (cx - w / 2.0) * scale_x
        y1 = (cy - h / 2.0) * scale_y
        x2 = (cx + w / 2.0) * scale_x
        y2 = (cy + h / 2.0) * scale_y

        boxes.append([x1, y1, x2, y2])
        scores.append(score)

    if not boxes:
        return []

    boxes_np = np.array(boxes)
    scores_np = np.array(scores)

    keep_idx = _nms(boxes_np, scores_np, cfg.YOLO_QUESTION_IOU_THRESHOLD)

    final: List[BBox] = []
    for i in keep_idx:
        x1, y1, x2, y2 = boxes_np[i]
        final.append((int(x1), int(y1), int(x2 - x1), int(y2 - y1)))

    final.sort(key=lambda b: (b[1], b[0]))
    return final


==========================================================================================
# FILE: ai/embedding/__init__.py
==========================================================================================
# apps/worker/ai/embedding/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai/embedding/service.py
==========================================================================================
# apps/worker/ai/embedding/service.py
from __future__ import annotations

from dataclasses import dataclass
from typing import List, Optional, Sequence, Literal
import math

from apps.worker.ai_worker.ai.config import AIConfig

EmbeddingBackendName = Literal["local", "openai"]


@dataclass
class EmbeddingBatch:
    vectors: List[List[float]]
    backend: EmbeddingBackendName


def cosine_similarity(a: Sequence[float], b: Sequence[float]) -> float:
    """
    cosine similarity normalized to 0~1
    """
    if not a or not b:
        return 0.0
    if len(a) != len(b):
        return 0.0

    dot = 0.0
    na = 0.0
    nb = 0.0
    for x, y in zip(a, b):
        dot += x * y
        na += x * x
        nb += y * y

    if na <= 0.0 or nb <= 0.0:
        return 0.0

    sim = dot / (math.sqrt(na) * math.sqrt(nb))
    sim = max(-1.0, min(1.0, sim))
    return (sim + 1.0) / 2.0


# -------- local (sentence-transformers) --------
try:
    from sentence_transformers import SentenceTransformer  # type: ignore
except Exception:
    SentenceTransformer = None  # type: ignore

_local_model: Optional["SentenceTransformer"] = None


def _get_local_model() -> "SentenceTransformer":
    global _local_model
    if _local_model is not None:
        return _local_model

    if SentenceTransformer is None:
        raise RuntimeError("sentence-transformers not installed")

    cfg = AIConfig.load()
    _local_model = SentenceTransformer(cfg.EMBEDDING_LOCAL_MODEL)
    return _local_model


def _embed_local(texts: List[str]) -> EmbeddingBatch:
    model = _get_local_model()
    vectors = model.encode(texts, convert_to_numpy=False)
    vectors_list = [list(map(float, v)) for v in vectors]
    return EmbeddingBatch(vectors=vectors_list, backend="local")


# -------- openai --------
try:
    from openai import OpenAI  # type: ignore
except Exception:
    OpenAI = None  # type: ignore

_openai_client: Optional["OpenAI"] = None


def _get_openai_client() -> "OpenAI":
    global _openai_client
    if _openai_client is not None:
        return _openai_client

    if OpenAI is None:
        raise RuntimeError("openai not installed")

    cfg = AIConfig.load()
    if not cfg.OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY not set")

    _openai_client = OpenAI(api_key=cfg.OPENAI_API_KEY)
    return _openai_client


def _embed_openai(texts: List[str]) -> EmbeddingBatch:
    cfg = AIConfig.load()
    client = _get_openai_client()
    response = client.embeddings.create(model=cfg.EMBEDDING_OPENAI_MODEL, input=texts)
    vectors = [list(map(float, d.embedding)) for d in response.data]
    return EmbeddingBatch(vectors=vectors, backend="openai")


def _choose_backend() -> EmbeddingBackendName:
    cfg = AIConfig.load()
    mode = (cfg.EMBEDDING_BACKEND or "auto").lower()

    if mode == "local":
        return "local"
    if mode == "openai":
        return "openai"

    # auto: local 가능하면 local
    if SentenceTransformer is not None:
        try:
            _get_local_model()
            return "local"
        except Exception:
            pass
    return "openai"


def get_embeddings(texts: List[str]) -> EmbeddingBatch:
    if not texts:
        return EmbeddingBatch(vectors=[], backend=_choose_backend())

    backend = _choose_backend()
    norm = [(t or "").strip() for t in texts]

    if backend == "local":
        return _embed_local(norm)
    return _embed_openai(norm)


==========================================================================================
# FILE: ai/handwriting/__init__.py
==========================================================================================
# apps/worker/ai/handwriting/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai/handwriting/detector.py
==========================================================================================
# apps/worker/ai/handwriting/detector.py
from __future__ import annotations

from typing import Dict
import cv2  # type: ignore
import numpy as np  # type: ignore


def analyze_handwriting(image_path: str) -> Dict[str, float]:
    """
    legacy(doc_ai/handwriting/handwriting_detector.py) 이식본
    - 한 이미지에서 필기 흔적/계산식 형태 흔적 여부 점수 반환

    return:
      {
        "writing_score": 0~1,
        "calculation_score": 0~1
      }
    """
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        return {"writing_score": 0.0, "calculation_score": 0.0}

    blur = cv2.GaussianBlur(img, (5, 5), 0)
    edges = cv2.Canny(blur, 50, 150)

    writing_density = float(np.sum(edges > 0) / edges.size)

    sobel_x = cv2.Sobel(blur, cv2.CV_64F, 1, 0, ksize=5)
    sobel_y = cv2.Sobel(blur, cv2.CV_64F, 0, 1, ksize=5)
    grad_mag = (np.mean(np.abs(sobel_x)) + np.mean(np.abs(sobel_y))) / 255.0

    writing_score = min(max(writing_density * 12.0, 0.0), 1.0)
    calculation_score = min(max(grad_mag * 3.0, 0.0), 1.0)

    return {
        "writing_score": float(writing_score),
        "calculation_score": float(calculation_score),
    }


==========================================================================================
# FILE: ai/homework/__init__.py
==========================================================================================



==========================================================================================
# FILE: ai/ocr/__init__.py
==========================================================================================
# apps/worker/ai/ocr/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai/ocr/engine.py
==========================================================================================
from __future__ import annotations
from typing import List
from .schemas import OCRResultPayload, OMRDetectedAnswer


def run_ocr_engine(
    *,
    image_path: str,
) -> OCRResultPayload:
    """
    실제 OCR/OMR 엔진 자리
    - 지금은 더미
    - 나중에 OpenCV / Tesseract / 외부 API 교체
    """

    answers: List[OMRDetectedAnswer] = [
        {
            "question_number": 1,
            "detected": ["B"],
            "confidence": 0.92,
            "marking": "single",
            "status": "ok",
        },
        {
            "question_number": 2,
            "detected": ["D"],
            "confidence": 0.88,
            "marking": "single",
            "status": "ok",
        },
    ]

    return {
        "version": "v1",
        "answers": answers,
        "raw_text": None,
    }


==========================================================================================
# FILE: ai/ocr/google.py
==========================================================================================
# apps/worker/ai/ocr/google.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Optional

# google cloud vision
from google.cloud import vision  # type: ignore


@dataclass
class OCRResult:
    text: str
    confidence: Optional[float] = None
    raw: Optional[Any] = None


def google_ocr(image_path: str) -> OCRResult:
    """
    Worker에서 실행되는 Google OCR
    - service account는 GOOGLE_APPLICATION_CREDENTIALS 또는 기본 환경에 따름
    """
    client = vision.ImageAnnotatorClient()

    with open(image_path, "rb") as f:
        content = f.read()

    image = vision.Image(content=content)
    response = client.text_detection(image=image)

    if getattr(response, "error", None) and response.error.message:
        return OCRResult(text="", confidence=None, raw={"error": response.error.message})

    annotations = getattr(response, "text_annotations", None) or []
    if not annotations:
        return OCRResult(text="", confidence=None, raw=None)

    return OCRResult(
        text=annotations[0].description or "",
        confidence=None,
        raw=None,  # raw를 통째로 넘기면 직렬화 이슈가 생길 수 있어 기본 None
    )


==========================================================================================
# FILE: ai/ocr/schemas.py
==========================================================================================
from __future__ import annotations
from typing import TypedDict, List, Optional


class OMRDetectedAnswer(TypedDict):
    question_number: int
    detected: List[str]
    confidence: float
    marking: str     # single / multi / blank
    status: str      # ok / error


class OCRResultPayload(TypedDict):
    version: str
    answers: List[OMRDetectedAnswer]
    raw_text: Optional[str]


==========================================================================================
# FILE: ai/ocr/tesseract.py
==========================================================================================
# apps/worker/ai/ocr/tesseract.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Optional

from PIL import Image  # type: ignore
import pytesseract  # type: ignore


@dataclass
class OCRResult:
    text: str
    confidence: Optional[float] = None
    raw: Optional[Any] = None


def tesseract_ocr(image_path: str) -> OCRResult:
    img = Image.open(image_path)

    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)
    text = "\n".join(data.get("text", [])).strip()

    confs = [c for c in data.get("conf", []) if c != -1]
    confidence = (sum(confs) / len(confs)) if confs else None

    # raw=data 는 너무 클 수 있어 필요 시만 켜기
    return OCRResult(text=text, confidence=confidence, raw=None)


==========================================================================================
# FILE: ai/omr/__init__.py
==========================================================================================



==========================================================================================
# FILE: ai/omr/engine.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/identifier.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore

from apps.worker.ai_worker.ai.omr.meta_px import build_page_scale_from_meta, PageScale


BBox = Tuple[int, int, int, int]


@dataclass(frozen=True)
class IdentifierConfigV1:
    """
    Identifier OMR v1 (8 digits, each digit 0~9 single mark).

    Principles:
    - ROI based fill score (same philosophy as detect_omr_answers_v1)
    - Robust to scan/photo noise by sampling a square ROI around each bubble
    - No DB, no external calls, worker-only judgement/extraction
    """
    # 주변 ROI(버블 중심 기준) 확장 계수: r * k
    roi_expand_k: float = 1.55

    # blank 판단: 해당 digit에서 최고 fill이 이 값보다 작으면 blank
    blank_threshold: float = 0.070

    # ambiguous 판단: top-2 gap이 이 값보다 작으면 ambiguous
    conf_gap_threshold: float = 0.060

    # (운영 편의) digit-level confidence clamp
    min_confidence: float = 0.0
    max_confidence: float = 1.0


def _clamp(v: int, lo: int, hi: int) -> int:
    return max(lo, min(hi, v))


def _crop(gray: np.ndarray, bbox: BBox) -> np.ndarray:
    x, y, w, h = bbox
    x = _clamp(int(x), 0, gray.shape[1] - 1)
    y = _clamp(int(y), 0, gray.shape[0] - 1)
    w = max(1, int(w))
    h = max(1, int(h))
    w = min(w, gray.shape[1] - x)
    h = min(h, gray.shape[0] - y)
    return gray[y:y + h, x:x + w]


def _fill_score(roi_gray: np.ndarray) -> float:
    """
    Same core idea as OMR v1:
    - blur
    - OTSU + INV
    - filled pixel ratio
    """
    if roi_gray.size == 0:
        return 0.0

    blur = cv2.GaussianBlur(roi_gray, (5, 5), 0)
    _, th = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    filled = float(np.sum(th > 0))
    total = float(th.size) if th.size > 0 else 1.0
    score = filled / total
    return float(max(0.0, min(1.0, score)))


def _bubble_roi_bbox_px(
    *,
    center_px: Tuple[int, int],
    r_px: int,
    cfg: IdentifierConfigV1,
    img_w: int,
    img_h: int,
) -> BBox:
    cx, cy = center_px
    side = int(round(max(2, r_px) * cfg.roi_expand_k)) * 2
    x = int(cx - side // 2)
    y = int(cy - side // 2)
    x = _clamp(x, 0, img_w - 1)
    y = _clamp(y, 0, img_h - 1)
    w = _clamp(side, 1, img_w - x)
    h = _clamp(side, 1, img_h - y)
    return (x, y, w, h)


def detect_identifier_v1(
    *,
    image_bgr: np.ndarray,
    meta: Dict[str, Any],
    cfg: Optional[IdentifierConfigV1] = None,
) -> Dict[str, Any]:
    """
    Extract identifier(8 digits) from aligned full-page image.

    meta requirements:
      meta["identifier"]["bubbles"] list with:
        - digit_index (1..8)
        - number (0..9)
        - center: {x(mm), y(mm)}
        - r(mm)

    return contract:
      {
        "identifier": "12345678" | None,
        "digits": [{"digit_index":1,"value":1,"status":"ok|blank|ambiguous","confidence":0.91,"marks":[...]}...],
        "confidence": 0.0~1.0,
        "status": "ok|ambiguous|blank|error"
      }
    """
    cfg = cfg or IdentifierConfigV1()

    if image_bgr is None or image_bgr.size == 0:
        return {"identifier": None, "digits": [], "confidence": 0.0, "status": "error"}

    h, w = image_bgr.shape[:2]
    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)

    ident = meta.get("identifier") or {}
    bubbles = list(ident.get("bubbles") or [])
    if not bubbles:
        return {"identifier": None, "digits": [], "confidence": 0.0, "status": "error"}

    scale: PageScale = build_page_scale_from_meta(meta=meta, image_size_px=(w, h))

    # group bubbles by digit_index
    by_digit: Dict[int, List[Dict[str, Any]]] = {}
    for b in bubbles:
        try:
            di = int(b.get("digit_index") or 0)
        except Exception:
            continue
        if di <= 0:
            continue
        by_digit.setdefault(di, []).append(b)

    digits_out: List[Dict[str, Any]] = []
    identifier_chars: List[str] = []
    status_rollup = "ok"
    confidences: List[float] = []

    for digit_index in sorted(by_digit.keys()):
        bs = by_digit[digit_index]

        marks: List[Dict[str, Any]] = []
        for b in bs:
            num = int(b.get("number") or 0)
            c = b.get("center") or {}
            r_mm = float(b.get("r") or 0.0)

            cx_mm = float(c.get("x") or 0.0)
            cy_mm = float(c.get("y") or 0.0)

            cx_px, cy_px = scale.mm_to_px_point(cx_mm, cy_mm)

            # radius: use average scale for robustness (mm->px)
            r_px_x = max(1, scale.mm_to_px_len_x(r_mm))
            r_px_y = max(1, scale.mm_to_px_len_y(r_mm))
            r_px = max(1, int(round((r_px_x + r_px_y) / 2.0)))

            bbox = _bubble_roi_bbox_px(
                center_px=(cx_px, cy_px),
                r_px=r_px,
                cfg=cfg,
                img_w=w,
                img_h=h,
            )
            roi = _crop(gray, bbox)
            fill = _fill_score(roi)

            marks.append(
                {
                    "number": int(num),
                    "fill": float(fill),
                    "center_px": {"x": int(cx_px), "y": int(cy_px)},
                    "roi_px": {"x": int(bbox[0]), "y": int(bbox[1]), "w": int(bbox[2]), "h": int(bbox[3])},
                }
            )

        marks_sorted = sorted(marks, key=lambda m: float(m.get("fill") or 0.0), reverse=True)
        top = marks_sorted[0] if marks_sorted else {"number": 0, "fill": 0.0}
        second = marks_sorted[1] if len(marks_sorted) > 1 else {"number": 0, "fill": 0.0}

        top_fill = float(top.get("fill") or 0.0)
        second_fill = float(second.get("fill") or 0.0)
        gap = float(top_fill - second_fill)

        if top_fill < cfg.blank_threshold:
            digits_out.append(
                {
                    "digit_index": int(digit_index),
                    "value": None,
                    "status": "blank",
                    "confidence": 0.0,
                    "marks": marks_sorted,
                }
            )
            identifier_chars.append("?")
            status_rollup = "blank" if status_rollup == "ok" else status_rollup
            continue

        if gap < cfg.conf_gap_threshold:
            digits_out.append(
                {
                    "digit_index": int(digit_index),
                    "value": int(top.get("number") or 0),
                    "status": "ambiguous",
                    "confidence": float(max(cfg.min_confidence, min(cfg.max_confidence, top_fill))),
                    "gap": float(gap),
                    "marks": marks_sorted,
                }
            )
            identifier_chars.append(str(int(top.get("number") or 0)))
            status_rollup = "ambiguous" if status_rollup in ("ok",) else status_rollup
            confidences.append(float(top_fill))
            continue

        # ok
        conf = float(max(cfg.min_confidence, min(cfg.max_confidence, top_fill)))
        digits_out.append(
            {
                "digit_index": int(digit_index),
                "value": int(top.get("number") or 0),
                "status": "ok",
                "confidence": conf,
                "gap": float(gap),
                "marks": marks_sorted,
            }
        )
        identifier_chars.append(str(int(top.get("number") or 0)))
        confidences.append(conf)

    # identifier validity: must be 8 digits all ok/ambiguous (blank이면 ? 포함)
    identifier = "".join(identifier_chars)
    if "?" in identifier:
        identifier_final: Optional[str] = None
    else:
        identifier_final = identifier

    # overall confidence: conservative (mean of digit conf where available)
    overall_conf = float(sum(confidences) / len(confidences)) if confidences else 0.0

    return {
        "identifier": identifier_final,
        "raw_identifier": identifier,  # '?' 포함 가능 (운영 디버그/리트라이용)
        "digits": digits_out,
        "confidence": float(max(0.0, min(1.0, overall_conf))),
        "status": status_rollup,
    }


==========================================================================================
# FILE: ai/omr/identifier.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/identifier.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore

from apps.worker.ai_worker.ai.omr.meta_px import build_page_scale_from_meta, PageScale


BBox = Tuple[int, int, int, int]


@dataclass(frozen=True)
class IdentifierConfigV1:
    """
    Identifier OMR v1 (8 digits, each digit 0~9 single mark).

    Principles:
    - ROI based fill score (same philosophy as detect_omr_answers_v1)
    - Robust to scan/photo noise by sampling a square ROI around each bubble
    - No DB, no external calls, worker-only judgement/extraction
    """
    # (OPS DEFAULT) 촬영/워프에서 중심 오차를 흡수하기 위해 소폭 확장
    # 주변 ROI(버블 중심 기준) 확장 계수: r * k
    roi_expand_k: float = 1.60

    # (OPS DEFAULT) blank 과다 방지: 실데이터에서 연필 농도 낮은 케이스 대응
    # blank 판단: 해당 digit에서 최고 fill이 이 값보다 작으면 blank
    blank_threshold: float = 0.055

    # (OPS DEFAULT) ambiguous 과다 방지: top-2 gap 기준 소폭 완화
    # ambiguous 판단: top-2 gap이 이 값보다 작으면 ambiguous
    conf_gap_threshold: float = 0.050

    # (운영 편의) digit-level confidence clamp
    min_confidence: float = 0.0
    max_confidence: float = 1.0


def _clamp(v: int, lo: int, hi: int) -> int:
    return max(lo, min(hi, v))


def _crop(gray: np.ndarray, bbox: BBox) -> np.ndarray:
    x, y, w, h = bbox
    x = _clamp(int(x), 0, gray.shape[1] - 1)
    y = _clamp(int(y), 0, gray.shape[0] - 1)
    w = max(1, int(w))
    h = max(1, int(h))
    w = min(w, gray.shape[1] - x)
    h = min(h, gray.shape[0] - y)
    return gray[y:y + h, x:x + w]


def _fill_score(roi_gray: np.ndarray) -> float:
    """
    Same core idea as OMR v1:
    - blur
    - OTSU + INV
    - filled pixel ratio
    """
    if roi_gray.size == 0:
        return 0.0

    blur = cv2.GaussianBlur(roi_gray, (5, 5), 0)
    _, th = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    filled = float(np.sum(th > 0))
    total = float(th.size) if th.size > 0 else 1.0
    score = filled / total
    return float(max(0.0, min(1.0, score)))


def _bubble_roi_bbox_px(
    *,
    center_px: Tuple[int, int],
    r_px: int,
    cfg: IdentifierConfigV1,
    img_w: int,
    img_h: int,
) -> BBox:
    cx, cy = center_px
    side = int(round(max(2, r_px) * cfg.roi_expand_k)) * 2
    x = int(cx - side // 2)
    y = int(cy - side // 2)
    x = _clamp(x, 0, img_w - 1)
    y = _clamp(y, 0, img_h - 1)
    w = _clamp(side, 1, img_w - x)
    h = _clamp(side, 1, img_h - y)
    return (x, y, w, h)


def detect_identifier_v1(
    *,
    image_bgr: np.ndarray,
    meta: Dict[str, Any],
    cfg: Optional[IdentifierConfigV1] = None,
) -> Dict[str, Any]:
    """
    Extract identifier(8 digits) from aligned full-page image.

    meta requirements:
      meta["identifier"]["bubbles"] list with:
        - digit_index (1..8)
        - number (0..9)
        - center: {x(mm), y(mm)}
        - r(mm)

    return contract:
      {
        "identifier": "12345678" | None,
        "digits": [{"digit_index":1,"value":1,"status":"ok|blank|ambiguous","confidence":0.91,"marks":[...]}...],
        "confidence": 0.0~1.0,
        "status": "ok|ambiguous|blank|error"
      }
    """
    cfg = cfg or IdentifierConfigV1()

    if image_bgr is None or image_bgr.size == 0:
        return {"identifier": None, "digits": [], "confidence": 0.0, "status": "error"}

    h, w = image_bgr.shape[:2]
    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)

    ident = meta.get("identifier") or {}
    bubbles = list(ident.get("bubbles") or [])
    if not bubbles:
        return {"identifier": None, "digits": [], "confidence": 0.0, "status": "error"}

    scale: PageScale = build_page_scale_from_meta(meta=meta, image_size_px=(w, h))

    # group bubbles by digit_index
    by_digit: Dict[int, List[Dict[str, Any]]] = {}
    for b in bubbles:
        try:
            di = int(b.get("digit_index") or 0)
        except Exception:
            continue
        if di <= 0:
            continue
        by_digit.setdefault(di, []).append(b)

    digits_out: List[Dict[str, Any]] = []
    identifier_chars: List[str] = []
    status_rollup = "ok"
    confidences: List[float] = []

    for digit_index in sorted(by_digit.keys()):
        bs = by_digit[digit_index]

        marks: List[Dict[str, Any]] = []
        for b in bs:
            num = int(b.get("number") or 0)
            c = b.get("center") or {}
            r_mm = float(b.get("r") or 0.0)

            cx_mm = float(c.get("x") or 0.0)
            cy_mm = float(c.get("y") or 0.0)

            cx_px, cy_px = scale.mm_to_px_point(cx_mm, cy_mm)

            # radius: use average scale for robustness (mm->px)
            r_px_x = max(1, scale.mm_to_px_len_x(r_mm))
            r_px_y = max(1, scale.mm_to_px_len_y(r_mm))
            r_px = max(1, int(round((r_px_x + r_px_y) / 2.0)))

            bbox = _bubble_roi_bbox_px(
                center_px=(cx_px, cy_px),
                r_px=r_px,
                cfg=cfg,
                img_w=w,
                img_h=h,
            )
            roi = _crop(gray, bbox)
            fill = _fill_score(roi)

            marks.append(
                {
                    "number": int(num),
                    "fill": float(fill),
                    "center_px": {"x": int(cx_px), "y": int(cy_px)},
                    "roi_px": {"x": int(bbox[0]), "y": int(bbox[1]), "w": int(bbox[2]), "h": int(bbox[3])},
                }
            )

        marks_sorted = sorted(marks, key=lambda m: float(m.get("fill") or 0.0), reverse=True)
        top = marks_sorted[0] if marks_sorted else {"number": 0, "fill": 0.0}
        second = marks_sorted[1] if len(marks_sorted) > 1 else {"number": 0, "fill": 0.0}

        top_fill = float(top.get("fill") or 0.0)
        second_fill = float(second.get("fill") or 0.0)
        gap = float(top_fill - second_fill)

        if top_fill < cfg.blank_threshold:
            digits_out.append(
                {
                    "digit_index": int(digit_index),
                    "value": None,
                    "status": "blank",
                    "confidence": 0.0,
                    "marks": marks_sorted,
                }
            )
            identifier_chars.append("?")
            status_rollup = "blank" if status_rollup == "ok" else status_rollup
            continue

        if gap < cfg.conf_gap_threshold:
            digits_out.append(
                {
                    "digit_index": int(digit_index),
                    "value": int(top.get("number") or 0),
                    "status": "ambiguous",
                    "confidence": float(max(cfg.min_confidence, min(cfg.max_confidence, top_fill))),
                    "gap": float(gap),
                    "marks": marks_sorted,
                }
            )
            identifier_chars.append(str(int(top.get("number") or 0)))
            status_rollup = "ambiguous" if status_rollup in ("ok",) else status_rollup
            confidences.append(float(top_fill))
            continue

        # ok
        conf = float(max(cfg.min_confidence, min(cfg.max_confidence, top_fill)))
        digits_out.append(
            {
                "digit_index": int(digit_index),
                "value": int(top.get("number") or 0),
                "status": "ok",
                "confidence": conf,
                "gap": float(gap),
                "marks": marks_sorted,
            }
        )
        identifier_chars.append(str(int(top.get("number") or 0)))
        confidences.append(conf)

    # identifier validity: must be 8 digits all ok/ambiguous (blank이면 ? 포함)
    identifier = "".join(identifier_chars)
    if "?" in identifier:
        identifier_final: Optional[str] = None
    else:
        identifier_final = identifier

    # overall confidence: conservative (mean of digit conf where available)
    overall_conf = float(sum(confidences) / len(confidences)) if confidences else 0.0

    return {
        "identifier": identifier_final,
        "raw_identifier": identifier,  # '?' 포함 가능 (운영 디버그/리트라이용)
        "digits": digits_out,
        "confidence": float(max(0.0, min(1.0, overall_conf))),
        "status": status_rollup,
    }


==========================================================================================
# FILE: ai/omr/meta_px.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/meta_px.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, Tuple


def _clamp(v: int, lo: int, hi: int) -> int:
    return max(lo, min(hi, v))


@dataclass(frozen=True)
class PageScale:
    """
    Convert meta(mm) -> px.
    Boundary rule (fixed):
      - meta(mm) is assets single truth
      - px conversion is worker responsibility
      - aligned/warped image must represent the full page
    """
    sx: float
    sy: float
    img_w: int
    img_h: int

    def mm_to_px_point(self, x_mm: float, y_mm: float) -> Tuple[int, int]:
        x = int(round(float(x_mm) * self.sx))
        y = int(round(float(y_mm) * self.sy))
        x = _clamp(x, 0, self.img_w - 1)
        y = _clamp(y, 0, self.img_h - 1)
        return x, y

    def mm_to_px_len_x(self, v_mm: float) -> int:
        return max(1, int(round(float(v_mm) * self.sx)))

    def mm_to_px_len_y(self, v_mm: float) -> int:
        return max(1, int(round(float(v_mm) * self.sy)))


def build_page_scale_from_meta(
    *,
    meta: Dict[str, Any],
    image_size_px: Tuple[int, int],
) -> PageScale:
    """
    Build scaler from template meta.
    meta page size is mm. image_size_px is (width, height).
    """
    img_w, img_h = int(image_size_px[0]), int(image_size_px[1])

    page = meta.get("page") or {}
    size = page.get("size") or {}
    page_w_mm = float(size.get("width") or 0.0)
    page_h_mm = float(size.get("height") or 0.0)

    if img_w <= 0 or img_h <= 0:
        raise ValueError("invalid image_size_px")
    if page_w_mm <= 0.0 or page_h_mm <= 0.0:
        raise ValueError("invalid meta page size")

    sx = img_w / page_w_mm
    sy = img_h / page_h_mm
    return PageScale(sx=float(sx), sy=float(sy), img_w=img_w, img_h=img_h)


==========================================================================================
# FILE: ai/omr/template_meta.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/template_meta.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Tuple, Optional

import requests


@dataclass(frozen=True)
class TemplateMeta:
    raw: Dict[str, Any]

    @property
    def units(self) -> str:
        return str(self.raw.get("units") or "mm")

    @property
    def page_size_mm(self) -> Tuple[float, float]:
        page = self.raw.get("page") or {}
        size = page.get("size") or {}
        return float(size.get("width") or 0.0), float(size.get("height") or 0.0)

    @property
    def questions(self) -> List[Dict[str, Any]]:
        return list(self.raw.get("questions") or [])


class TemplateMetaFetchError(RuntimeError):
    pass


def fetch_objective_meta(
    *,
    base_url: str,
    question_count: int,
    # auth options (choose what your deployment uses)
    auth_cookie_header: Optional[str] = None,
    bearer_token: Optional[str] = None,
    worker_token_header: Optional[str] = None,  # e.g. X-Worker-Token (if API allows)
    extra_headers: Optional[Dict[str, str]] = None,
    timeout: int = 10,
) -> TemplateMeta:
    """
    worker -> API (assets meta)

    Stability goals:
    - clear timeouts
    - explicit error messages
    - flexible auth (cookie/bearer/custom header)
    - returns structured exception for caller to gracefully fallback

    NOTE:
    - assets endpoint currently uses IsAuthenticated.
      Production typically uses cookie session OR bearer token.
      worker_token_header is optional if you later add internal auth for workers.
    """
    url = f"{base_url.rstrip('/')}/api/v1/assets/omr/objective/meta/"
    params = {"question_count": str(int(question_count))}

    headers: Dict[str, str] = {"Accept": "application/json"}
    if auth_cookie_header:
        headers["Cookie"] = auth_cookie_header
    if bearer_token:
        headers["Authorization"] = f"Bearer {bearer_token}"
    if worker_token_header:
        headers["X-Worker-Token"] = str(worker_token_header)
    if extra_headers:
        for k, v in extra_headers.items():
            if k and v:
                headers[str(k)] = str(v)

    try:
        r = requests.get(url, params=params, headers=headers, timeout=timeout)
    except Exception as e:
        raise TemplateMetaFetchError(f"meta_fetch_network_error: {e!r}") from e

    if r.status_code >= 400:
        body = (r.text or "")[:2000]
        raise TemplateMetaFetchError(
            f"meta_fetch_http_error: status={r.status_code} body={body}"
        )

    try:
        data = r.json()
    except Exception as e:
        raise TemplateMetaFetchError(f"meta_fetch_invalid_json: {e!r}") from e

    return TemplateMeta(raw=data)


==========================================================================================
# FILE: ai/omr/types.py
==========================================================================================
# apps/worker/ai/omr/types.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Literal


OMRStatus = Literal["ok", "blank", "ambiguous", "low_confidence", "error"]
OMRMarking = Literal["blank", "single", "multi"]


@dataclass(frozen=True)
class OMRAnswerV1:
    """
    Worker-side OMR payload v1 (question-level)
    This should be embedded into API-side SubmissionAnswer.meta["omr"] later.

    - version: "v1" fixed
    - detected: list[str] ex) ["B"] or ["B","D"]
    - marking: blank/single/multi
    - confidence: 0~1 (top mark confidence)
    - status: ok/blank/ambiguous/low_confidence/error
    - raw: debug info (optional)
    """
    version: str
    question_id: int

    detected: List[str]
    marking: OMRMarking
    confidence: float
    status: OMRStatus

    raw: Optional[Dict[str, Any]] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "version": self.version,
            "question_id": int(self.question_id),
            "detected": list(self.detected or []),
            "marking": self.marking,
            "confidence": float(self.confidence or 0.0),
            "status": self.status,
            "raw": self.raw,
        }


==========================================================================================
# FILE: ai/pipelines/__init__.py
==========================================================================================
# apps/worker/ai/pipelines/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai/pipelines/dispatcher.py
==========================================================================================
# apps/worker/ai_worker/ai/pipelines/dispatcher.py
from __future__ import annotations

from typing import Any, Dict

from apps.shared.contracts.ai_job import AIJob
from apps.shared.contracts.ai_result import AIResult

from apps.worker.ai_worker.ai.config import AIConfig
from apps.worker.ai_worker.ai.ocr.google import google_ocr
from apps.worker.ai_worker.ai.ocr.tesseract import tesseract_ocr
from apps.worker.ai_worker.ai.detection.segment_dispatcher import segment_questions
from apps.worker.ai_worker.ai.handwriting.detector import analyze_handwriting
from apps.worker.ai_worker.ai.embedding.service import get_embeddings
from apps.worker.ai_worker.ai.problem.generator import generate_problem_from_ocr
from apps.worker.ai_worker.ai.pipelines.homework_video_analyzer import analyze_homework_video
from apps.worker.ai_worker.storage.downloader import download_to_tmp


def handle_ai_job(job: AIJob) -> AIResult:
    try:
        cfg = AIConfig.load()
        payload: Dict[str, Any] = job.payload or {}

        download_url = payload.get("download_url")
        if not download_url:
            return AIResult.failed(job.id, "download_url missing")

        local_path = download_to_tmp(
            download_url=download_url,
            job_id=str(job.id),
        )

        # --------------------------------------------------
        # OCR
        # --------------------------------------------------
        if job.type == "ocr":
            engine = (payload.get("engine") or cfg.OCR_ENGINE or "auto").lower()

            if engine == "tesseract":
                r = tesseract_ocr(local_path)
            elif engine == "google":
                r = google_ocr(local_path)
            else:
                try:
                    r = google_ocr(local_path)
                    if not r.text.strip():
                        r = tesseract_ocr(local_path)
                except Exception:
                    r = tesseract_ocr(local_path)

            return AIResult.done(
                job.id,
                {"text": r.text, "confidence": r.confidence},
            )

        # --------------------------------------------------
        # Question segmentation
        # --------------------------------------------------
        if job.type == "question_segmentation":
            boxes = segment_questions(local_path)
            return AIResult.done(job.id, {"boxes": boxes})

        # --------------------------------------------------
        # Handwriting analysis
        # --------------------------------------------------
        if job.type == "handwriting_analysis":
            scores = analyze_handwriting(local_path)
            return AIResult.done(job.id, scores)

        # --------------------------------------------------
        # Embedding
        # --------------------------------------------------
        if job.type == "embedding":
            texts = payload.get("texts") or []
            batch = get_embeddings(list(texts))
            return AIResult.done(
                job.id,
                {"backend": batch.backend, "vectors": batch.vectors},
            )

        # --------------------------------------------------
        # Problem generation
        # --------------------------------------------------
        if job.type == "problem_generation":
            ocr_text = payload.get("ocr_text") or ""
            parsed = generate_problem_from_ocr(ocr_text)
            return AIResult.done(
                job.id,
                {
                    "body": parsed.body,
                    "choices": parsed.choices,
                    "answer": parsed.answer,
                    "difficulty": parsed.difficulty,
                    "tag": parsed.tag,
                    "summary": parsed.summary,
                    "explanation": parsed.explanation,
                },
            )

        # --------------------------------------------------
        # Homework video analysis
        # --------------------------------------------------
        if job.type == "homework_video_analysis":
            frame_stride = int(payload.get("frame_stride") or 10)
            min_frame_count = int(payload.get("min_frame_count") or 30)
            analysis = analyze_homework_video(
                video_path=local_path,
                frame_stride=frame_stride,
                min_frame_count=min_frame_count,
            )
            return AIResult.done(job.id, analysis)

        # --------------------------------------------------
        # OMR grading (meta-aware) - production final
        # --------------------------------------------------
        if job.type == "omr_grading":
            """
            payload options (recommended):
              - mode: "scan" | "photo" | "auto" (default auto)
              - question_count: 10|20|30 (required if template_fetch is used)
              - template_meta: dict (inject meta directly; no API call)
              - template_fetch:
                    {
                      "base_url": "...",
                      "cookie": "...",
                      "bearer_token": "...",
                      "worker_token": "...",
                      "timeout": 10
                    }

            Output contract:
              {
                "version": "v1",
                "mode": "...",
                "aligned": true|false,
                "identifier": {...},
                "answers": [...],
                "meta_used": true|false
              }
            """
            import cv2  # type: ignore

            from apps.worker.ai_worker.ai.omr.engine import detect_omr_answers_v1, OMRConfigV1
            from apps.worker.ai_worker.ai.omr.roi_builder import build_questions_payload_from_meta
            from apps.worker.ai_worker.ai.omr.warp import warp_to_a4_landscape
            from apps.worker.ai_worker.ai.omr.template_meta import fetch_objective_meta, TemplateMetaFetchError
            from apps.worker.ai_worker.ai.omr.identifier import detect_identifier_v1, IdentifierConfigV1

            mode = str(payload.get("mode") or "auto").lower()
            if mode not in ("scan", "photo", "auto"):
                mode = "auto"

            # 1) meta 확보
            meta = payload.get("template_meta")
            meta_used = False
            meta_fetch_error = None

            if not meta:
                tf = payload.get("template_fetch") or {}
                base_url = tf.get("base_url")
                if base_url:
                    qc = int(payload.get("question_count") or 0)
                    if qc not in (10, 20, 30):
                        return AIResult.failed(job.id, "question_count required (10|20|30) for template_fetch")

                    try:
                        meta_obj = fetch_objective_meta(
                            base_url=str(base_url),
                            question_count=qc,
                            auth_cookie_header=tf.get("cookie"),
                            bearer_token=tf.get("bearer_token"),
                            worker_token_header=tf.get("worker_token"),
                            timeout=int(tf.get("timeout") or 10),
                        )
                        meta = meta_obj.raw
                        meta_used = True
                    except TemplateMetaFetchError as e:
                        meta = None
                        meta_fetch_error = str(e)[:500]

            # 2) 이미지 로드
            img_bgr = cv2.imread(local_path)
            if img_bgr is None:
                return AIResult.failed(job.id, "cannot read image")

            aligned = img_bgr

            # 3) mode 정책
            if mode == "photo":
                warped = warp_to_a4_landscape(img_bgr)
                if warped is None:
                    return AIResult.failed(job.id, "warp_failed_for_photo_mode")
                aligned = warped

            elif mode == "auto":
                warped = warp_to_a4_landscape(img_bgr)
                if warped is not None:
                    aligned = warped

            # 4) meta 없으면 legacy
            if not meta:
                questions = payload.get("questions") or []
                if not questions:
                    return AIResult.failed(job.id, "template_meta/template_fetch failed and legacy questions missing")

                answers = detect_omr_answers_v1(
                    image_path=local_path,
                    questions=list(questions),
                    cfg=None,
                )
                return AIResult.done(
                    job.id,
                    {
                        "version": "v1",
                        "mode": "legacy_questions",
                        "aligned": False,
                        "identifier": None,
                        "answers": answers,
                        "meta_used": False,
                        "debug": {
                            "meta_fetch_error": meta_fetch_error,
                        },
                    },
                )

            # 5) ROI + identifier
            h, w = aligned.shape[:2]
            questions_payload = build_questions_payload_from_meta(meta, (w, h))

            ident = detect_identifier_v1(
                image_bgr=aligned,
                meta=meta,
                cfg=IdentifierConfigV1(),
            )

            # 6) aligned 저장 후 OMR
            import tempfile, os

            tmp_path = os.path.join(tempfile.gettempdir(), f"omr_aligned_{job.id}.jpg")
            cv2.imwrite(tmp_path, aligned)

            cfg = OMRConfigV1()
            answers = detect_omr_answers_v1(
                image_path=tmp_path,
                questions=list(questions_payload),
                cfg=cfg,
            )

            return AIResult.done(
                job.id,
                {
                    "version": "v1",
                    "mode": mode,
                    "aligned": bool(aligned is not img_bgr),
                    "identifier": ident,
                    "answers": answers,
                    "meta_used": meta_used,
                    "debug": {
                        "meta_fetch_error": meta_fetch_error,
                    },
                },
            )

        return AIResult.failed(job.id, f"Unsupported job type: {job.type}")

    except Exception as e:
        return AIResult.failed(job.id, str(e))


==========================================================================================
# FILE: ai/pipelines/homework_video_analyzer.py
==========================================================================================
# apps/worker/ai/pipelines/homework_video_analyzer.py
from __future__ import annotations

from typing import Dict, Any, List
import cv2  # type: ignore
import numpy as np  # type: ignore


def _estimate_writing_score(gray_roi: np.ndarray) -> float:
    if gray_roi.size == 0:
        return 0.0
    dark = (gray_roi < 220).sum()
    total = gray_roi.size
    return float(dark) / float(total)


def analyze_homework_video(
    video_path: str,
    frame_stride: int = 10,
    min_frame_count: int = 30,
) -> Dict[str, Any]:
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"cannot open video: {video_path}")

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_idx = 0
    frame_results: List[Dict[str, Any]] = []

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        if frame_idx % frame_stride != 0:
            frame_idx += 1
            continue

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        score = _estimate_writing_score(gray)

        frame_results.append(
            {
                "index": frame_idx,
                "writing_score": round(float(score), 4),
                "has_writing": bool(score >= 0.05),
            }
        )
        frame_idx += 1

    cap.release()

    sampled = len(frame_results)
    if sampled == 0:
        return {
            "total_frames": total_frames,
            "sampled_frames": 0,
            "avg_writing_score": 0.0,
            "filled_ratio": 0.0,
            "frames": [],
            "too_short": total_frames < min_frame_count,
        }

    avg = sum(fr["writing_score"] for fr in frame_results) / sampled
    filled = sum(1 for fr in frame_results if fr["has_writing"])
    ratio = filled / sampled

    return {
        "total_frames": total_frames,
        "sampled_frames": sampled,
        "avg_writing_score": round(float(avg), 4),
        "filled_ratio": round(float(ratio), 4),
        "frames": frame_results,
        "too_short": total_frames < min_frame_count,
    }


==========================================================================================
# FILE: ai/problem/__init__.py
==========================================================================================
# apps/worker/ai/problem/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai/problem/generator.py
==========================================================================================
# apps/worker/ai/problem/generator.py
from __future__ import annotations

import json
from dataclasses import dataclass
from typing import Optional

from apps.worker.ai_worker.ai.config import AIConfig
from apps.worker.ai_worker.ai.problem.prompt import BASE_PROMPT

try:
    from openai import OpenAI  # type: ignore
except Exception:
    OpenAI = None  # type: ignore


@dataclass
class ParsedProblem:
    body: str
    choices: list
    answer: Optional[str]
    difficulty: int
    tag: str
    summary: str
    explanation: str


_client: Optional["OpenAI"] = None


def _get_client() -> "OpenAI":
    global _client
    if _client is not None:
        return _client

    if OpenAI is None:
        raise RuntimeError("openai not installed")

    cfg = AIConfig.load()
    if not cfg.OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY not set")

    _client = OpenAI(api_key=cfg.OPENAI_API_KEY)
    return _client


def generate_problem_from_ocr(ocr_text: str) -> ParsedProblem:
    cfg = AIConfig.load()
    prompt = BASE_PROMPT.format(ocr_text=ocr_text)

    client = _get_client()
    response = client.chat.completions.create(
        model=cfg.PROBLEM_GEN_MODEL,
        messages=[
            {"role": "system", "content": "당신은 교육용 시험 문제를 자동 생성하는 엔진입니다."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.2,
    )

    # SDK 형태 차이 방어
    msg = response.choices[0].message
    content = getattr(msg, "content", None) or msg.get("content")  # type: ignore

    data = json.loads(content)

    return ParsedProblem(
        body=data.get("body", ""),
        choices=data.get("choices", []),
        answer=data.get("answer"),
        difficulty=int(data.get("difficulty", 3)),
        tag=data.get("tag", ""),
        summary=data.get("summary", ""),
        explanation=data.get("explanation", ""),
    )


==========================================================================================
# FILE: ai/problem/prompt.py
==========================================================================================
# apps/worker/ai/problem/prompt.py
BASE_PROMPT = """
다음은 시험 문제의 OCR 결과입니다.
아래 텍스트를 기반으로 문제 정보를 JSON 형식으로 추출하세요.

요구사항:
1) 문제 본문 (body)
2) 선택지 (choices): 없으면 빈 배열
3) 정답 (answer): 명시된 정답이 없으면 AI가 추론, 추론 불가 시 null
4) 난이도 (difficulty): 1~5 정수로 추정
5) 태그 (tag): 수학/과학/국어 등 간단한 분류
6) 문제 요약 (summary)
7) 해설 (explanation): 간단명료하게

출력은 반드시 JSON 형식만 사용하세요. 다른 텍스트는 포함하지 마세요.

출력 형식 예시:
{
  "body": "...",
  "choices": ["A...", "B...", "C...", "D..."],
  "answer": "C",
  "difficulty": 3,
  "tag": "수학",
  "summary": "...",
  "explanation": "..."
}

OCR 텍스트:
\"\"\"
{ocr_text}
\"\"\"
"""


==========================================================================================
# FILE: apps/worker/ai_worker/celery.py
==========================================================================================



==========================================================================================
# FILE: queue/redis_queue.py
==========================================================================================
import redis
import json
import time
import logging

logger = logging.getLogger(__name__)

class RedisJobQueue:
    def __init__(self, redis_url: str):
        self.r = redis.from_url(redis_url, decode_responses=True)
        self.jobs = "ai:jobs"
        self.processing = "ai:processing"
        self.dead = "ai:jobs:dead"

    def claim(self, timeout=5):
        return self.r.brpoplpush(self.jobs, self.processing, timeout)

    def ack(self, raw):
        self.r.lrem(self.processing, 1, raw)

    def retry_or_dead(self, job_dict, raw, reason):
        attempt = int(job_dict.get("attempt", 0)) + 1
        max_attempts = int(job_dict.get("max_attempts", 5))

        self.ack(raw)

        if attempt >= max_attempts:
            job_dict["error"] = reason
            self.r.lpush(self.dead, json.dumps(job_dict))
            return

        job_dict["attempt"] = attempt
        self.r.lpush(self.jobs, json.dumps(job_dict))


==========================================================================================
# FILE: storage/__init__.py
==========================================================================================



==========================================================================================
# FILE: storage/downloader.py
==========================================================================================
# ==============================================================================
# PATH: apps/worker/storage/downloader.py
#
# PURPOSE:
# - AI worker 전용 파일 다운로드 유틸
# - presigned GET URL → /tmp local file
# - R2 / S3 credential 사용 ❌
# - video_worker 코드와 절대 공유하지 않음
# ==============================================================================

from __future__ import annotations

import os
import tempfile
import requests
from pathlib import Path


def download_to_tmp(
    *,
    download_url: str,
    job_id: str,
    suffix: str | None = None,
    timeout: int = 60,
    chunk_size: int = 1024 * 1024,  # 1MB
) -> str:
    """
    presigned GET URL로 파일을 다운로드하여 local temp file 경로 반환

    규칙:
    - AI worker는 URL만 신뢰
    - 파일은 /tmp 또는 OS temp dir에 생성
    - 호출자는 반환된 path만 사용
    """

    tmp_dir = Path(tempfile.gettempdir())
    ext = suffix or ""

    filename = f"ai_job_{job_id}{ext}"
    tmp_path = tmp_dir / filename
    part_path = tmp_dir / f"{filename}.part"

    try:
        with requests.get(download_url, stream=True, timeout=timeout) as r:
            r.raise_for_status()

            expected_len = r.headers.get("Content-Length")
            expected_len = int(expected_len) if expected_len and expected_len.isdigit() else None

            written = 0
            with open(part_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=chunk_size):
                    if not chunk:
                        continue
                    f.write(chunk)
                    written += len(chunk)

            if expected_len is not None and written != expected_len:
                raise RuntimeError(
                    f"download size mismatch (expected={expected_len}, got={written})"
                )

        part_path.replace(tmp_path)
        return str(tmp_path)

    except Exception:
        try:
            if part_path.exists():
                part_path.unlink()
        except Exception:
            pass
        raise


==========================================================================================
# FILE: wrong_notes/__init__.py
==========================================================================================
# apps/worker/wrong_notes/__init__.py


==========================================================================================
# FILE: wrong_notes/templates/wrong_note.html
==========================================================================================
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="utf-8" />
    <style>
        body {
            font-family: Arial, Helvetica, sans-serif;
            font-size: 12px;
            line-height: 1.6;
        }
        h1 {
            border-bottom: 2px solid #333;
            padding-bottom: 4px;
        }
        .exam {
            margin-top: 20px;
        }
        .question {
            margin-left: 12px;
        }
        .wrong {
            color: #c0392b;
        }
    </style>
</head>
<body>
    <h1>오답 노트</h1>

    <p>
        학생 ID: {{ enrollment_id }}<br/>
        생성일: {{ created_at }}
    </p>

    {% for exam_id, items in grouped.items %}
        <div class="exam">
            <h2>시험 {{ exam_id }}</h2>
            {% for item in items %}
                <div class="question">
                    <span class="wrong">
                        Q{{ item.question_id }}
                    </span>
                    / 제출 답안: {{ item.answer }}
                </div>
            {% endfor %}
        </div>
    {% endfor %}
</body>
</html>
