====================================================================================================
# BACKEND APP: ai_worker
# ROOT PATH: C:\academy\apps\worker\ai_worker
====================================================================================================


==========================================================================================
# FILE: __init__.py
==========================================================================================



==========================================================================================
# FILE: run.py
==========================================================================================
# PATH: apps/worker/ai_worker/run.py
from __future__ import annotations

import os
import sys
import logging
import requests
import time

from apps.shared.contracts.ai_job import AIJob
from apps.shared.contracts.ai_result import AIResult
from apps.worker.ai_worker.ai.pipelines.dispatcher import handle_ai_job

# ==============================================================================
# AI WORKER – SINGLE RUN MODE (PRODUCTION FINAL)
#
# DESIGN PRINCIPLES (ENTERPRISE STANDARD):
# - Worker is NOT a daemon
# - Bounded polling within fixed lifetime window
# - No infinite loop
# - One execution = at most one job
# - Exit immediately after job 처리 or idle window expiration
#
# NOTE (OPS):
# - Process lifetime is capped to match EC2 billing granularity (default 60s)
# - Polling is allowed ONLY within this window
# ==============================================================================

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] [AI-WORKER] %(message)s",
)
logger = logging.getLogger(__name__)

API_BASE_URL = os.getenv("API_BASE_URL", "http://localhost:8000")
INTERNAL_WORKER_TOKEN = os.getenv("INTERNAL_WORKER_TOKEN", "long-random-secret")

# OPS: worker lifetime & polling
WORKER_MIN_LIFETIME = int(os.getenv("WORKER_MIN_LIFETIME", "60"))
POLL_INTERVAL_SECONDS = int(os.getenv("WORKER_POLL_INTERVAL", "5"))


def _headers() -> dict:
    return {
        "X-Worker-Token": INTERNAL_WORKER_TOKEN,
        "X-Worker-Id": os.getenv("HOSTNAME", "ai-worker"),
    }


def fetch_job() -> AIJob | None:
    """
    Ask API for the next AI job.
    API is the single source of truth.
    """
    url = f"{API_BASE_URL.rstrip('/')}/api/v1/internal/ai/job/next/"
    resp = requests.get(url, headers=_headers(), timeout=10)
    resp.raise_for_status()

    data = resp.json()
    job_data = data.get("job")
    if not job_data:
        return None

    return AIJob.from_dict(job_data)


def submit_result(*, result: AIResult, job: AIJob) -> None:
    """
    Submit job result back to API.
    """
    url = f"{API_BASE_URL.rstrip('/')}/api/v1/internal/ai/job/result/"
    headers = _headers()
    headers["Content-Type"] = "application/json"

    submission_id = None
    try:
        if job.source_id is not None and str(job.source_id).isdigit():
            submission_id = int(str(job.source_id))
    except Exception:
        submission_id = None

    payload = {
        "job_id": job.id,
        "submission_id": submission_id,
        "status": result.status,
        "result": result.result,
        "error": result.error,
    }

    resp = requests.post(url, json=payload, headers=headers, timeout=20)
    resp.raise_for_status()


# ------------------------------------------------------------------------------
# EC2 SELF-STOP (MINIMAL, SAFE)
# ------------------------------------------------------------------------------
def _stop_self_ec2() -> None:
    """
    Stop this EC2 instance itself.

    - Uses IMDSv2
    - Requires IAM role permission: ec2:StopInstances
    """
    try:
        import boto3

        token = requests.put(
            "http://169.254.169.254/latest/api/token",
            headers={"X-aws-ec2-metadata-token-ttl-seconds": "21600"},
            timeout=2,
        ).text

        headers = {"X-aws-ec2-metadata-token": token}

        instance_id = requests.get(
            "http://169.254.169.254/latest/meta-data/instance-id",
            headers=headers,
            timeout=2,
        ).text

        region = requests.get(
            "http://169.254.169.254/latest/meta-data/placement/region",
            headers=headers,
            timeout=2,
        ).text

        ec2 = boto3.client("ec2", region_name=region)
        ec2.stop_instances(InstanceIds=[instance_id])

        logger.info("EC2 self-stop requested (instance_id=%s)", instance_id)

    except Exception as e:
        logger.exception("EC2 self-stop failed (ignored): %s", e)


def main() -> int:
    """
    Single-run worker entrypoint with bounded polling.

    Flow:
    - Poll for job within WORKER_MIN_LIFETIME window
    - If job found → process once → exit
    - If no job until timeout → exit
    """
    start_ts = time.monotonic()
    deadline = start_ts + WORKER_MIN_LIFETIME

    logger.info(
        "AI Worker started (API_BASE_URL=%s, lifetime=%ss)",
        API_BASE_URL,
        WORKER_MIN_LIFETIME,
    )

    try:
        while True:
            now = time.monotonic()
            if now >= deadline:
                logger.info("Idle window expired (%ss). exiting.", WORKER_MIN_LIFETIME)
                return 0

            try:
                job = fetch_job()
            except Exception:
                logger.exception("fetch_job failed")
                return 1

            if job is None:
                sleep_sec = min(POLL_INTERVAL_SECONDS, max(0.0, deadline - now))
                if sleep_sec > 0:
                    time.sleep(sleep_sec)
                continue

            logger.info("Job received: id=%s type=%s", job.id, job.type)

            try:
                result = handle_ai_job(job)
                submit_result(result=result, job=job)
                logger.info("Job finished: id=%s status=%s", job.id, result.status)
                return 0
            except Exception:
                logger.exception("Job processing failed")
                return 1

    finally:
        elapsed = time.monotonic() - start_ts
        remain = WORKER_MIN_LIFETIME - elapsed

        if remain > 0:
            logger.info("Graceful sleep before shutdown: %.1fs", remain)
            time.sleep(remain)

        logger.info("AI Worker shutdown complete")
        _stop_self_ec2()


if __name__ == "__main__":
    sys.exit(main())


==========================================================================================
# FILE: sqs_main.py
==========================================================================================
"""
AI Worker - SQS 기반 메인 엔트리포인트

기존 HTTP polling 방식에서 SQS Long Polling으로 전환
CPU/GPU 큐 분리 지원
"""

from __future__ import annotations

import json
import logging
import os
import signal
import sys
import time
import uuid
from typing import Optional

import boto3
import requests

from apps.worker.ai_worker.ai.pipelines.dispatcher import handle_ai_job
from apps.shared.contracts.ai_job import AIJob
from apps.shared.contracts.ai_result import AIResult
from src.infrastructure.ai import AISQSAdapter

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] [AI-WORKER-SQS] %(message)s",
)
logger = logging.getLogger("ai_worker_sqs")

_shutdown = False
_current_job_receipt_handle: Optional[str] = None  # Graceful shutdown: 현재 처리 중인 작업 추적
_current_job_start_time: Optional[float] = None  # 로그 가시성: 작업 시작 시간

# SQS Long Polling 설정
SQS_WAIT_TIME_SECONDS = 20  # 최대 대기 시간 (Long Polling)
SQS_VISIBILITY_TIMEOUT = 300  # 메시지 처리 시간 (5분)

# EC2 Self-Stop 설정 (비용 최적화)
IDLE_STOP_THRESHOLD = int(os.getenv("EC2_IDLE_STOP_THRESHOLD", "5"))  # 연속 빈 폴링 5회 = 100초


def _handle_signal(sig, frame):
    """
    Graceful shutdown 핸들러
    
    50명 원장 확장 대비: 현재 처리 중인 작업 완료 후 종료
    """
    global _shutdown, _current_job_receipt_handle
    signal_name = signal.Signals(sig).name
    logger.info(
        "Received %s, initiating graceful shutdown... | current_job=%s",
        signal_name,
        "processing" if _current_job_receipt_handle else "idle",
    )
    _shutdown = True
    # 현재 작업이 있으면 완료될 때까지 대기 (메인 루프에서 처리)


def _stop_self_ec2() -> None:
    """
    SQS 큐가 연속으로 비어있을 때 EC2 인스턴스 자동 종료
    
    비용 최적화: idle 상태 인스턴스 자동 종료로 월 $30-50 절감
    IMDSv2를 사용하여 안전하게 인스턴스 메타데이터 조회
    """
    try:
        # EC2 메타데이터에서 인스턴스 정보 가져오기 (IMDSv2)
        token = requests.put(
            "http://169.254.169.254/latest/api/token",
            headers={"X-aws-ec2-metadata-token-ttl-seconds": "21600"},
            timeout=2,
        ).text
        
        headers = {"X-aws-ec2-metadata-token": token}
        instance_id = requests.get(
            "http://169.254.169.254/latest/meta-data/instance-id",
            headers=headers,
            timeout=2,
        ).text
        
        region = requests.get(
            "http://169.254.169.254/latest/meta-data/placement/region",
            headers=headers,
            timeout=2,
        ).text
        
        ec2 = boto3.client("ec2", region_name=region)
        ec2.stop_instances(InstanceIds=[instance_id])
        
        logger.info("EC2 instance stopped due to idle queues: instance_id=%s", instance_id)
        
    except Exception as e:
        logger.exception("EC2 self-stop failed (ignored): %s", e)


def main() -> int:
    """
    SQS 기반 AI Worker 메인 루프
    
    Flow:
    1. SQS에서 메시지 Long Polling (CPU 또는 GPU 큐)
    2. 메시지 수신 시 AI 작업 처리
    3. 성공 시 메시지 삭제
    4. 실패 시 메시지는 SQS가 자동으로 재시도 (DLQ로 전송 전까지)
    """
    signal.signal(signal.SIGTERM, _handle_signal)
    signal.signal(signal.SIGINT, _handle_signal)
    
    # GPU 사용 여부 확인 (환경변수)
    use_gpu = os.getenv("AI_WORKER_USE_GPU", "false").lower() == "true"
    queue = AISQSAdapter()
    
    logger.info(
        "AI Worker (SQS) started | queue=%s | use_gpu=%s | wait_time=%ss",
        queue._get_queue_name(use_gpu=use_gpu),
        use_gpu,
        SQS_WAIT_TIME_SECONDS,
    )
    
    consecutive_errors = 0
    max_consecutive_errors = 10
    consecutive_empty_polls = 0  # 비용 최적화: 빈 폴링 카운터
    
    try:
        while not _shutdown:
            try:
                # SQS Long Polling으로 메시지 수신
                message = queue.receive_message(
                    use_gpu=use_gpu,
                    wait_time_seconds=SQS_WAIT_TIME_SECONDS,
                )
                
                if not message:
                    consecutive_empty_polls += 1
                    consecutive_errors = 0
                    
                    # 연속 빈 폴링이 임계값을 초과하면 EC2 인스턴스 종료
                    if consecutive_empty_polls >= IDLE_STOP_THRESHOLD:
                        logger.info(
                            "All queues empty for %d consecutive polls (threshold=%d), stopping EC2 instance",
                            consecutive_empty_polls,
                            IDLE_STOP_THRESHOLD,
                        )
                        _stop_self_ec2()
                        return 0
                    
                    continue
                
                # 메시지가 있으면 카운터 리셋
                consecutive_empty_polls = 0
                
                receipt_handle = message.get("receipt_handle")
                if not receipt_handle:
                    logger.error("Message missing receipt_handle: %s", message)
                    continue
                
                # 메시지에서 작업 데이터 추출
                job_id = message.get("job_id")
                job_type = message.get("job_type")
                payload = message.get("payload", {})
                message_created_at = message.get("created_at")  # SQS 메시지 수명 추적
                
                if not job_id or not job_type:
                    logger.error("Invalid message format: %s", message)
                    # 잘못된 메시지는 삭제하여 DLQ로 이동하지 않도록
                    queue.delete_message(receipt_handle, use_gpu=use_gpu)
                    continue
                
                # 로그 가시성: request_id 생성 및 메시지 수명 추적
                request_id = str(uuid.uuid4())[:8]
                message_received_at = time.time()
                queue_wait_time = message_received_at - (float(message_created_at) if message_created_at else message_received_at)
                
                logger.info(
                    "SQS_MESSAGE_RECEIVED | request_id=%s | job_id=%s | job_type=%s | use_gpu=%s | queue_wait_sec=%.2f | created_at=%s",
                    request_id,
                    job_id,
                    job_type,
                    use_gpu,
                    queue_wait_time,
                    message_created_at or "unknown",
                )
                
                # Graceful shutdown: 현재 작업 추적 시작
                global _current_job_receipt_handle, _current_job_start_time
                _current_job_receipt_handle = receipt_handle
                _current_job_start_time = time.time()
                
                # 작업을 RUNNING 상태로 변경 (멱등성 보장)
                if not queue.mark_processing(job_id):
                    logger.warning(
                        "Cannot mark AI job %s as RUNNING, skipping",
                        job_id,
                    )
                    # 상태 변경 실패 시 메시지 삭제 (재시도하지 않음)
                    queue.delete_message(receipt_handle, use_gpu=use_gpu)
                    continue
                
                # AIJob 객체 생성
                job = AIJob(
                    id=job_id,
                    type=job_type,
                    payload=payload,
                    source_id=message.get("source_id"),
                )
                
                # AI 작업 처리
                try:
                    processing_start = time.time()
                    result = handle_ai_job(job)
                    processing_duration = time.time() - processing_start
                    
                    # 완료 처리
                    complete_start = time.time()
                    ok, reason = queue.complete_job(
                        job_id=job_id,
                        result_payload=result.result if isinstance(result.result, dict) else {},
                    )
                    complete_duration = time.time() - complete_start
                    
                    if not ok:
                        logger.error(
                            "SQS_JOB_COMPLETE_FAILED | request_id=%s | job_id=%s | reason=%s | processing_duration=%.2f",
                            request_id,
                            job_id,
                            reason,
                            processing_duration,
                        )
                        # 메시지는 삭제하지 않음 (재시도)
                        _current_job_receipt_handle = None
                        _current_job_start_time = None
                        continue
                    
                    # 성공 시 메시지 삭제
                    queue.delete_message(receipt_handle, use_gpu=use_gpu)
                    total_duration = time.time() - _current_job_start_time
                    
                    # 로그 가시성: 전체 처리 시간 추적
                    logger.info(
                        "SQS_JOB_COMPLETED | request_id=%s | job_id=%s | status=%s | processing_duration=%.2f | complete_duration=%.2f | total_duration=%.2f | queue_wait_sec=%.2f",
                        request_id,
                        job_id,
                        result.status,
                        processing_duration,
                        complete_duration,
                        total_duration,
                        queue_wait_time,
                    )
                    consecutive_errors = 0
                    
                    # Graceful shutdown: 작업 완료
                    _current_job_receipt_handle = None
                    _current_job_start_time = None
                    
                    # 종료 신호를 받았으면 루프 종료
                    if _shutdown:
                        logger.info("Graceful shutdown: current job completed, exiting")
                        break
                    
                except Exception as e:
                    processing_duration = time.time() - _current_job_start_time if _current_job_start_time else 0
                    
                    logger.exception(
                        "SQS_JOB_FAILED | request_id=%s | job_id=%s | error=%s | processing_duration=%.2f | queue_wait_sec=%.2f",
                        request_id,
                        job_id,
                        str(e)[:200],
                        processing_duration,
                        queue_wait_time,
                    )
                    
                    # 실패 처리 (작업 상태를 FAILED로 변경)
                    queue.fail_job(
                        job_id=job_id,
                        error_message=str(e)[:2000],
                    )
                    
                    # SQS Visibility Timeout 확인: 처리 시간이 timeout을 초과하면 메시지가 다시 보임
                    if processing_duration > SQS_VISIBILITY_TIMEOUT:
                        logger.warning(
                            "SQS_VISIBILITY_TIMEOUT_EXCEEDED | request_id=%s | job_id=%s | processing_duration=%.2f | visibility_timeout=%d | message_will_reappear",
                            request_id,
                            job_id,
                            processing_duration,
                            SQS_VISIBILITY_TIMEOUT,
                        )
                    
                    # 메시지는 삭제하지 않음 (SQS가 자동으로 재시도)
                    # 재시도 횟수 초과 시 자동으로 DLQ로 이동
                    consecutive_errors += 1
                    
                    # Graceful shutdown: 작업 실패 처리 완료
                    _current_job_receipt_handle = None
                    _current_job_start_time = None
                    
                    if consecutive_errors >= max_consecutive_errors:
                        logger.error(
                            "Too many consecutive errors (%s), shutting down",
                            consecutive_errors,
                        )
                        return 1
                
            except KeyboardInterrupt:
                logger.info("Keyboard interrupt received")
                break
            except Exception as e:
                logger.exception("Unexpected error in main loop: %s", e)
                consecutive_errors += 1
                
                if consecutive_errors >= max_consecutive_errors:
                    logger.error(
                        "Too many consecutive errors (%s), shutting down",
                        consecutive_errors,
                    )
                    return 1
                
                # 에러 후 짧은 대기
                time.sleep(5)
        
        # Graceful shutdown: 현재 작업이 있으면 완료 대기
        if _current_job_receipt_handle:
            logger.info(
                "Graceful shutdown: waiting for current job to complete | receipt_handle=%s",
                _current_job_receipt_handle[:20] + "...",
            )
            # 메인 루프가 현재 작업을 완료할 때까지 대기
            # (실제로는 위의 break로 루프가 종료되므로 여기 도달 시 작업 완료됨)
        
        logger.info("AI Worker shutdown complete")
        return 0
        
    except Exception:
        logger.exception("Fatal error in AI Worker")
        return 1


if __name__ == "__main__":
    import os
    sys.exit(main())


==========================================================================================
# FILE: sqs_main_cpu.py
==========================================================================================
"""
AI Worker CPU - SQS 기반 메인 엔트리포인트

Lite + Basic 큐 처리 (우선순위 로직 포함)
"""

from __future__ import annotations

import json
import logging
import os
import random
import signal
import sys
import time
import uuid
from typing import Optional

import boto3
import requests

from apps.worker.ai_worker.ai.pipelines.dispatcher import handle_ai_job
from apps.worker.ai_worker.ai.pipelines.tier_enforcer import enforce_tier_limits
from apps.shared.contracts.ai_job import AIJob
from apps.shared.contracts.ai_result import AIResult
from src.infrastructure.ai import AISQSAdapter

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] [AI-WORKER-CPU] %(message)s",
)
logger = logging.getLogger("ai_worker_cpu")

_shutdown = False
_current_job_receipt_handle: Optional[str] = None  # Graceful shutdown: 현재 처리 중인 작업 추적
_current_job_start_time: Optional[float] = None  # 로그 가시성: 작업 시작 시간

# SQS Long Polling 설정
SQS_WAIT_TIME_SECONDS = 20  # 최대 대기 시간 (Long Polling)
SQS_VISIBILITY_TIMEOUT = 300  # 메시지 처리 시간 (5분)

# 우선순위 설정 (Basic이 Lite보다 더 자주 폴링)
# 예: basic_weight=3, lite_weight=1 이면 Basic을 3배 더 자주 폴링
BASIC_POLL_WEIGHT = int(os.getenv("AI_WORKER_BASIC_POLL_WEIGHT", "3"))
LITE_POLL_WEIGHT = int(os.getenv("AI_WORKER_LITE_POLL_WEIGHT", "1"))

# EC2 Self-Stop 설정 (비용 최적화)
IDLE_STOP_THRESHOLD = int(os.getenv("EC2_IDLE_STOP_THRESHOLD", "5"))  # 연속 빈 폴링 5회 = 100초


def _handle_signal(sig, frame):
    """
    Graceful shutdown 핸들러
    
    50명 원장 확장 대비: 현재 처리 중인 작업 완료 후 종료
    """
    global _shutdown, _current_job_receipt_handle
    signal_name = signal.Signals(sig).name
    logger.info(
        "Received %s, initiating graceful shutdown... | current_job=%s",
        signal_name,
        "processing" if _current_job_receipt_handle else "idle",
    )
    _shutdown = True
    # 현재 작업이 있으면 완료될 때까지 대기 (메인 루프에서 처리)


def _stop_self_ec2() -> None:
    """
    SQS 큐가 연속으로 비어있을 때 EC2 인스턴스 자동 종료
    
    비용 최적화: idle 상태 인스턴스 자동 종료로 월 $30-50 절감
    IMDSv2를 사용하여 안전하게 인스턴스 메타데이터 조회
    """
    try:
        # EC2 메타데이터에서 인스턴스 정보 가져오기 (IMDSv2)
        token = requests.put(
            "http://169.254.169.254/latest/api/token",
            headers={"X-aws-ec2-metadata-token-ttl-seconds": "21600"},
            timeout=2,
        ).text
        
        headers = {"X-aws-ec2-metadata-token": token}
        instance_id = requests.get(
            "http://169.254.169.254/latest/meta-data/instance-id",
            headers=headers,
            timeout=2,
        ).text
        
        region = requests.get(
            "http://169.254.169.254/latest/meta-data/placement/region",
            headers=headers,
            timeout=2,
        ).text
        
        ec2 = boto3.client("ec2", region_name=region)
        ec2.stop_instances(InstanceIds=[instance_id])
        
        logger.info("EC2 instance stopped due to idle queues: instance_id=%s", instance_id)
        
    except Exception as e:
        logger.exception("EC2 self-stop failed (ignored): %s", e)


def _weighted_poll(queue: AISQSAdapter) -> tuple[Optional[dict], str]:
    """
    Weighted polling으로 Basic과 Lite 큐에서 메시지 수신
    
    Basic을 더 자주 폴링하여 Lite가 Basic을 굶주리게 하지 않도록 함
    
    Returns:
        tuple: (message, tier) 또는 (None, tier)
    """
    # Weighted random selection
    total_weight = BASIC_POLL_WEIGHT + LITE_POLL_WEIGHT
    rand = random.randint(1, total_weight)
    
    if rand <= BASIC_POLL_WEIGHT:
        # Basic 큐 폴링
        tier = "basic"
        message = queue.receive_message(tier="basic", wait_time_seconds=SQS_WAIT_TIME_SECONDS)
        return message, tier
    else:
        # Lite 큐 폴링
        tier = "lite"
        message = queue.receive_message(tier="lite", wait_time_seconds=SQS_WAIT_TIME_SECONDS)
        return message, tier


def main() -> int:
    """
    SQS 기반 AI Worker CPU 메인 루프
    
    Lite + Basic 큐 처리 (우선순위 로직 포함)
    
    Flow:
    1. Weighted polling으로 Basic/Lite 큐에서 메시지 수신
    2. 메시지 수신 시 AI 작업 처리
    3. 성공 시 메시지 삭제
    4. 실패 시 메시지는 SQS가 자동으로 재시도 (DLQ로 전송 전까지)
    """
    signal.signal(signal.SIGTERM, _handle_signal)
    signal.signal(signal.SIGINT, _handle_signal)
    
    queue = AISQSAdapter()
    
    logger.info(
        "AI Worker CPU started | queues=[lite, basic] | weights=[lite=%d, basic=%d] | wait_time=%ss",
        LITE_POLL_WEIGHT,
        BASIC_POLL_WEIGHT,
        SQS_WAIT_TIME_SECONDS,
    )
    
    consecutive_errors = 0
    max_consecutive_errors = 10
    consecutive_empty_polls = 0  # 비용 최적화: 빈 폴링 카운터
    
    try:
        while not _shutdown:
            try:
                # Weighted polling으로 메시지 수신
                message, tier = _weighted_poll(queue)
                
                if not message:
                    consecutive_empty_polls += 1
                    consecutive_errors = 0
                    
                    # 연속 빈 폴링이 임계값을 초과하면 EC2 인스턴스 종료
                    if consecutive_empty_polls >= IDLE_STOP_THRESHOLD:
                        logger.info(
                            "All queues empty for %d consecutive polls (threshold=%d), stopping EC2 instance",
                            consecutive_empty_polls,
                            IDLE_STOP_THRESHOLD,
                        )
                        _stop_self_ec2()
                        return 0
                    
                    continue
                
                # 메시지가 있으면 카운터 리셋
                consecutive_empty_polls = 0
                
                receipt_handle = message.get("receipt_handle")
                if not receipt_handle:
                    logger.error("Message missing receipt_handle: %s", message)
                    continue
                
                # 메시지에서 작업 데이터 추출
                job_id = message.get("job_id")
                job_type = message.get("job_type")
                tier_from_message = message.get("tier", tier)  # 메시지의 tier 우선
                payload = message.get("payload", {})
                message_created_at = message.get("created_at")  # SQS 메시지 수명 추적
                
                if not job_id or not job_type:
                    logger.error("Invalid message format: %s", message)
                    # 잘못된 메시지는 삭제하여 DLQ로 이동하지 않도록
                    queue.delete_message(receipt_handle, tier=tier_from_message)
                    continue
                
                # 로그 가시성: request_id 생성 및 메시지 수명 추적
                request_id = str(uuid.uuid4())[:8]
                message_received_at = time.time()
                queue_wait_time = message_received_at - (float(message_created_at) if message_created_at else message_received_at)
                
                logger.info(
                    "SQS_MESSAGE_RECEIVED | request_id=%s | job_id=%s | job_type=%s | tier=%s | queue_wait_sec=%.2f | created_at=%s",
                    request_id,
                    job_id,
                    job_type,
                    tier_from_message,
                    queue_wait_time,
                    message_created_at or "unknown",
                )
                
                # Graceful shutdown: 현재 작업 추적 시작
                global _current_job_receipt_handle, _current_job_start_time
                _current_job_receipt_handle = receipt_handle
                _current_job_start_time = time.time()
                
                # Tier 제한 검증
                allowed, error_msg = enforce_tier_limits(
                    tier=tier_from_message,
                    job_type=job_type,
                )
                if not allowed:
                    logger.error(
                        "Tier limit violation: job_id=%s, tier=%s, job_type=%s, error=%s",
                        job_id,
                        tier_from_message,
                        job_type,
                        error_msg,
                    )
                    # Tier 제한 위반 시 실패 처리
                    queue.fail_job(
                        job_id=job_id,
                        error_message=error_msg or "Tier limit violation",
                    )
                    queue.delete_message(receipt_handle, tier=tier_from_message)
                    continue
                
                # 작업을 RUNNING 상태로 변경 (멱등성 보장)
                if not queue.mark_processing(job_id):
                    logger.warning(
                        "Cannot mark AI job %s as RUNNING, skipping",
                        job_id,
                    )
                    # 상태 변경 실패 시 메시지 삭제 (재시도하지 않음)
                    queue.delete_message(receipt_handle, tier=tier_from_message)
                    continue
                
                # AIJob 객체 생성
                job = AIJob(
                    id=job_id,
                    type=job_type,
                    payload=payload,
                    source_id=message.get("source_id"),
                )
                
                # AI 작업 처리
                try:
                    processing_start = time.time()
                    result = handle_ai_job(job)
                    processing_duration = time.time() - processing_start
                    
                    # 완료 처리
                    complete_start = time.time()
                    ok, reason = queue.complete_job(
                        job_id=job_id,
                        result_payload=result.result if isinstance(result.result, dict) else {},
                    )
                    complete_duration = time.time() - complete_start
                    
                    if not ok:
                        logger.error(
                            "SQS_JOB_COMPLETE_FAILED | request_id=%s | job_id=%s | tier=%s | reason=%s | processing_duration=%.2f",
                            request_id,
                            job_id,
                            tier_from_message,
                            reason,
                            processing_duration,
                        )
                        # 메시지는 삭제하지 않음 (재시도)
                        _current_job_receipt_handle = None
                        _current_job_start_time = None
                        continue
                    
                    # 성공 시 메시지 삭제
                    queue.delete_message(receipt_handle, tier=tier_from_message)
                    total_duration = time.time() - _current_job_start_time
                    
                    # 로그 가시성: 전체 처리 시간 추적
                    logger.info(
                        "SQS_JOB_COMPLETED | request_id=%s | job_id=%s | tier=%s | job_type=%s | status=%s | processing_duration=%.2f | complete_duration=%.2f | total_duration=%.2f | queue_wait_sec=%.2f",
                        request_id,
                        job_id,
                        tier_from_message,
                        job_type,
                        result.status,
                        processing_duration,
                        complete_duration,
                        total_duration,
                        queue_wait_time,
                    )
                    consecutive_errors = 0
                    
                    # Graceful shutdown: 작업 완료
                    _current_job_receipt_handle = None
                    _current_job_start_time = None
                    
                    # 종료 신호를 받았으면 루프 종료
                    if _shutdown:
                        logger.info("Graceful shutdown: current job completed, exiting")
                        break
                    
                except Exception as e:
                    processing_duration = time.time() - _current_job_start_time if _current_job_start_time else 0
                    
                    logger.exception(
                        "SQS_JOB_FAILED | request_id=%s | job_id=%s | tier=%s | error=%s | processing_duration=%.2f | queue_wait_sec=%.2f",
                        request_id,
                        job_id,
                        tier_from_message,
                        str(e)[:200],
                        processing_duration,
                        queue_wait_time,
                    )
                    
                    # 실패 처리 (작업 상태를 FAILED로 변경)
                    queue.fail_job(
                        job_id=job_id,
                        error_message=str(e)[:2000],
                    )
                    
                    # SQS Visibility Timeout 확인: 처리 시간이 timeout을 초과하면 메시지가 다시 보임
                    if processing_duration > SQS_VISIBILITY_TIMEOUT:
                        logger.warning(
                            "SQS_VISIBILITY_TIMEOUT_EXCEEDED | request_id=%s | job_id=%s | processing_duration=%.2f | visibility_timeout=%d | message_will_reappear",
                            request_id,
                            job_id,
                            processing_duration,
                            SQS_VISIBILITY_TIMEOUT,
                        )
                    
                    # 메시지는 삭제하지 않음 (SQS가 자동으로 재시도)
                    # 재시도 횟수 초과 시 자동으로 DLQ로 이동
                    consecutive_errors += 1
                    
                    # Graceful shutdown: 작업 실패 처리 완료
                    _current_job_receipt_handle = None
                    _current_job_start_time = None
                    
                    if consecutive_errors >= max_consecutive_errors:
                        logger.error(
                            "Too many consecutive errors (%s), shutting down",
                            consecutive_errors,
                        )
                        return 1
                
            except KeyboardInterrupt:
                logger.info("Keyboard interrupt received")
                break
            except Exception as e:
                logger.exception("Unexpected error in main loop: %s", e)
                consecutive_errors += 1
                
                if consecutive_errors >= max_consecutive_errors:
                    logger.error(
                        "Too many consecutive errors (%s), shutting down",
                        consecutive_errors,
                    )
                    return 1
                
                # 에러 후 짧은 대기
                time.sleep(5)
        
        # Graceful shutdown: 현재 작업이 있으면 완료 대기
        if _current_job_receipt_handle:
            logger.info(
                "Graceful shutdown: waiting for current job to complete | receipt_handle=%s",
                _current_job_receipt_handle[:20] + "...",
            )
        
        logger.info("AI Worker CPU shutdown complete")
        return 0
        
    except Exception:
        logger.exception("Fatal error in AI Worker CPU")
        return 1


if __name__ == "__main__":
    sys.exit(main())


==========================================================================================
# FILE: sqs_main_gpu.py
==========================================================================================
"""
AI Worker GPU - SQS 기반 메인 엔트리포인트

Premium 큐 처리 (향후 GPU 지원)
"""

from __future__ import annotations

import json
import logging
import os
import signal
import sys
import time
import uuid
from typing import Optional

import boto3
import requests

from apps.worker.ai_worker.ai.pipelines.dispatcher import handle_ai_job
from apps.worker.ai_worker.ai.pipelines.tier_enforcer import enforce_tier_limits
from apps.shared.contracts.ai_job import AIJob
from apps.shared.contracts.ai_result import AIResult
from src.infrastructure.ai import AISQSAdapter

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] [AI-WORKER-GPU] %(message)s",
)
logger = logging.getLogger("ai_worker_gpu")

_shutdown = False

# SQS Long Polling 설정
SQS_WAIT_TIME_SECONDS = 20  # 최대 대기 시간 (Long Polling)
SQS_VISIBILITY_TIMEOUT = 600  # 메시지 처리 시간 (10분, GPU 작업은 더 오래 걸릴 수 있음)

# EC2 Self-Stop 설정 (비용 최적화)
IDLE_STOP_THRESHOLD = int(os.getenv("EC2_IDLE_STOP_THRESHOLD", "5"))  # 연속 빈 폴링 5회 = 100초


def _handle_signal(sig, frame):
    """
    Graceful shutdown 핸들러
    
    50명 원장 확장 대비: 현재 처리 중인 작업 완료 후 종료
    """
    global _shutdown, _current_job_receipt_handle
    signal_name = signal.Signals(sig).name
    logger.info(
        "Received %s, initiating graceful shutdown... | current_job=%s",
        signal_name,
        "processing" if _current_job_receipt_handle else "idle",
    )
    _shutdown = True
    # 현재 작업이 있으면 완료될 때까지 대기 (메인 루프에서 처리)


def _stop_self_ec2() -> None:
    """
    SQS 큐가 연속으로 비어있을 때 EC2 인스턴스 자동 종료
    
    비용 최적화: idle 상태 인스턴스 자동 종료로 월 $30-50 절감
    IMDSv2를 사용하여 안전하게 인스턴스 메타데이터 조회
    """
    try:
        # EC2 메타데이터에서 인스턴스 정보 가져오기 (IMDSv2)
        token = requests.put(
            "http://169.254.169.254/latest/api/token",
            headers={"X-aws-ec2-metadata-token-ttl-seconds": "21600"},
            timeout=2,
        ).text
        
        headers = {"X-aws-ec2-metadata-token": token}
        instance_id = requests.get(
            "http://169.254.169.254/latest/meta-data/instance-id",
            headers=headers,
            timeout=2,
        ).text
        
        region = requests.get(
            "http://169.254.169.254/latest/meta-data/placement/region",
            headers=headers,
            timeout=2,
        ).text
        
        ec2 = boto3.client("ec2", region_name=region)
        ec2.stop_instances(InstanceIds=[instance_id])
        
        logger.info("EC2 instance stopped due to idle queues: instance_id=%s", instance_id)
        
    except Exception as e:
        logger.exception("EC2 self-stop failed (ignored): %s", e)


def main() -> int:
    """
    SQS 기반 AI Worker GPU 메인 루프
    
    Premium 큐 처리 (향후 GPU 지원)
    
    Flow:
    1. SQS Long Polling으로 Premium 큐에서 메시지 수신
    2. 메시지 수신 시 AI 작업 처리 (GPU 사용)
    3. 성공 시 메시지 삭제
    4. 실패 시 메시지는 SQS가 자동으로 재시도 (DLQ로 전송 전까지)
    """
    signal.signal(signal.SIGTERM, _handle_signal)
    signal.signal(signal.SIGINT, _handle_signal)
    
    queue = AISQSAdapter()
    
    logger.info(
        "AI Worker GPU started | queue=premium | wait_time=%ss",
        SQS_WAIT_TIME_SECONDS,
    )
    
    consecutive_errors = 0
    max_consecutive_errors = 10
    consecutive_empty_polls = 0  # 비용 최적화: 빈 폴링 카운터
    
    try:
        while not _shutdown:
            try:
                # SQS Long Polling으로 Premium 큐에서 메시지 수신
                message = queue.receive_message(
                    tier="premium",
                    wait_time_seconds=SQS_WAIT_TIME_SECONDS,
                )
                
                if not message:
                    consecutive_empty_polls += 1
                    consecutive_errors = 0
                    
                    # 연속 빈 폴링이 임계값을 초과하면 EC2 인스턴스 종료
                    if consecutive_empty_polls >= IDLE_STOP_THRESHOLD:
                        logger.info(
                            "All queues empty for %d consecutive polls (threshold=%d), stopping EC2 instance",
                            consecutive_empty_polls,
                            IDLE_STOP_THRESHOLD,
                        )
                        _stop_self_ec2()
                        return 0
                    
                    continue
                
                # 메시지가 있으면 카운터 리셋
                consecutive_empty_polls = 0
                
                receipt_handle = message.get("receipt_handle")
                if not receipt_handle:
                    logger.error("Message missing receipt_handle: %s", message)
                    continue
                
                # 메시지에서 작업 데이터 추출
                job_id = message.get("job_id")
                job_type = message.get("job_type")
                tier = message.get("tier", "premium")
                payload = message.get("payload", {})
                message_created_at = message.get("created_at")  # SQS 메시지 수명 추적
                
                if not job_id or not job_type:
                    logger.error("Invalid message format: %s", message)
                    # 잘못된 메시지는 삭제하여 DLQ로 이동하지 않도록
                    queue.delete_message(receipt_handle, tier=tier)
                    continue
                
                # 로그 가시성: request_id 생성 및 메시지 수명 추적
                request_id = str(uuid.uuid4())[:8]
                message_received_at = time.time()
                queue_wait_time = message_received_at - (float(message_created_at) if message_created_at else message_received_at)
                
                logger.info(
                    "SQS_MESSAGE_RECEIVED | request_id=%s | job_id=%s | job_type=%s | tier=%s | queue_wait_sec=%.2f | created_at=%s",
                    request_id,
                    job_id,
                    job_type,
                    tier,
                    queue_wait_time,
                    message_created_at or "unknown",
                )
                
                # Graceful shutdown: 현재 작업 추적 시작
                global _current_job_receipt_handle, _current_job_start_time
                _current_job_receipt_handle = receipt_handle
                _current_job_start_time = time.time()
                
                # Tier 제한 검증 (Premium만 허용)
                if tier != "premium":
                    logger.error(
                        "GPU worker only processes premium tier: job_id=%s, tier=%s",
                        job_id,
                        tier,
                    )
                    queue.fail_job(
                        job_id=job_id,
                        error_message=f"GPU worker only processes premium tier, got {tier}",
                    )
                    queue.delete_message(receipt_handle, tier=tier)
                    continue
                
                # Tier 제한 검증
                allowed, error_msg = enforce_tier_limits(
                    tier=tier,
                    job_type=job_type,
                )
                if not allowed:
                    logger.error(
                        "Tier limit violation: job_id=%s, tier=%s, job_type=%s, error=%s",
                        job_id,
                        tier,
                        job_type,
                        error_msg,
                    )
                    queue.fail_job(
                        job_id=job_id,
                        error_message=error_msg or "Tier limit violation",
                    )
                    queue.delete_message(receipt_handle, tier=tier)
                    continue
                
                # 작업을 RUNNING 상태로 변경 (멱등성 보장)
                if not queue.mark_processing(job_id):
                    logger.warning(
                        "Cannot mark AI job %s as RUNNING, skipping",
                        job_id,
                    )
                    # 상태 변경 실패 시 메시지 삭제 (재시도하지 않음)
                    queue.delete_message(receipt_handle, tier=tier)
                    continue
                
                # AIJob 객체 생성
                job = AIJob(
                    id=job_id,
                    type=job_type,
                    payload=payload,
                    source_id=message.get("source_id"),
                )
                
                # AI 작업 처리 (GPU 사용)
                try:
                    processing_start = time.time()
                    result = handle_ai_job(job)
                    processing_duration = time.time() - processing_start
                    
                    # 완료 처리
                    complete_start = time.time()
                    ok, reason = queue.complete_job(
                        job_id=job_id,
                        result_payload=result.result if isinstance(result.result, dict) else {},
                    )
                    complete_duration = time.time() - complete_start
                    
                    if not ok:
                        logger.error(
                            "SQS_JOB_COMPLETE_FAILED | request_id=%s | job_id=%s | tier=%s | reason=%s | processing_duration=%.2f",
                            request_id,
                            job_id,
                            tier,
                            reason,
                            processing_duration,
                        )
                        # 메시지는 삭제하지 않음 (재시도)
                        _current_job_receipt_handle = None
                        _current_job_start_time = None
                        continue
                    
                    # 성공 시 메시지 삭제
                    queue.delete_message(receipt_handle, tier=tier)
                    total_duration = time.time() - _current_job_start_time
                    
                    # 로그 가시성: 전체 처리 시간 추적
                    logger.info(
                        "SQS_JOB_COMPLETED | request_id=%s | job_id=%s | tier=%s | status=%s | processing_duration=%.2f | complete_duration=%.2f | total_duration=%.2f | queue_wait_sec=%.2f",
                        request_id,
                        job_id,
                        tier,
                        result.status,
                        processing_duration,
                        complete_duration,
                        total_duration,
                        queue_wait_time,
                    )
                    consecutive_errors = 0
                    
                    # Graceful shutdown: 작업 완료
                    _current_job_receipt_handle = None
                    _current_job_start_time = None
                    
                    # 종료 신호를 받았으면 루프 종료
                    if _shutdown:
                        logger.info("Graceful shutdown: current job completed, exiting")
                        break
                    
                except Exception as e:
                    processing_duration = time.time() - _current_job_start_time if _current_job_start_time else 0
                    
                    logger.exception(
                        "SQS_JOB_FAILED | request_id=%s | job_id=%s | tier=%s | error=%s | processing_duration=%.2f | queue_wait_sec=%.2f",
                        request_id,
                        job_id,
                        tier,
                        str(e)[:200],
                        processing_duration,
                        queue_wait_time,
                    )
                    
                    # 실패 처리 (작업 상태를 FAILED로 변경)
                    queue.fail_job(
                        job_id=job_id,
                        error_message=str(e)[:2000],
                    )
                    
                    # SQS Visibility Timeout 확인: 처리 시간이 timeout을 초과하면 메시지가 다시 보임
                    if processing_duration > SQS_VISIBILITY_TIMEOUT:
                        logger.warning(
                            "SQS_VISIBILITY_TIMEOUT_EXCEEDED | request_id=%s | job_id=%s | processing_duration=%.2f | visibility_timeout=%d | message_will_reappear",
                            request_id,
                            job_id,
                            processing_duration,
                            SQS_VISIBILITY_TIMEOUT,
                        )
                    
                    # 메시지는 삭제하지 않음 (SQS가 자동으로 재시도)
                    # 재시도 횟수 초과 시 자동으로 DLQ로 이동
                    consecutive_errors += 1
                    
                    # Graceful shutdown: 작업 실패 처리 완료
                    _current_job_receipt_handle = None
                    _current_job_start_time = None
                    
                    if consecutive_errors >= max_consecutive_errors:
                        logger.error(
                            "Too many consecutive errors (%s), shutting down",
                            consecutive_errors,
                        )
                        return 1
                
            except KeyboardInterrupt:
                logger.info("Keyboard interrupt received")
                break
            except Exception as e:
                logger.exception("Unexpected error in main loop: %s", e)
                consecutive_errors += 1
                
                if consecutive_errors >= max_consecutive_errors:
                    logger.error(
                        "Too many consecutive errors (%s), shutting down",
                        consecutive_errors,
                    )
                    return 1
                
                # 에러 후 짧은 대기
                time.sleep(5)
        
        # Graceful shutdown: 현재 작업이 있으면 완료 대기
        if _current_job_receipt_handle:
            logger.info(
                "Graceful shutdown: waiting for current job to complete | receipt_handle=%s",
                _current_job_receipt_handle[:20] + "...",
            )
        
        logger.info("AI Worker GPU shutdown complete")
        return 0
        
    except Exception:
        logger.exception("Fatal error in AI Worker GPU")
        return 1


if __name__ == "__main__":
    sys.exit(main())


==========================================================================================
# FILE: ai/__init__.py
==========================================================================================
# apps/worker/ai/__init__.py
# Celery 제거됨. run.py 단일 진입


==========================================================================================
# FILE: ai/config.py
==========================================================================================
# apps/worker/ai/config.py
from __future__ import annotations

import os
from dataclasses import dataclass
from typing import List, Optional


def _env(name: str, default: Optional[str] = None) -> Optional[str]:
    v = os.getenv(name)
    return v if v not in (None, "") else default


@dataclass(frozen=True)
class AIConfig:
    # OCR
    OCR_ENGINE: str = "google"  # google | tesseract | auto
    GOOGLE_APPLICATION_CREDENTIALS: Optional[str] = None  # optional (google sdk default)

    # Segmentation
    QUESTION_SEGMENTATION_ENGINE: str = "auto"  # yolo|opencv|template|auto

    # YOLO (optional)
    YOLO_QUESTION_MODEL_PATH: Optional[str] = None
    YOLO_QUESTION_INPUT_SIZE: int = 640
    YOLO_QUESTION_CONF_THRESHOLD: float = 0.4
    YOLO_QUESTION_IOU_THRESHOLD: float = 0.5

    # Embedding
    EMBEDDING_BACKEND: str = "auto"  # local|openai|auto
    EMBEDDING_LOCAL_MODEL: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    EMBEDDING_OPENAI_MODEL: str = "text-embedding-3-small"
    OPENAI_API_KEY: Optional[str] = None

    # Problem generation
    PROBLEM_GEN_MODEL: str = "gpt-4.1-mini"  # default

    @staticmethod
    def load() -> "AIConfig":
        return AIConfig(
            OCR_ENGINE=_env("OCR_ENGINE", "google") or "google",
            GOOGLE_APPLICATION_CREDENTIALS=_env("GOOGLE_APPLICATION_CREDENTIALS"),

            QUESTION_SEGMENTATION_ENGINE=_env("QUESTION_SEGMENTATION_ENGINE", "auto") or "auto",

            YOLO_QUESTION_MODEL_PATH=_env("YOLO_QUESTION_MODEL_PATH"),
            YOLO_QUESTION_INPUT_SIZE=int(_env("YOLO_QUESTION_INPUT_SIZE", "640") or "640"),
            YOLO_QUESTION_CONF_THRESHOLD=float(_env("YOLO_QUESTION_CONF_THRESHOLD", "0.4") or "0.4"),
            YOLO_QUESTION_IOU_THRESHOLD=float(_env("YOLO_QUESTION_IOU_THRESHOLD", "0.5") or "0.5"),

            EMBEDDING_BACKEND=_env("EMBEDDING_BACKEND", "auto") or "auto",
            EMBEDDING_LOCAL_MODEL=_env(
                "EMBEDDING_LOCAL_MODEL",
                "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            ) or "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
            EMBEDDING_OPENAI_MODEL=_env("EMBEDDING_OPENAI_MODEL", "text-embedding-3-small") or "text-embedding-3-small",
            OPENAI_API_KEY=_env("OPENAI_API_KEY") or _env("EMBEDDING_OPENAI_API_KEY"),

            PROBLEM_GEN_MODEL=_env("PROBLEM_GEN_MODEL", "gpt-4.1-mini") or "gpt-4.1-mini",
        )


==========================================================================================
# FILE: ai/detection/__init__.py
==========================================================================================
# apps/worker/ai/detection/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai/detection/segment_dispatcher.py
==========================================================================================
# apps/worker/ai/detection/segment_dispatcher.py
from __future__ import annotations

from typing import List, Tuple

from apps.worker.ai_worker.ai.config import AIConfig
from apps.worker.ai_worker.ai.detection.segment_opencv import segment_questions_opencv
from apps.worker.ai_worker.ai.detection.segment_yolo import segment_questions_yolo

BBox = Tuple[int, int, int, int]


def segment_questions(image_path: str) -> List[BBox]:
    """
    worker-side segmentation single entrypoint
    """
    cfg = AIConfig.load()
    engine = (cfg.QUESTION_SEGMENTATION_ENGINE or "auto").lower()

    if engine == "opencv":
        return segment_questions_opencv(image_path)
    if engine == "yolo":
        return segment_questions_yolo(image_path)

    # auto: yolo -> opencv
    try:
        boxes = segment_questions_yolo(image_path)
        if boxes:
            return boxes
    except Exception:
        pass
    return segment_questions_opencv(image_path)


==========================================================================================
# FILE: ai/detection/segment_opencv.py
==========================================================================================
# apps/worker/ai/detection/segment_opencv.py
from __future__ import annotations

from typing import List, Tuple
import cv2  # type: ignore

BBox = Tuple[int, int, int, int]


def segment_questions_opencv(image_path: str) -> List[BBox]:
    """
    legacy 섞여있던 opencv segmentation 정리본
    입력: image_path
    출력: [(x,y,w,h), ...]
    """
    image_bgr = cv2.imread(image_path)
    if image_bgr is None:
        return []

    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5, 5), 0)

    _, thresh = cv2.threshold(
        blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU
    )

    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))
    dilated = cv2.dilate(thresh, kernel, iterations=1)

    contours, _ = cv2.findContours(
        dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
    )

    h_img, w_img = gray.shape[:2]
    min_area = w_img * h_img * 0.005
    max_area = w_img * h_img * 0.9

    boxes: List[BBox] = []
    for cnt in contours:
        x, y, w, h = cv2.boundingRect(cnt)
        area = w * h
        if area < min_area or area > max_area:
            continue

        aspect = h / (w + 1e-6)
        if aspect < 0.3:
            continue

        boxes.append((x, y, w, h))

    boxes.sort(key=lambda b: (b[1], b[0]))
    return boxes


==========================================================================================
# FILE: ai/detection/segment_yolo.py
==========================================================================================
# apps/worker/ai/detection/segment_yolo.py
from __future__ import annotations

from functools import lru_cache
from typing import List, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore

from apps.worker.ai_worker.ai.config import AIConfig

BBox = Tuple[int, int, int, int]


class YoloNotConfiguredError(RuntimeError):
    pass


try:
    import onnxruntime as ort  # type: ignore
    _HAS_ORT = True
except Exception:
    _HAS_ORT = False


@lru_cache()
def _get_session():
    cfg = AIConfig.load()

    if not _HAS_ORT:
        raise YoloNotConfiguredError("onnxruntime not installed")

    if not cfg.YOLO_QUESTION_MODEL_PATH:
        raise YoloNotConfiguredError("YOLO_QUESTION_MODEL_PATH not set")

    providers = ["CPUExecutionProvider"]
    return ort.InferenceSession(str(cfg.YOLO_QUESTION_MODEL_PATH), providers=providers)


def _preprocess(image_bgr, input_size: int):
    h0, w0 = image_bgr.shape[:2]
    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
    resized = cv2.resize(image_rgb, (input_size, input_size))
    resized = resized.astype(np.float32) / 255.0

    tensor = np.transpose(resized, (2, 0, 1))
    tensor = np.expand_dims(tensor, axis=0)

    scale_x = w0 / float(input_size)
    scale_y = h0 / float(input_size)
    return tensor, scale_x, scale_y


def _nms(boxes, scores, iou_threshold: float):
    if len(boxes) == 0:
        return []

    boxes = boxes.astype(np.float32)
    scores = scores.astype(np.float32)

    x1, y1, x2, y2 = boxes.T
    areas = (x2 - x1) * (y2 - y1)
    order = scores.argsort()[::-1]

    keep = []
    while order.size > 0:
        i = int(order[0])
        keep.append(i)

        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])

        w = np.maximum(0.0, xx2 - xx1)
        h = np.maximum(0.0, yy2 - yy1)
        inter = w * h

        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-6)
        idxs = np.where(iou <= iou_threshold)[0]
        order = order[idxs + 1]

    return keep


def segment_questions_yolo(image_path: str) -> List[BBox]:
    cfg = AIConfig.load()
    sess = _get_session()

    image_bgr = cv2.imread(image_path)
    if image_bgr is None:
        return []

    input_tensor, scale_x, scale_y = _preprocess(image_bgr, cfg.YOLO_QUESTION_INPUT_SIZE)

    input_name = sess.get_inputs()[0].name
    outputs = sess.run(None, {input_name: input_tensor})
    preds = outputs[0]
    if preds.ndim == 3:
        preds = preds[0]

    boxes = []
    scores = []

    for det in preds:
        cx, cy, w, h, obj_conf = det[:5]
        cls_scores = det[5:]
        cls_conf = float(cls_scores.max()) if cls_scores.size > 0 else 1.0

        score = float(obj_conf * cls_conf)
        if score < cfg.YOLO_QUESTION_CONF_THRESHOLD:
            continue

        x1 = (cx - w / 2.0) * scale_x
        y1 = (cy - h / 2.0) * scale_y
        x2 = (cx + w / 2.0) * scale_x
        y2 = (cy + h / 2.0) * scale_y

        boxes.append([x1, y1, x2, y2])
        scores.append(score)

    if not boxes:
        return []

    boxes_np = np.array(boxes)
    scores_np = np.array(scores)

    keep_idx = _nms(boxes_np, scores_np, cfg.YOLO_QUESTION_IOU_THRESHOLD)

    final: List[BBox] = []
    for i in keep_idx:
        x1, y1, x2, y2 = boxes_np[i]
        final.append((int(x1), int(y1), int(x2 - x1), int(y2 - y1)))

    final.sort(key=lambda b: (b[1], b[0]))
    return final


==========================================================================================
# FILE: ai/embedding/__init__.py
==========================================================================================
# apps/worker/ai/embedding/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai/embedding/service.py
==========================================================================================
# apps/worker/ai/embedding/service.py
from __future__ import annotations

from dataclasses import dataclass
from typing import List, Optional, Sequence, Literal
import math

from apps.worker.ai_worker.ai.config import AIConfig

EmbeddingBackendName = Literal["local", "openai"]


@dataclass
class EmbeddingBatch:
    vectors: List[List[float]]
    backend: EmbeddingBackendName


def cosine_similarity(a: Sequence[float], b: Sequence[float]) -> float:
    """
    cosine similarity normalized to 0~1
    """
    if not a or not b:
        return 0.0
    if len(a) != len(b):
        return 0.0

    dot = 0.0
    na = 0.0
    nb = 0.0
    for x, y in zip(a, b):
        dot += x * y
        na += x * x
        nb += y * y

    if na <= 0.0 or nb <= 0.0:
        return 0.0

    sim = dot / (math.sqrt(na) * math.sqrt(nb))
    sim = max(-1.0, min(1.0, sim))
    return (sim + 1.0) / 2.0


# -------- local (sentence-transformers) --------
try:
    from sentence_transformers import SentenceTransformer  # type: ignore
except Exception:
    SentenceTransformer = None  # type: ignore

_local_model: Optional["SentenceTransformer"] = None


def _get_local_model() -> "SentenceTransformer":
    global _local_model
    if _local_model is not None:
        return _local_model

    if SentenceTransformer is None:
        raise RuntimeError("sentence-transformers not installed")

    cfg = AIConfig.load()
    _local_model = SentenceTransformer(cfg.EMBEDDING_LOCAL_MODEL)
    return _local_model


def _embed_local(texts: List[str]) -> EmbeddingBatch:
    model = _get_local_model()
    vectors = model.encode(texts, convert_to_numpy=False)
    vectors_list = [list(map(float, v)) for v in vectors]
    return EmbeddingBatch(vectors=vectors_list, backend="local")


# -------- openai --------
try:
    from openai import OpenAI  # type: ignore
except Exception:
    OpenAI = None  # type: ignore

_openai_client: Optional["OpenAI"] = None


def _get_openai_client() -> "OpenAI":
    global _openai_client
    if _openai_client is not None:
        return _openai_client

    if OpenAI is None:
        raise RuntimeError("openai not installed")

    cfg = AIConfig.load()
    if not cfg.OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY not set")

    _openai_client = OpenAI(api_key=cfg.OPENAI_API_KEY)
    return _openai_client


def _embed_openai(texts: List[str]) -> EmbeddingBatch:
    cfg = AIConfig.load()
    client = _get_openai_client()
    response = client.embeddings.create(model=cfg.EMBEDDING_OPENAI_MODEL, input=texts)
    vectors = [list(map(float, d.embedding)) for d in response.data]
    return EmbeddingBatch(vectors=vectors, backend="openai")


def _choose_backend() -> EmbeddingBackendName:
    cfg = AIConfig.load()
    mode = (cfg.EMBEDDING_BACKEND or "auto").lower()

    if mode == "local":
        return "local"
    if mode == "openai":
        return "openai"

    # auto: local 가능하면 local
    if SentenceTransformer is not None:
        try:
            _get_local_model()
            return "local"
        except Exception:
            pass
    return "openai"


def get_embeddings(texts: List[str]) -> EmbeddingBatch:
    if not texts:
        return EmbeddingBatch(vectors=[], backend=_choose_backend())

    backend = _choose_backend()
    norm = [(t or "").strip() for t in texts]

    if backend == "local":
        return _embed_local(norm)
    return _embed_openai(norm)


==========================================================================================
# FILE: ai/handwriting/__init__.py
==========================================================================================
# apps/worker/ai/handwriting/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai/handwriting/detector.py
==========================================================================================
# apps/worker/ai/handwriting/detector.py
from __future__ import annotations

from typing import Dict
import cv2  # type: ignore
import numpy as np  # type: ignore


def analyze_handwriting(image_path: str) -> Dict[str, float]:
    """
    legacy(doc_ai/handwriting/handwriting_detector.py) 이식본
    - 한 이미지에서 필기 흔적/계산식 형태 흔적 여부 점수 반환

    return:
      {
        "writing_score": 0~1,
        "calculation_score": 0~1
      }
    """
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if img is None:
        return {"writing_score": 0.0, "calculation_score": 0.0}

    blur = cv2.GaussianBlur(img, (5, 5), 0)
    edges = cv2.Canny(blur, 50, 150)

    writing_density = float(np.sum(edges > 0) / edges.size)

    sobel_x = cv2.Sobel(blur, cv2.CV_64F, 1, 0, ksize=5)
    sobel_y = cv2.Sobel(blur, cv2.CV_64F, 0, 1, ksize=5)
    grad_mag = (np.mean(np.abs(sobel_x)) + np.mean(np.abs(sobel_y))) / 255.0

    writing_score = min(max(writing_density * 12.0, 0.0), 1.0)
    calculation_score = min(max(grad_mag * 3.0, 0.0), 1.0)

    return {
        "writing_score": float(writing_score),
        "calculation_score": float(calculation_score),
    }


==========================================================================================
# FILE: ai/homework/__init__.py
==========================================================================================



==========================================================================================
# FILE: ai/ocr/__init__.py
==========================================================================================
# apps/worker/ai/ocr/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai/ocr/engine.py
==========================================================================================
from __future__ import annotations
from typing import List
from .schemas import OCRResultPayload, OMRDetectedAnswer


def run_ocr_engine(
    *,
    image_path: str,
) -> OCRResultPayload:
    """
    실제 OCR/OMR 엔진 자리
    - 지금은 더미
    - 나중에 OpenCV / Tesseract / 외부 API 교체
    """

    answers: List[OMRDetectedAnswer] = [
        {
            "question_number": 1,
            "detected": ["B"],
            "confidence": 0.92,
            "marking": "single",
            "status": "ok",
        },
        {
            "question_number": 2,
            "detected": ["D"],
            "confidence": 0.88,
            "marking": "single",
            "status": "ok",
        },
    ]

    return {
        "version": "v1",
        "answers": answers,
        "raw_text": None,
    }


==========================================================================================
# FILE: ai/ocr/google.py
==========================================================================================
# apps/worker/ai/ocr/google.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Optional

# google cloud vision
from google.cloud import vision  # type: ignore


@dataclass
class OCRResult:
    text: str
    confidence: Optional[float] = None
    raw: Optional[Any] = None


def google_ocr(image_path: str) -> OCRResult:
    """
    Worker에서 실행되는 Google OCR
    - service account는 GOOGLE_APPLICATION_CREDENTIALS 또는 기본 환경에 따름
    """
    client = vision.ImageAnnotatorClient()

    with open(image_path, "rb") as f:
        content = f.read()

    image = vision.Image(content=content)
    response = client.text_detection(image=image)

    if getattr(response, "error", None) and response.error.message:
        return OCRResult(text="", confidence=None, raw={"error": response.error.message})

    annotations = getattr(response, "text_annotations", None) or []
    if not annotations:
        return OCRResult(text="", confidence=None, raw=None)

    return OCRResult(
        text=annotations[0].description or "",
        confidence=None,
        raw=None,  # raw를 통째로 넘기면 직렬화 이슈가 생길 수 있어 기본 None
    )


==========================================================================================
# FILE: ai/ocr/schemas.py
==========================================================================================
from __future__ import annotations
from typing import TypedDict, List, Optional


class OMRDetectedAnswer(TypedDict):
    question_number: int
    detected: List[str]
    confidence: float
    marking: str     # single / multi / blank
    status: str      # ok / error


class OCRResultPayload(TypedDict):
    version: str
    answers: List[OMRDetectedAnswer]
    raw_text: Optional[str]


==========================================================================================
# FILE: ai/ocr/tesseract.py
==========================================================================================
# apps/worker/ai/ocr/tesseract.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Optional

from PIL import Image  # type: ignore
import pytesseract  # type: ignore


@dataclass
class OCRResult:
    text: str
    confidence: Optional[float] = None
    raw: Optional[Any] = None


def tesseract_ocr(image_path: str) -> OCRResult:
    img = Image.open(image_path)

    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)
    text = "\n".join(data.get("text", [])).strip()

    confs = [c for c in data.get("conf", []) if c != -1]
    confidence = (sum(confs) / len(confs)) if confs else None

    # raw=data 는 너무 클 수 있어 필요 시만 켜기
    return OCRResult(text=text, confidence=confidence, raw=None)


==========================================================================================
# FILE: ai/omr/__init__.py
==========================================================================================



==========================================================================================
# FILE: ai/omr/engine.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/identifier.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore

from apps.worker.ai_worker.ai.omr.meta_px import build_page_scale_from_meta, PageScale
from apps.worker.ai_worker.ai.utils.image_resizer import resize_if_large


BBox = Tuple[int, int, int, int]


@dataclass(frozen=True)
class IdentifierConfigV1:
    """
    Identifier OMR v1 (8 digits, each digit 0~9 single mark).

    Principles:
    - ROI based fill score (same philosophy as detect_omr_answers_v1)
    - Robust to scan/photo noise by sampling a square ROI around each bubble
    - No DB, no external calls, worker-only judgement/extraction
    """
    # 주변 ROI(버블 중심 기준) 확장 계수: r * k
    roi_expand_k: float = 1.55

    # blank 판단: 해당 digit에서 최고 fill이 이 값보다 작으면 blank
    blank_threshold: float = 0.070

    # ambiguous 판단: top-2 gap이 이 값보다 작으면 ambiguous
    conf_gap_threshold: float = 0.060

    # (운영 편의) digit-level confidence clamp
    min_confidence: float = 0.0
    max_confidence: float = 1.0


def _clamp(v: int, lo: int, hi: int) -> int:
    return max(lo, min(hi, v))


def _crop(gray: np.ndarray, bbox: BBox) -> np.ndarray:
    x, y, w, h = bbox
    x = _clamp(int(x), 0, gray.shape[1] - 1)
    y = _clamp(int(y), 0, gray.shape[0] - 1)
    w = max(1, int(w))
    h = max(1, int(h))
    w = min(w, gray.shape[1] - x)
    h = min(h, gray.shape[0] - y)
    return gray[y:y + h, x:x + w]


def _fill_score(roi_gray: np.ndarray) -> float:
    """
    Same core idea as OMR v1:
    - blur
    - OTSU + INV
    - filled pixel ratio
    """
    if roi_gray.size == 0:
        return 0.0

    blur = cv2.GaussianBlur(roi_gray, (5, 5), 0)
    _, th = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    filled = float(np.sum(th > 0))
    total = float(th.size) if th.size > 0 else 1.0
    score = filled / total
    return float(max(0.0, min(1.0, score)))


def _bubble_roi_bbox_px(
    *,
    center_px: Tuple[int, int],
    r_px: int,
    cfg: IdentifierConfigV1,
    img_w: int,
    img_h: int,
) -> BBox:
    cx, cy = center_px
    side = int(round(max(2, r_px) * cfg.roi_expand_k)) * 2
    x = int(cx - side // 2)
    y = int(cy - side // 2)
    x = _clamp(x, 0, img_w - 1)
    y = _clamp(y, 0, img_h - 1)
    w = _clamp(side, 1, img_w - x)
    h = _clamp(side, 1, img_h - y)
    return (x, y, w, h)


def detect_identifier_v1(
    *,
    image_bgr: np.ndarray,
    meta: Dict[str, Any],
    cfg: Optional[IdentifierConfigV1] = None,
) -> Dict[str, Any]:
    """
    Extract identifier(8 digits) from aligned full-page image.

    meta requirements:
      meta["identifier"]["bubbles"] list with:
        - digit_index (1..8)
        - number (0..9)
        - center: {x(mm), y(mm)}
        - r(mm)

    return contract:
      {
        "identifier": "12345678" | None,
        "digits": [{"digit_index":1,"value":1,"status":"ok|blank|ambiguous","confidence":0.91,"marks":[...]}...],
        "confidence": 0.0~1.0,
        "status": "ok|ambiguous|blank|error"
      }
    """
    cfg = cfg or IdentifierConfigV1()

    if image_bgr is None or image_bgr.size == 0:
        return {"identifier": None, "digits": [], "confidence": 0.0, "status": "error"}

    h, w = image_bgr.shape[:2]
    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)

    ident = meta.get("identifier") or {}
    bubbles = list(ident.get("bubbles") or [])
    if not bubbles:
        return {"identifier": None, "digits": [], "confidence": 0.0, "status": "error"}

    scale: PageScale = build_page_scale_from_meta(meta=meta, image_size_px=(w, h))

    # group bubbles by digit_index
    by_digit: Dict[int, List[Dict[str, Any]]] = {}
    for b in bubbles:
        try:
            di = int(b.get("digit_index") or 0)
        except Exception:
            continue
        if di <= 0:
            continue
        by_digit.setdefault(di, []).append(b)

    digits_out: List[Dict[str, Any]] = []
    identifier_chars: List[str] = []
    status_rollup = "ok"
    confidences: List[float] = []

    for digit_index in sorted(by_digit.keys()):
        bs = by_digit[digit_index]

        marks: List[Dict[str, Any]] = []
        for b in bs:
            num = int(b.get("number") or 0)
            c = b.get("center") or {}
            r_mm = float(b.get("r") or 0.0)

            cx_mm = float(c.get("x") or 0.0)
            cy_mm = float(c.get("y") or 0.0)

            cx_px, cy_px = scale.mm_to_px_point(cx_mm, cy_mm)

            # radius: use average scale for robustness (mm->px)
            r_px_x = max(1, scale.mm_to_px_len_x(r_mm))
            r_px_y = max(1, scale.mm_to_px_len_y(r_mm))
            r_px = max(1, int(round((r_px_x + r_px_y) / 2.0)))

            bbox = _bubble_roi_bbox_px(
                center_px=(cx_px, cy_px),
                r_px=r_px,
                cfg=cfg,
                img_w=w,
                img_h=h,
            )
            roi = _crop(gray, bbox)
            fill = _fill_score(roi)

            marks.append(
                {
                    "number": int(num),
                    "fill": float(fill),
                    "center_px": {"x": int(cx_px), "y": int(cy_px)},
                    "roi_px": {"x": int(bbox[0]), "y": int(bbox[1]), "w": int(bbox[2]), "h": int(bbox[3])},
                }
            )

        marks_sorted = sorted(marks, key=lambda m: float(m.get("fill") or 0.0), reverse=True)
        top = marks_sorted[0] if marks_sorted else {"number": 0, "fill": 0.0}
        second = marks_sorted[1] if len(marks_sorted) > 1 else {"number": 0, "fill": 0.0}

        top_fill = float(top.get("fill") or 0.0)
        second_fill = float(second.get("fill") or 0.0)
        gap = float(top_fill - second_fill)

        if top_fill < cfg.blank_threshold:
            digits_out.append(
                {
                    "digit_index": int(digit_index),
                    "value": None,
                    "status": "blank",
                    "confidence": 0.0,
                    "marks": marks_sorted,
                }
            )
            identifier_chars.append("?")
            status_rollup = "blank" if status_rollup == "ok" else status_rollup
            continue

        if gap < cfg.conf_gap_threshold:
            digits_out.append(
                {
                    "digit_index": int(digit_index),
                    "value": int(top.get("number") or 0),
                    "status": "ambiguous",
                    "confidence": float(max(cfg.min_confidence, min(cfg.max_confidence, top_fill))),
                    "gap": float(gap),
                    "marks": marks_sorted,
                }
            )
            identifier_chars.append(str(int(top.get("number") or 0)))
            status_rollup = "ambiguous" if status_rollup in ("ok",) else status_rollup
            confidences.append(float(top_fill))
            continue

        # ok
        conf = float(max(cfg.min_confidence, min(cfg.max_confidence, top_fill)))
        digits_out.append(
            {
                "digit_index": int(digit_index),
                "value": int(top.get("number") or 0),
                "status": "ok",
                "confidence": conf,
                "gap": float(gap),
                "marks": marks_sorted,
            }
        )
        identifier_chars.append(str(int(top.get("number") or 0)))
        confidences.append(conf)

    # identifier validity: must be 8 digits all ok/ambiguous (blank이면 ? 포함)
    identifier = "".join(identifier_chars)
    if "?" in identifier:
        identifier_final: Optional[str] = None
    else:
        identifier_final = identifier

    # overall confidence: conservative (mean of digit conf where available)
    overall_conf = float(sum(confidences) / len(confidences)) if confidences else 0.0

    return {
        "identifier": identifier_final,
        "raw_identifier": identifier,  # '?' 포함 가능 (운영 디버그/리트라이용)
        "digits": digits_out,
        "confidence": float(max(0.0, min(1.0, overall_conf))),
        "status": status_rollup,
    }


==========================================================================================
# FILE: ai/omr/identifier.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/identifier.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import cv2  # type: ignore
import numpy as np  # type: ignore

from apps.worker.ai_worker.ai.omr.meta_px import build_page_scale_from_meta, PageScale


BBox = Tuple[int, int, int, int]


@dataclass(frozen=True)
class IdentifierConfigV1:
    """
    Identifier OMR v1 (8 digits, each digit 0~9 single mark).

    Principles:
    - ROI based fill score (same philosophy as detect_omr_answers_v1)
    - Robust to scan/photo noise by sampling a square ROI around each bubble
    - No DB, no external calls, worker-only judgement/extraction
    """
    # (OPS DEFAULT) 촬영/워프에서 중심 오차를 흡수하기 위해 소폭 확장
    # 주변 ROI(버블 중심 기준) 확장 계수: r * k
    roi_expand_k: float = 1.60

    # (OPS DEFAULT) blank 과다 방지: 실데이터에서 연필 농도 낮은 케이스 대응
    # blank 판단: 해당 digit에서 최고 fill이 이 값보다 작으면 blank
    blank_threshold: float = 0.055

    # (OPS DEFAULT) ambiguous 과다 방지: top-2 gap 기준 소폭 완화
    # ambiguous 판단: top-2 gap이 이 값보다 작으면 ambiguous
    conf_gap_threshold: float = 0.050

    # (운영 편의) digit-level confidence clamp
    min_confidence: float = 0.0
    max_confidence: float = 1.0


def _clamp(v: int, lo: int, hi: int) -> int:
    return max(lo, min(hi, v))


def _crop(gray: np.ndarray, bbox: BBox) -> np.ndarray:
    x, y, w, h = bbox
    x = _clamp(int(x), 0, gray.shape[1] - 1)
    y = _clamp(int(y), 0, gray.shape[0] - 1)
    w = max(1, int(w))
    h = max(1, int(h))
    w = min(w, gray.shape[1] - x)
    h = min(h, gray.shape[0] - y)
    return gray[y:y + h, x:x + w]


def _fill_score(roi_gray: np.ndarray) -> float:
    """
    Same core idea as OMR v1:
    - blur
    - OTSU + INV
    - filled pixel ratio
    """
    if roi_gray.size == 0:
        return 0.0

    blur = cv2.GaussianBlur(roi_gray, (5, 5), 0)
    _, th = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    filled = float(np.sum(th > 0))
    total = float(th.size) if th.size > 0 else 1.0
    score = filled / total
    return float(max(0.0, min(1.0, score)))


def _bubble_roi_bbox_px(
    *,
    center_px: Tuple[int, int],
    r_px: int,
    cfg: IdentifierConfigV1,
    img_w: int,
    img_h: int,
) -> BBox:
    cx, cy = center_px
    side = int(round(max(2, r_px) * cfg.roi_expand_k)) * 2
    x = int(cx - side // 2)
    y = int(cy - side // 2)
    x = _clamp(x, 0, img_w - 1)
    y = _clamp(y, 0, img_h - 1)
    w = _clamp(side, 1, img_w - x)
    h = _clamp(side, 1, img_h - y)
    return (x, y, w, h)


def detect_identifier_v1(
    *,
    image_bgr: np.ndarray,
    meta: Dict[str, Any],
    cfg: Optional[IdentifierConfigV1] = None,
) -> Dict[str, Any]:
    """
    Extract identifier(8 digits) from aligned full-page image.

    meta requirements:
      meta["identifier"]["bubbles"] list with:
        - digit_index (1..8)
        - number (0..9)
        - center: {x(mm), y(mm)}
        - r(mm)

    return contract:
      {
        "identifier": "12345678" | None,
        "digits": [{"digit_index":1,"value":1,"status":"ok|blank|ambiguous","confidence":0.91,"marks":[...]}...],
        "confidence": 0.0~1.0,
        "status": "ok|ambiguous|blank|error"
      }
    """
    cfg = cfg or IdentifierConfigV1()

    if image_bgr is None or image_bgr.size == 0:
        return {"identifier": None, "digits": [], "confidence": 0.0, "status": "error"}

    # 대용량 이미지 리사이징 (처리 전)
    image_bgr, _ = resize_if_large(image_bgr, max_megapixels=4.0)
    
    h, w = image_bgr.shape[:2]
    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)

    ident = meta.get("identifier") or {}
    bubbles = list(ident.get("bubbles") or [])
    if not bubbles:
        return {"identifier": None, "digits": [], "confidence": 0.0, "status": "error"}

    scale: PageScale = build_page_scale_from_meta(meta=meta, image_size_px=(w, h))

    # group bubbles by digit_index
    by_digit: Dict[int, List[Dict[str, Any]]] = {}
    for b in bubbles:
        try:
            di = int(b.get("digit_index") or 0)
        except Exception:
            continue
        if di <= 0:
            continue
        by_digit.setdefault(di, []).append(b)

    digits_out: List[Dict[str, Any]] = []
    identifier_chars: List[str] = []
    status_rollup = "ok"
    confidences: List[float] = []

    for digit_index in sorted(by_digit.keys()):
        bs = by_digit[digit_index]

        marks: List[Dict[str, Any]] = []
        for b in bs:
            num = int(b.get("number") or 0)
            c = b.get("center") or {}
            r_mm = float(b.get("r") or 0.0)

            cx_mm = float(c.get("x") or 0.0)
            cy_mm = float(c.get("y") or 0.0)

            cx_px, cy_px = scale.mm_to_px_point(cx_mm, cy_mm)

            # radius: use average scale for robustness (mm->px)
            r_px_x = max(1, scale.mm_to_px_len_x(r_mm))
            r_px_y = max(1, scale.mm_to_px_len_y(r_mm))
            r_px = max(1, int(round((r_px_x + r_px_y) / 2.0)))

            bbox = _bubble_roi_bbox_px(
                center_px=(cx_px, cy_px),
                r_px=r_px,
                cfg=cfg,
                img_w=w,
                img_h=h,
            )
            roi = _crop(gray, bbox)
            fill = _fill_score(roi)

            marks.append(
                {
                    "number": int(num),
                    "fill": float(fill),
                    "center_px": {"x": int(cx_px), "y": int(cy_px)},
                    "roi_px": {"x": int(bbox[0]), "y": int(bbox[1]), "w": int(bbox[2]), "h": int(bbox[3])},
                }
            )

        marks_sorted = sorted(marks, key=lambda m: float(m.get("fill") or 0.0), reverse=True)
        top = marks_sorted[0] if marks_sorted else {"number": 0, "fill": 0.0}
        second = marks_sorted[1] if len(marks_sorted) > 1 else {"number": 0, "fill": 0.0}

        top_fill = float(top.get("fill") or 0.0)
        second_fill = float(second.get("fill") or 0.0)
        gap = float(top_fill - second_fill)

        if top_fill < cfg.blank_threshold:
            digits_out.append(
                {
                    "digit_index": int(digit_index),
                    "value": None,
                    "status": "blank",
                    "confidence": 0.0,
                    "marks": marks_sorted,
                }
            )
            identifier_chars.append("?")
            status_rollup = "blank" if status_rollup == "ok" else status_rollup
            continue

        if gap < cfg.conf_gap_threshold:
            digits_out.append(
                {
                    "digit_index": int(digit_index),
                    "value": int(top.get("number") or 0),
                    "status": "ambiguous",
                    "confidence": float(max(cfg.min_confidence, min(cfg.max_confidence, top_fill))),
                    "gap": float(gap),
                    "marks": marks_sorted,
                }
            )
            identifier_chars.append(str(int(top.get("number") or 0)))
            status_rollup = "ambiguous" if status_rollup in ("ok",) else status_rollup
            confidences.append(float(top_fill))
            continue

        # ok
        conf = float(max(cfg.min_confidence, min(cfg.max_confidence, top_fill)))
        digits_out.append(
            {
                "digit_index": int(digit_index),
                "value": int(top.get("number") or 0),
                "status": "ok",
                "confidence": conf,
                "gap": float(gap),
                "marks": marks_sorted,
            }
        )
        identifier_chars.append(str(int(top.get("number") or 0)))
        confidences.append(conf)

    # identifier validity: must be 8 digits all ok/ambiguous (blank이면 ? 포함)
    identifier = "".join(identifier_chars)
    if "?" in identifier:
        identifier_final: Optional[str] = None
    else:
        identifier_final = identifier

    # overall confidence: conservative (mean of digit conf where available)
    overall_conf = float(sum(confidences) / len(confidences)) if confidences else 0.0

    return {
        "identifier": identifier_final,
        "raw_identifier": identifier,  # '?' 포함 가능 (운영 디버그/리트라이용)
        "digits": digits_out,
        "confidence": float(max(0.0, min(1.0, overall_conf))),
        "status": status_rollup,
    }


==========================================================================================
# FILE: ai/omr/meta_px.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/meta_px.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, Tuple


def _clamp(v: int, lo: int, hi: int) -> int:
    return max(lo, min(hi, v))


@dataclass(frozen=True)
class PageScale:
    """
    Convert meta(mm) -> px.
    Boundary rule (fixed):
      - meta(mm) is assets single truth
      - px conversion is worker responsibility
      - aligned/warped image must represent the full page
    """
    sx: float
    sy: float
    img_w: int
    img_h: int

    def mm_to_px_point(self, x_mm: float, y_mm: float) -> Tuple[int, int]:
        x = int(round(float(x_mm) * self.sx))
        y = int(round(float(y_mm) * self.sy))
        x = _clamp(x, 0, self.img_w - 1)
        y = _clamp(y, 0, self.img_h - 1)
        return x, y

    def mm_to_px_len_x(self, v_mm: float) -> int:
        return max(1, int(round(float(v_mm) * self.sx)))

    def mm_to_px_len_y(self, v_mm: float) -> int:
        return max(1, int(round(float(v_mm) * self.sy)))


def build_page_scale_from_meta(
    *,
    meta: Dict[str, Any],
    image_size_px: Tuple[int, int],
) -> PageScale:
    """
    Build scaler from template meta.
    meta page size is mm. image_size_px is (width, height).
    """
    img_w, img_h = int(image_size_px[0]), int(image_size_px[1])

    page = meta.get("page") or {}
    size = page.get("size") or {}
    page_w_mm = float(size.get("width") or 0.0)
    page_h_mm = float(size.get("height") or 0.0)

    if img_w <= 0 or img_h <= 0:
        raise ValueError("invalid image_size_px")
    if page_w_mm <= 0.0 or page_h_mm <= 0.0:
        raise ValueError("invalid meta page size")

    sx = img_w / page_w_mm
    sy = img_h / page_h_mm
    return PageScale(sx=float(sx), sy=float(sy), img_w=img_w, img_h=img_h)


==========================================================================================
# FILE: ai/omr/template_meta.py
==========================================================================================
# apps/worker/ai_worker/ai/omr/template_meta.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Tuple, Optional

import requests


@dataclass(frozen=True)
class TemplateMeta:
    raw: Dict[str, Any]

    @property
    def units(self) -> str:
        return str(self.raw.get("units") or "mm")

    @property
    def page_size_mm(self) -> Tuple[float, float]:
        page = self.raw.get("page") or {}
        size = page.get("size") or {}
        return float(size.get("width") or 0.0), float(size.get("height") or 0.0)

    @property
    def questions(self) -> List[Dict[str, Any]]:
        return list(self.raw.get("questions") or [])


class TemplateMetaFetchError(RuntimeError):
    pass


def fetch_objective_meta(
    *,
    base_url: str,
    question_count: int,
    # auth options (choose what your deployment uses)
    auth_cookie_header: Optional[str] = None,
    bearer_token: Optional[str] = None,
    worker_token_header: Optional[str] = None,  # e.g. X-Worker-Token (if API allows)
    extra_headers: Optional[Dict[str, str]] = None,
    timeout: int = 10,
) -> TemplateMeta:
    """
    worker -> API (assets meta)

    Stability goals:
    - clear timeouts
    - explicit error messages
    - flexible auth (cookie/bearer/custom header)
    - returns structured exception for caller to gracefully fallback

    NOTE:
    - assets endpoint currently uses IsAuthenticated.
      Production typically uses cookie session OR bearer token.
      worker_token_header is optional if you later add internal auth for workers.
    """
    url = f"{base_url.rstrip('/')}/api/v1/assets/omr/objective/meta/"
    params = {"question_count": str(int(question_count))}

    headers: Dict[str, str] = {"Accept": "application/json"}
    if auth_cookie_header:
        headers["Cookie"] = auth_cookie_header
    if bearer_token:
        headers["Authorization"] = f"Bearer {bearer_token}"
    if worker_token_header:
        headers["X-Worker-Token"] = str(worker_token_header)
    if extra_headers:
        for k, v in extra_headers.items():
            if k and v:
                headers[str(k)] = str(v)

    try:
        r = requests.get(url, params=params, headers=headers, timeout=timeout)
    except Exception as e:
        raise TemplateMetaFetchError(f"meta_fetch_network_error: {e!r}") from e

    if r.status_code >= 400:
        body = (r.text or "")[:2000]
        raise TemplateMetaFetchError(
            f"meta_fetch_http_error: status={r.status_code} body={body}"
        )

    try:
        data = r.json()
    except Exception as e:
        raise TemplateMetaFetchError(f"meta_fetch_invalid_json: {e!r}") from e

    return TemplateMeta(raw=data)


==========================================================================================
# FILE: ai/omr/types.py
==========================================================================================
# apps/worker/ai/omr/types.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Literal


OMRStatus = Literal["ok", "blank", "ambiguous", "low_confidence", "error"]
OMRMarking = Literal["blank", "single", "multi"]


@dataclass(frozen=True)
class OMRAnswerV1:
    """
    Worker-side OMR payload v1 (question-level)
    This should be embedded into API-side SubmissionAnswer.meta["omr"] later.

    - version: "v1" fixed
    - detected: list[str] ex) ["B"] or ["B","D"]
    - marking: blank/single/multi
    - confidence: 0~1 (top mark confidence)
    - status: ok/blank/ambiguous/low_confidence/error
    - raw: debug info (optional)
    """
    version: str
    question_id: int

    detected: List[str]
    marking: OMRMarking
    confidence: float
    status: OMRStatus

    raw: Optional[Dict[str, Any]] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "version": self.version,
            "question_id": int(self.question_id),
            "detected": list(self.detected or []),
            "marking": self.marking,
            "confidence": float(self.confidence or 0.0),
            "status": self.status,
            "raw": self.raw,
        }


==========================================================================================
# FILE: ai/pipelines/__init__.py
==========================================================================================
# apps/worker/ai/pipelines/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai/pipelines/dispatcher.py
==========================================================================================
# apps/worker/ai_worker/ai/pipelines/dispatcher.py
from __future__ import annotations

from typing import Any, Dict

from apps.shared.contracts.ai_job import AIJob
from apps.shared.contracts.ai_result import AIResult

from apps.worker.ai_worker.ai.config import AIConfig
from apps.worker.ai_worker.ai.ocr.google import google_ocr
from apps.worker.ai_worker.ai.ocr.tesseract import tesseract_ocr
from apps.worker.ai_worker.ai.detection.segment_dispatcher import segment_questions
from apps.worker.ai_worker.ai.handwriting.detector import analyze_handwriting
from apps.worker.ai_worker.ai.embedding.service import get_embeddings
from apps.worker.ai_worker.ai.problem.generator import generate_problem_from_ocr
from apps.worker.ai_worker.ai.pipelines.homework_video_analyzer import analyze_homework_video
from apps.worker.ai_worker.ai.utils.image_resizer import resize_if_large
from apps.worker.ai_worker.storage.downloader import download_to_tmp
def handle_ai_job(job: AIJob) -> AIResult:
    try:
        cfg = AIConfig.load()
        payload: Dict[str, Any] = job.payload or {}

        download_url = payload.get("download_url")
        if not download_url:
            return AIResult.failed(job.id, "download_url missing")

        local_path = download_to_tmp(
            download_url=download_url,
            job_id=str(job.id),
        )

        # --------------------------------------------------
        # OCR
        # --------------------------------------------------
        if job.type == "ocr":
            engine = (payload.get("engine") or cfg.OCR_ENGINE or "auto").lower()

            if engine == "tesseract":
                r = tesseract_ocr(local_path)
            elif engine == "google":
                r = google_ocr(local_path)
            else:
                try:
                    r = google_ocr(local_path)
                    if not r.text.strip():
                        r = tesseract_ocr(local_path)
                except Exception:
                    r = tesseract_ocr(local_path)

            return AIResult.done(
                job.id,
                {"text": r.text, "confidence": r.confidence},
            )

        # --------------------------------------------------
        # Question segmentation
        # --------------------------------------------------
        if job.type == "question_segmentation":
            boxes = segment_questions(local_path)
            return AIResult.done(job.id, {"boxes": boxes})

        # --------------------------------------------------
        # Handwriting analysis
        # --------------------------------------------------
        if job.type == "handwriting_analysis":
            scores = analyze_handwriting(local_path)
            return AIResult.done(job.id, scores)

        # --------------------------------------------------
        # Embedding
        # --------------------------------------------------
        if job.type == "embedding":
            texts = payload.get("texts") or []
            batch = get_embeddings(list(texts))
            return AIResult.done(
                job.id,
                {"backend": batch.backend, "vectors": batch.vectors},
            )

        # --------------------------------------------------
        # Problem generation
        # --------------------------------------------------
        if job.type == "problem_generation":
            ocr_text = payload.get("ocr_text") or ""
            parsed = generate_problem_from_ocr(ocr_text)
            return AIResult.done(
                job.id,
                {
                    "body": parsed.body,
                    "choices": parsed.choices,
                    "answer": parsed.answer,
                    "difficulty": parsed.difficulty,
                    "tag": parsed.tag,
                    "summary": parsed.summary,
                    "explanation": parsed.explanation,
                },
            )

        # --------------------------------------------------
        # Homework video analysis
        # --------------------------------------------------
        if job.type == "homework_video_analysis":
            frame_stride = int(payload.get("frame_stride") or 10)
            min_frame_count = int(payload.get("min_frame_count") or 30)
            use_key_frames = payload.get("use_key_frames", True)  # 기본값: 키 프레임 사용
            max_pages = int(payload.get("max_pages") or 10)
            processing_timeout = int(payload.get("processing_timeout") or 60)
            
            analysis = analyze_homework_video(
                video_path=local_path,
                frame_stride=frame_stride,
                min_frame_count=min_frame_count,
                use_key_frames=use_key_frames,
                max_pages=max_pages,
                processing_timeout=processing_timeout,
            )
            return AIResult.done(job.id, analysis)

        # --------------------------------------------------
        # OMR grading (meta-aware) - production final
        # --------------------------------------------------
        if job.type == "omr_grading":
            """
            payload options (recommended):
              - mode: "scan" | "photo" | "auto" (default auto)
              - question_count: 10|20|30 (required if template_fetch is used)
              - template_meta: dict (inject meta directly; no API call)
              - template_fetch:
                    {
                      "base_url": "...",
                      "cookie": "...",
                      "bearer_token": "...",
                      "worker_token": "...",
                      "timeout": 10
                    }

            Output contract:
              {
                "version": "v1",
                "mode": "...",
                "aligned": true|false,
                "identifier": {...},
                "answers": [...],
                "meta_used": true|false
              }
            """
            import cv2  # type: ignore

            from apps.worker.ai_worker.ai.omr.engine import detect_omr_answers_v1, OMRConfigV1
            from apps.worker.ai_worker.ai.omr.roi_builder import build_questions_payload_from_meta
            from apps.worker.ai_worker.ai.omr.warp import warp_to_a4_landscape
            from apps.worker.ai_worker.ai.omr.template_meta import fetch_objective_meta, TemplateMetaFetchError
            from apps.worker.ai_worker.ai.omr.identifier import detect_identifier_v1, IdentifierConfigV1

            mode = str(payload.get("mode") or "auto").lower()
            if mode not in ("scan", "photo", "auto"):
                mode = "auto"

            # 1) meta 확보
            meta = payload.get("template_meta")
            meta_used = False
            meta_fetch_error = None

            if not meta:
                tf = payload.get("template_fetch") or {}
                base_url = tf.get("base_url")
                if base_url:
                    qc = int(payload.get("question_count") or 0)
                    if qc not in (10, 20, 30):
                        return AIResult.failed(job.id, "question_count required (10|20|30) for template_fetch")

                    try:
                        meta_obj = fetch_objective_meta(
                            base_url=str(base_url),
                            question_count=qc,
                            auth_cookie_header=tf.get("cookie"),
                            bearer_token=tf.get("bearer_token"),
                            worker_token_header=tf.get("worker_token"),
                            timeout=int(tf.get("timeout") or 10),
                        )
                        meta = meta_obj.raw
                        meta_used = True
                    except TemplateMetaFetchError as e:
                        meta = None
                        meta_fetch_error = str(e)[:500]

            # 2) 이미지 로드 및 리사이징
            img_bgr = cv2.imread(local_path)
            if img_bgr is None:
                return AIResult.failed(job.id, "cannot read image")
            
            # 대용량 이미지 리사이징 (처리 전)
            img_bgr, was_resized = resize_if_large(img_bgr, max_megapixels=4.0)

            aligned = img_bgr

            # 3) mode 정책
            if mode == "photo":
                warped = warp_to_a4_landscape(img_bgr)
                if warped is None:
                    return AIResult.failed(job.id, "warp_failed_for_photo_mode")
                aligned = warped

            elif mode == "auto":
                warped = warp_to_a4_landscape(img_bgr)
                if warped is not None:
                    aligned = warped

            # 4) meta 없으면 legacy
            if not meta:
                questions = payload.get("questions") or []
                if not questions:
                    return AIResult.failed(job.id, "template_meta/template_fetch failed and legacy questions missing")

                answers = detect_omr_answers_v1(
                    image_path=local_path,
                    questions=list(questions),
                    cfg=None,
                )
                return AIResult.done(
                    job.id,
                    {
                        "version": "v1",
                        "mode": "legacy_questions",
                        "aligned": False,
                        "identifier": None,
                        "answers": answers,
                        "meta_used": False,
                        "debug": {
                            "meta_fetch_error": meta_fetch_error,
                        },
                    },
                )

            # 5) ROI + identifier
            h, w = aligned.shape[:2]
            questions_payload = build_questions_payload_from_meta(meta, (w, h))

            ident = detect_identifier_v1(
                image_bgr=aligned,
                meta=meta,
                cfg=IdentifierConfigV1(),
            )

            # 6) aligned 저장 후 OMR
            import tempfile, os

            tmp_path = os.path.join(tempfile.gettempdir(), f"omr_aligned_{job.id}.jpg")
            cv2.imwrite(tmp_path, aligned)

            cfg = OMRConfigV1()
            answers = detect_omr_answers_v1(
                image_path=tmp_path,
                questions=list(questions_payload),
                cfg=cfg,
            )

            return AIResult.done(
                job.id,
                {
                    "version": "v1",
                    "mode": mode,
                    "aligned": bool(aligned is not img_bgr),
                    "identifier": ident,
                    "answers": answers,
                    "meta_used": meta_used,
                    "debug": {
                        "meta_fetch_error": meta_fetch_error,
                    },
                },
            )

        return AIResult.failed(job.id, f"Unsupported job type: {job.type}")

    except Exception as e:
        return AIResult.failed(job.id, str(e))


==========================================================================================
# FILE: ai/pipelines/homework_video_analyzer.py
==========================================================================================
# apps/worker/ai/pipelines/homework_video_analyzer.py
from __future__ import annotations

from typing import Dict, Any, List
import cv2  # type: ignore
import numpy as np  # type: ignore

from apps.worker.ai_worker.ai.pipelines.video_frame_extractor import (
    extract_key_frames,
    extract_frame_at_index,
)
from apps.worker.ai_worker.ai.utils.image_resizer import resize_if_large


def _estimate_writing_score(gray_roi: np.ndarray) -> float:
    if gray_roi.size == 0:
        return 0.0
    dark = (gray_roi < 220).sum()
    total = gray_roi.size
    return float(dark) / float(total)


def analyze_homework_video(
    video_path: str,
    frame_stride: int = 10,
    min_frame_count: int = 30,
    use_key_frames: bool = True,  # 키 프레임 추출 사용 여부
    max_pages: int = 10,  # 최대 페이지 수
    processing_timeout: int = 60,  # 처리 타임아웃 (초)
) -> Dict[str, Any]:
    # 키 프레임 추출 사용 (권장)
    if use_key_frames:
        try:
            key_frames_info = extract_key_frames(
                video_path=video_path,
                target_fps=2.0,  # 1-3 fps 권장
                max_pages=max_pages,
                processing_timeout=processing_timeout,
            )
            
            # 키 프레임에서만 분석
            frame_results: List[Dict[str, Any]] = []
            
            for key_frame in key_frames_info["key_frames"]:
                frame_idx = key_frame["frame_index"]
                frame = extract_frame_at_index(video_path, frame_idx)
                
                if frame is None:
                    continue
                
                # 이미지 리사이징 (대용량 처리 전)
                frame_resized, was_resized = resize_if_large(frame, max_megapixels=4.0)
                gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)
                score = _estimate_writing_score(gray)
                
                frame_results.append({
                    "index": frame_idx,
                    "timestamp": key_frame["timestamp"],
                    "page_number": key_frame["page_number"],
                    "writing_score": round(float(score), 4),
                    "has_writing": bool(score >= 0.05),
                    "was_resized": was_resized,
                })
            
            sampled = len(frame_results)
            if sampled == 0:
                return {
                    "total_frames": key_frames_info["total_frames"],
                    "sampled_frames": 0,
                    "avg_writing_score": 0.0,
                    "filled_ratio": 0.0,
                    "frames": [],
                    "pages_detected": 0,
                    "too_short": key_frames_info["total_frames"] < min_frame_count,
                }
            
            avg = sum(fr["writing_score"] for fr in frame_results) / sampled
            filled = sum(1 for fr in frame_results if fr["has_writing"])
            ratio = filled / sampled
            
            return {
                "total_frames": key_frames_info["total_frames"],
                "sampled_frames": sampled,
                "pages_detected": key_frames_info["pages_detected"],
                "avg_writing_score": round(float(avg), 4),
                "filled_ratio": round(float(ratio), 4),
                "frames": frame_results,
                "too_short": key_frames_info["total_frames"] < min_frame_count,
                "key_frames_extraction": True,
            }
        except Exception as e:
            # 키 프레임 추출 실패 시 레거시 방식으로 fallback
            import logging
            logger = logging.getLogger(__name__)
            logger.warning("Key frame extraction failed, using legacy method: %s", e)
    
    # 레거시 방식 (전체 프레임 샘플링)
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"cannot open video: {video_path}")

    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_idx = 0
    frame_results: List[Dict[str, Any]] = []

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        if frame_idx % frame_stride != 0:
            frame_idx += 1
            continue

        # 이미지 리사이징 (대용량 처리 전)
        frame_resized, was_resized = resize_if_large(frame, max_megapixels=4.0)
        gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)
        score = _estimate_writing_score(gray)

        frame_results.append(
            {
                "index": frame_idx,
                "writing_score": round(float(score), 4),
                "has_writing": bool(score >= 0.05),
                "was_resized": was_resized,
            }
        )
        frame_idx += 1

    cap.release()

    sampled = len(frame_results)
    if sampled == 0:
        return {
            "total_frames": total_frames,
            "sampled_frames": 0,
            "avg_writing_score": 0.0,
            "filled_ratio": 0.0,
            "frames": [],
            "too_short": total_frames < min_frame_count,
            "key_frames_extraction": False,
        }

    avg = sum(fr["writing_score"] for fr in frame_results) / sampled
    filled = sum(1 for fr in frame_results if fr["has_writing"])
    ratio = filled / sampled

    return {
        "total_frames": total_frames,
        "sampled_frames": sampled,
        "avg_writing_score": round(float(avg), 4),
        "filled_ratio": round(float(ratio), 4),
        "frames": frame_results,
        "too_short": total_frames < min_frame_count,
        "key_frames_extraction": False,
    }


==========================================================================================
# FILE: ai/pipelines/tier_enforcer.py
==========================================================================================
"""
Tier별 처리 제한 강제

비즈니스 규칙:
- Lite: CPU OCR만 허용
- Basic: CPU 기반 OMR/status detection + 개선된 CPU OCR
- Premium: GPU 기반 전체 OCR + 고급 분석 (향후)
"""

from __future__ import annotations

import logging
from typing import Optional

logger = logging.getLogger(__name__)


def enforce_tier_limits(
    *,
    tier: str,
    job_type: str,
) -> tuple[bool, Optional[str]]:
    """
    Tier별 처리 제한 강제
    
    Args:
        tier: Tier ("lite" | "basic" | "premium")
        job_type: 작업 타입
        
    Returns:
        tuple: (허용 여부, 에러 메시지)
    """
    tier = tier.lower()
    job_type_lower = job_type.lower()
    
    # Lite: OCR만 허용
    if tier == "lite":
        if job_type_lower not in ("ocr",):
            return False, f"Tier 'lite' only allows 'ocr' job type, got '{job_type}'"
        return True, None
    
    # Basic: OCR + OMR/status detection
    if tier == "basic":
        allowed_types = ("ocr", "omr_grading", "homework_video_analysis")
        if job_type_lower not in allowed_types:
            return False, f"Tier 'basic' only allows {allowed_types}, got '{job_type}'"
        return True, None
    
    # Premium: 모든 작업 허용
    if tier == "premium":
        return True, None
    
    return False, f"Unknown tier: {tier}"


==========================================================================================
# FILE: ai/pipelines/video_frame_extractor.py
==========================================================================================
"""
비디오 프레임 추출 모듈

요구사항:
- 전체 비디오 프레임 분석 금지
- 1-3 fps로 샘플링
- 페이지 전환 감지 (SSIM/Frame Difference)
- 페이지당 대표 프레임 1개만 추출
- 최대 페이지 수 제한
- 처리 타임아웃 강제
"""

from __future__ import annotations

import logging
import time
from typing import List, Dict, Any, Optional, Tuple
import cv2  # type: ignore
import numpy as np  # type: ignore

try:
    from skimage.metrics import structural_similarity as ssim  # type: ignore
    HAS_SSIM = True
except ImportError:
    HAS_SSIM = False

logger = logging.getLogger(__name__)


def _calculate_ssim(img1: np.ndarray, img2: np.ndarray) -> float:
    """
    두 이미지 간 SSIM 계산
    
    Args:
        img1: 첫 번째 이미지 (grayscale)
        img2: 두 번째 이미지 (grayscale)
        
    Returns:
        float: SSIM 값 (0.0 ~ 1.0)
    """
    if not HAS_SSIM:
        # SSIM이 없으면 Frame Difference 사용
        return 1.0 - _calculate_frame_diff(img1, img2)
    
    try:
        # 이미지 크기 맞추기
        if img1.shape != img2.shape:
            img2 = cv2.resize(img2, (img1.shape[1], img1.shape[0]))
        
        # SSIM 계산
        score = ssim(img1, img2, data_range=255)
        return float(score)
    except Exception as e:
        logger.warning("SSIM calculation failed: %s", e)
        # Fallback to frame diff
        return 1.0 - _calculate_frame_diff(img1, img2)


def _calculate_frame_diff(img1: np.ndarray, img2: np.ndarray) -> float:
    """
    두 프레임 간 차이 계산 (간단한 방법)
    
    Args:
        img1: 첫 번째 프레임 (grayscale)
        img2: 두 번째 프레임 (grayscale)
        
    Returns:
        float: 차이 비율 (0.0 ~ 1.0)
    """
    try:
        # 이미지 크기 맞추기
        if img1.shape != img2.shape:
            img2 = cv2.resize(img2, (img1.shape[1], img1.shape[0]))
        
        # 절대 차이 계산
        diff = cv2.absdiff(img1, img2)
        diff_ratio = np.sum(diff > 30) / diff.size  # 임계값 30
        return float(diff_ratio)
    except Exception as e:
        logger.warning("Frame diff calculation failed: %s", e)
        return 0.0


def extract_key_frames(
    video_path: str,
    target_fps: float = 2.0,  # 1-3 fps 권장
    max_pages: int = 10,  # 최대 페이지 수 제한
    page_change_threshold: float = 0.3,  # 페이지 전환 임계값 (SSIM 또는 diff)
    use_ssim: bool = True,  # SSIM 사용 여부 (False면 frame diff 사용)
    processing_timeout: int = 60,  # 처리 타임아웃 (초)
) -> Dict[str, Any]:
    """
    비디오에서 키 프레임 추출 (페이지당 1개)
    
    Args:
        video_path: 비디오 파일 경로
        target_fps: 목표 FPS (1-3 권장)
        max_pages: 최대 페이지 수
        page_change_threshold: 페이지 전환 감지 임계값
        use_ssim: SSIM 사용 여부
        processing_timeout: 처리 타임아웃 (초)
        
    Returns:
        dict: 추출된 키 프레임 정보
    """
    start_time = time.time()
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"cannot open video: {video_path}")
    
    try:
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0
        
        # 샘플링 간격 계산 (target_fps에 맞춤)
        frame_interval = int(fps / target_fps) if fps > 0 else 1
        frame_interval = max(1, frame_interval)  # 최소 1프레임
        
        logger.info(
            "Video info: fps=%.2f, total_frames=%d, duration=%.2fs, frame_interval=%d",
            fps,
            total_frames,
            duration,
            frame_interval,
        )
        
        key_frames: List[Dict[str, Any]] = []
        prev_frame: Optional[np.ndarray] = None
        current_page_frames: List[Tuple[int, np.ndarray]] = []  # (frame_idx, frame)
        frame_idx = 0
        
        while True:
            # 타임아웃 체크
            if time.time() - start_time > processing_timeout:
                logger.warning("Processing timeout reached: %ds", processing_timeout)
                break
            
            ret, frame = cap.read()
            if not ret:
                break
            
            # 샘플링: target_fps에 맞춰 프레임 선택
            if frame_idx % frame_interval != 0:
                frame_idx += 1
                continue
            
            # Grayscale 변환
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # 첫 프레임이거나 페이지 전환 감지
            is_page_change = False
            
            if prev_frame is None:
                # 첫 프레임
                is_page_change = True
            else:
                # 페이지 전환 감지
                if use_ssim:
                    similarity = _calculate_ssim(prev_frame, gray)
                    is_page_change = similarity < (1.0 - page_change_threshold)
                else:
                    diff_ratio = _calculate_frame_diff(prev_frame, gray)
                    is_page_change = diff_ratio > page_change_threshold
            
            if is_page_change:
                # 이전 페이지의 대표 프레임 선택 (중간 프레임)
                if current_page_frames:
                    mid_idx = len(current_page_frames) // 2
                    rep_frame_idx, rep_frame = current_page_frames[mid_idx]
                    
                    key_frames.append({
                        "frame_index": rep_frame_idx,
                        "timestamp": rep_frame_idx / fps if fps > 0 else 0,
                        "page_number": len(key_frames) + 1,
                    })
                    
                    logger.debug(
                        "Page %d detected: frame_idx=%d, frames_in_page=%d",
                        len(key_frames),
                        rep_frame_idx,
                        len(current_page_frames),
                    )
                
                # 최대 페이지 수 체크
                if len(key_frames) >= max_pages:
                    logger.warning("Max pages reached: %d", max_pages)
                    break
                
                # 새 페이지 시작
                current_page_frames = [(frame_idx, gray)]
            else:
                # 같은 페이지에 프레임 추가
                current_page_frames.append((frame_idx, gray))
            
            prev_frame = gray
            frame_idx += 1
        
        # 마지막 페이지 처리
        if current_page_frames and len(key_frames) < max_pages:
            mid_idx = len(current_page_frames) // 2
            rep_frame_idx, rep_frame = current_page_frames[mid_idx]
            
            key_frames.append({
                "frame_index": rep_frame_idx,
                "timestamp": rep_frame_idx / fps if fps > 0 else 0,
                "page_number": len(key_frames) + 1,
            })
        
        processing_time = time.time() - start_time
        
        return {
            "total_frames": total_frames,
            "video_fps": fps,
            "duration": duration,
            "key_frames": key_frames,
            "pages_detected": len(key_frames),
            "processing_time": processing_time,
            "frame_interval": frame_interval,
        }
        
    finally:
        cap.release()


def extract_frame_at_index(
    video_path: str,
    frame_index: int,
) -> Optional[np.ndarray]:
    """
    특정 인덱스의 프레임 추출
    
    Args:
        video_path: 비디오 파일 경로
        frame_index: 프레임 인덱스
        
    Returns:
        np.ndarray: 프레임 이미지 (BGR) 또는 None
    """
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        return None
    
    try:
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
        ret, frame = cap.read()
        return frame if ret else None
    finally:
        cap.release()


==========================================================================================
# FILE: ai/problem/__init__.py
==========================================================================================
# apps/worker/ai/problem/__init__.py
from __future__ import annotations


==========================================================================================
# FILE: ai/problem/generator.py
==========================================================================================
# apps/worker/ai/problem/generator.py
from __future__ import annotations

import json
from dataclasses import dataclass
from typing import Optional

from apps.worker.ai_worker.ai.config import AIConfig
from apps.worker.ai_worker.ai.problem.prompt import BASE_PROMPT

try:
    from openai import OpenAI  # type: ignore
except Exception:
    OpenAI = None  # type: ignore


@dataclass
class ParsedProblem:
    body: str
    choices: list
    answer: Optional[str]
    difficulty: int
    tag: str
    summary: str
    explanation: str


_client: Optional["OpenAI"] = None


def _get_client() -> "OpenAI":
    global _client
    if _client is not None:
        return _client

    if OpenAI is None:
        raise RuntimeError("openai not installed")

    cfg = AIConfig.load()
    if not cfg.OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY not set")

    _client = OpenAI(api_key=cfg.OPENAI_API_KEY)
    return _client


def generate_problem_from_ocr(ocr_text: str) -> ParsedProblem:
    cfg = AIConfig.load()
    prompt = BASE_PROMPT.format(ocr_text=ocr_text)

    client = _get_client()
    response = client.chat.completions.create(
        model=cfg.PROBLEM_GEN_MODEL,
        messages=[
            {"role": "system", "content": "당신은 교육용 시험 문제를 자동 생성하는 엔진입니다."},
            {"role": "user", "content": prompt},
        ],
        temperature=0.2,
    )

    # SDK 형태 차이 방어
    msg = response.choices[0].message
    content = getattr(msg, "content", None) or msg.get("content")  # type: ignore

    data = json.loads(content)

    return ParsedProblem(
        body=data.get("body", ""),
        choices=data.get("choices", []),
        answer=data.get("answer"),
        difficulty=int(data.get("difficulty", 3)),
        tag=data.get("tag", ""),
        summary=data.get("summary", ""),
        explanation=data.get("explanation", ""),
    )


==========================================================================================
# FILE: ai/problem/prompt.py
==========================================================================================
# apps/worker/ai/problem/prompt.py
BASE_PROMPT = """
다음은 시험 문제의 OCR 결과입니다.
아래 텍스트를 기반으로 문제 정보를 JSON 형식으로 추출하세요.

요구사항:
1) 문제 본문 (body)
2) 선택지 (choices): 없으면 빈 배열
3) 정답 (answer): 명시된 정답이 없으면 AI가 추론, 추론 불가 시 null
4) 난이도 (difficulty): 1~5 정수로 추정
5) 태그 (tag): 수학/과학/국어 등 간단한 분류
6) 문제 요약 (summary)
7) 해설 (explanation): 간단명료하게

출력은 반드시 JSON 형식만 사용하세요. 다른 텍스트는 포함하지 마세요.

출력 형식 예시:
{
  "body": "...",
  "choices": ["A...", "B...", "C...", "D..."],
  "answer": "C",
  "difficulty": 3,
  "tag": "수학",
  "summary": "...",
  "explanation": "..."
}

OCR 텍스트:
\"\"\"
{ocr_text}
\"\"\"
"""


==========================================================================================
# FILE: ai/utils/image_resizer.py
==========================================================================================
"""
이미지 리사이징 유틸리티

대용량 이미지 처리 전 리사이징으로 성능 향상 및 메모리 사용량 감소
"""

from __future__ import annotations

import logging
from typing import Optional, Tuple
import cv2  # type: ignore
import numpy as np  # type: ignore

logger = logging.getLogger(__name__)


def resize_if_large(
    image: np.ndarray,
    max_width: int = 1920,
    max_height: int = 1920,
    max_megapixels: float = 4.0,  # 4MP (약 2000x2000)
) -> Tuple[np.ndarray, bool]:
    """
    이미지가 크면 리사이징
    
    Args:
        image: 입력 이미지 (BGR 또는 Grayscale)
        max_width: 최대 너비
        max_height: 최대 높이
        max_megapixels: 최대 메가픽셀 수
        
    Returns:
        tuple: (리사이징된 이미지, 리사이징 여부)
    """
    h, w = image.shape[:2]
    current_mp = (w * h) / 1_000_000
    
    # 메가픽셀 체크
    if current_mp <= max_megapixels and w <= max_width and h <= max_height:
        return image, False
    
    # 리사이징 비율 계산
    scale_w = max_width / w if w > max_width else 1.0
    scale_h = max_height / h if h > max_height else 1.0
    scale_mp = np.sqrt(max_megapixels / current_mp) if current_mp > max_megapixels else 1.0
    
    # 가장 작은 스케일 사용 (가장 제한적인 조건)
    scale = min(scale_w, scale_h, scale_mp)
    
    new_w = int(w * scale)
    new_h = int(h * scale)
    
    logger.info(
        "Resizing image: %dx%d -> %dx%d (scale=%.2f, mp=%.2f -> %.2f)",
        w,
        h,
        new_w,
        new_h,
        scale,
        current_mp,
        (new_w * new_h) / 1_000_000,
    )
    
    resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)
    return resized, True


def resize_to_fit(
    image: np.ndarray,
    target_width: Optional[int] = None,
    target_height: Optional[int] = None,
    maintain_aspect: bool = True,
) -> np.ndarray:
    """
    이미지를 지정된 크기에 맞게 리사이징
    
    Args:
        image: 입력 이미지
        target_width: 목표 너비
        target_height: 목표 높이
        maintain_aspect: 종횡비 유지 여부
        
    Returns:
        np.ndarray: 리사이징된 이미지
    """
    h, w = image.shape[:2]
    
    if target_width is None and target_height is None:
        return image
    
    if maintain_aspect:
        if target_width and target_height:
            # 둘 다 지정된 경우, 작은 쪽에 맞춤
            scale_w = target_width / w
            scale_h = target_height / h
            scale = min(scale_w, scale_h)
        elif target_width:
            scale = target_width / w
        elif target_height:
            scale = target_height / h
        else:
            return image
        
        new_w = int(w * scale)
        new_h = int(h * scale)
    else:
        new_w = target_width or w
        new_h = target_height or h
    
    if new_w == w and new_h == h:
        return image
    
    return cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)


==========================================================================================
# FILE: apps/worker/ai_worker/celery.py
==========================================================================================



==========================================================================================
# FILE: storage/__init__.py
==========================================================================================
# apps.worker.ai_worker.storage


==========================================================================================
# FILE: storage/downloader.py
==========================================================================================
# ==============================================================================
# PATH: apps/worker/storage/downloader.py
#
# PURPOSE:
# - AI worker 전용 파일 다운로드 유틸
# - presigned GET URL → /tmp local file
# - R2 / S3 credential 사용 ❌
# - video_worker 코드와 절대 공유하지 않음
# ==============================================================================

from __future__ import annotations

import os
import tempfile
import requests
from pathlib import Path


def download_to_tmp(
    *,
    download_url: str,
    job_id: str,
    suffix: str | None = None,
    timeout: int = 60,
    chunk_size: int = 1024 * 1024,  # 1MB
) -> str:
    """
    presigned GET URL로 파일을 다운로드하여 local temp file 경로 반환

    규칙:
    - AI worker는 URL만 신뢰
    - 파일은 /tmp 또는 OS temp dir에 생성
    - 호출자는 반환된 path만 사용
    """

    tmp_dir = Path(tempfile.gettempdir())
    ext = suffix or ""

    filename = f"ai_job_{job_id}{ext}"
    tmp_path = tmp_dir / filename
    part_path = tmp_dir / f"{filename}.part"

    try:
        with requests.get(download_url, stream=True, timeout=timeout) as r:
            r.raise_for_status()

            expected_len = r.headers.get("Content-Length")
            expected_len = int(expected_len) if expected_len and expected_len.isdigit() else None

            written = 0
            with open(part_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=chunk_size):
                    if not chunk:
                        continue
                    f.write(chunk)
                    written += len(chunk)

            if expected_len is not None and written != expected_len:
                raise RuntimeError(
                    f"download size mismatch (expected={expected_len}, got={written})"
                )

        part_path.replace(tmp_path)
        return str(tmp_path)

    except Exception:
        try:
            if part_path.exists():
                part_path.unlink()
        except Exception:
            pass
        raise


==========================================================================================
# FILE: wrong_notes/__init__.py
==========================================================================================
# apps/worker/wrong_notes/__init__.py


==========================================================================================
# FILE: wrong_notes/templates/wrong_note.html
==========================================================================================
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="utf-8" />
    <style>
        body {
            font-family: Arial, Helvetica, sans-serif;
            font-size: 12px;
            line-height: 1.6;
        }
        h1 {
            border-bottom: 2px solid #333;
            padding-bottom: 4px;
        }
        .exam {
            margin-top: 20px;
        }
        .question {
            margin-left: 12px;
        }
        .wrong {
            color: #c0392b;
        }
    </style>
</head>
<body>
    <h1>오답 노트</h1>

    <p>
        학생 ID: {{ enrollment_id }}<br/>
        생성일: {{ created_at }}
    </p>

    {% for exam_id, items in grouped.items %}
        <div class="exam">
            <h2>시험 {{ exam_id }}</h2>
            {% for item in items %}
                <div class="question">
                    <span class="wrong">
                        Q{{ item.question_id }}
                    </span>
                    / 제출 답안: {{ item.answer }}
                </div>
            {% endfor %}
        </div>
    {% endfor %}
</body>
</html>
